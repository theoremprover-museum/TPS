::::::::::::::
epr-npr-translate.tex
::::::::::::::
Here is a summary of what happens after matingsearch has terminated with
a proof. 
The functions involved are located in the files {\it mating-merge.lisp},
{\it mating-merge2.lisp} and {\it mating-merge-eq.lisp}.

\begin{enumerate}
\item Apply Pfenning's Merge algorithm ({\tt etr-merge}), put resulting
      etree and mating in variable \indexother{current-eproof}
  \begin{enumerate}
\item  Get a list of the connections from mating

\item Get a list of substitutions required
      \begin{enumerate}
\item  Extract substitutions from unification tree
        
\item  Replace occurrences of PI and SIGMA by quantifiers
        
\item  Lambda-normalize each substitution
        
\item  Alpha-beta-normalize each substitution
      \end{enumerate}
    
\item  Prune any unused expansions from the tree
    
\item  Make substitutions for variables
    
\item  Carry out merging ({\tt merge-all})
  \end{enumerate}

\item  Replace skolem terms by parameters (if applicable) 
({\tt subst-skol-terms})

\item  If remove-leibniz is T, apply Pfenning's algorithm for removing
      the Leibniz equality nodes for substitution of equality nodes
      ({\tt remove-leibniz-nodes})

\item  Try to replace selected parameters by the actual bound variables
      ({\tt subst-vars-for-params}), not always possible because of restriction
      that a parameter should appear at most once

\item  Raise lambda rewrite nodes, so that in natural deduction the lambda
      normalization occurs as soon as possible. ({\tt raise-lambda-nodes})

\item  Clean up the etree ({\tt cleanup-etree}).  For each expansion term
      in the tree,
   \begin{enumerate}
\item  Lambda-normalize it
      
\item  Minimize the superscripts on bound variables
      
\item Make a new expansion with the new term
      
\item  Deepen the new expansion like the original, but removing 
            unnecessary lambda-norm steps.
      
\item  Remove the original expansion
   \end{enumerate}

 Begin natural deduction proof, using \indexother{current-eproof}
   \begin{itemize}
\item  Set up planned line 
	\begin{enumerate}
\item  Use shallow wff of the current-eproof's etree
          
\item  Give it the tree as value for its NODE property
          
\item  Give it the current-eproof's mating (list of pairs of node names)
                as its MATING property
	\end{enumerate}
   \end{itemize}

 Call {\tt use-tactic} with tactic desired
  \begin{enumerate}
\item  Each line in the proof will correspond to a node in the etree; the 
natural deduction proof is stored in the variable \indexother{dproof}.
    
\item   Here is an important property which should remain invariant during
    the translation process:  It should always be the case that the 
    line-mating of the planned line is a p-acceptable mating for the
    etree that one could construct by making an implication whose antecedent
    is the conjunction of the line-nodes of the supports, and whose 
    consequent is the line-node of the planned line.  This will assure us
    that we have sufficient information to carry out the translation.
  \end{enumerate}
\end{enumerate}

It was observed that when path-focused duplication had been
used, the expansion proof would often have a great deal of redundancy
in the sense that the same expansion term would be used for a given variable
many times. More precisely, if one defines an expansion branch by
looking at sequences of nested expansion nodes, attaching one expansion
term to each expansion node in the sequence, there would be many identical
expansion branches. 

In response to this, {\it mating-merge.lisp} was modified in the following ways:
\begin{itemize}
\item Don't do pruning of unnecessary nodes at the beginning of the merge,
when the tree is its greatest size. 

\item Instead, prune all branches that couldn't possibly have been used; 
they are those that have a zero status. This is probably not necessary,
but certainly makes debugging easier and doesn't cost much.

\item After merging of identical expansions has been done, call the original
pruning function.
\end{itemize}

uning::::::::::::::
etps.tex
::::::::::::::
\chapter{ETPS}\label{etps}

\section{The Outline Modules}

The {\tt OUTLINE} modules in TPS have two main subparts; 
the bookkeeping functions, and the \indexcommand{GO} command which
gives sophisticated help or constructs a proof automatically.
They are collected in the modules of the form {\tt OTL*}.

In ETPS only the bookkeeping functions are present.

The discussion below is aimed at understanding the {\tt OUTLINE} modules
independently of the system, but we generally assume we are working in
\ETPS.  If \TPS differs, this is noted.

We often talk about {\it proofs}, even though they are properly only incomplete
proofs or {\it proof outlines}.  It is assumed that the reader knows what
planned lines ({\it plines}) and deduced lines ({\it dlines}) are.  This and 
general familiarity with \ETPS are necessary to understand this discussion.

\subsection{Proofs as a Data Structure}

Proofs in \ETPS are represented by a single atom with a variety of properties.
The global variable {\tt DPROOF} has as value the name of the current proof.
In case you are working, say on exercise {\tt X6200}, {\tt DPROOF} will have the 
value {\tt X6200}.  The current proof name then has a variety of properties.

\begin{description}
\item [{\tt LINES}] {\tt ({\it line} {\it line} ...)}.  This is simply an ordered list of all
lines in the current proof without repetition.  The order is such that
lines with a lower number appear first in the list.
\\ {\bf WARNING}:  This property is frequently changed destructively.
As a consequence it may never be empty and should be used for other
purposes only in a copy.

\item [{\tt LINEALIASES}] {\tt (({\it line} . {\it no})({\it line} . {\it no}) ...)}.  This is an 
unordered association list correlating lines with their numbers.  No line 
should ever appear in more than one pair, and neither should a number.  Try to 
think of arguments for and against this representation, compared to one where
the number of a line is stored on the line's property list.
\\ {\bf WARNING}:  This property is frequently changed destructively.
As a consequence it may never be empty and should be used for other
purposes only in a copy.

\item [{\tt PLANS}] {\tt {(({\it pline} . {\it supportlist})({\it pline} . {\it supportlist}) ...)}}.
This stores the important {\it plan}-{\it support} {\it structure} of the current
proof.  {\it pline} is a still unjustified line in the current proof,
{\it supportlist} is a list of deduced lines supporting a {\it pline}.
A {\it pline} may never have a justification other than {\tt PLAN}{\it i}, 
a {\it sline} (support line) must always be completely justified, i.e.
may not ultimately depend on a planned line.

The association list is ordered such that the most recently affected
{\it pline} is closer to the front of the list.  The order can be changed
explicitly with the {\tt SUBPROOF} command.

{\bf WARNING}:  This property is frequently changed destructively.
As a consequence it may never be empty and should be used for other
purposes only in a copy.

\item [{\tt GAPS}] ({\it gap} {\it gap} ...). This is a list of the gaps between lines in the
proof. Each gap has the properties ({\tt MIN-LABEL} {\it line}) and ({\tt MAX-LABEL} {\it line}).

\item [{\tt NEXTPLAN-NO}] {\it integer}. This is just the next number that will be used for 
a planned line.

\item [{\tt ASSERTION}] {\it gwff}. This is the assertion being proven.
\end{description}

When a proof is saved using the \indexcommand{SAVEPROOF} command, a checksum may be generated.
This is used by \ETPS to verify that the saved proof has not been manually edited by a student
(otherwise it would be possible to edit out the planned lines and convince \ETPS to issue the 
\indexcommand{DONE} command). Since it takes time to generate the checksum, it is only 
generated if the flag \indexflag{EXPERTFLAG} is NIL. This means that proofs written by \TPS
with \indexflag{EXPERTFLAG} T cannot be read into \ETPS with \indexflag{EXPERTFLAG} NIL.


\subsection{Proof Lines as a Data Structure}

Proof lines in \ETPS have a variety of properties:

\begin{description}
\item [{\tt REPRESENTS}] This is the wff asserted by the line.  In the original 
TPS this had to be an
atomized wff of very particular structure, which lead to numerous problems
in higher-order logic.  In \ETPS this has been maintained for the present. Our
goal, of course, is to allow arbitrary {\it gwffs} as {\tt REPRESENTS} of lines.

\item [{\tt HYPOTHESES}] This is a list of lines assumed as hypotheses for the line.
The list of hypotheses is ordered (lowest numbered line first), but to
my knowledge no function assumes this.  It simply looks better in the output.
No line should appear twice as an hypothesis (this fact may actually be used
here and there).

\item [{\tt JUSTIFICATION}]  {\tt {{\it RULE} {\it gwfflist} {\it linelist}}} The line can be
inferred by an inference rule {\it RULE}from {\it linelist}.  
{\it gwfflist} has somehow been used to infer the line.

\item [{\tt LINENUMBER}] The line number associated with the line.
\end{description}

\input{outline}
::::::::::::::
flags.tex
::::::::::::::
\chapter{Flags}

Here is an example of how to add the flag {\it neg-prim-sub} to \TPS:

Insert into the file prim.lisp the code:
\begin{verbatim}
(defflag neg-prim-sub
  (flagtype boolean)
  (default nil)
  (subjects primsubs)
  (mhelp "When T, one of the primitive substitutions will introduce negation."))

\end{verbatim}

Actually, that code almost worked, but changing the flag did not have the
desired effect. The primsubs were stored in a hashtable which, once computed,
was never changed again, so the code had to be  
replaced by:

\begin{verbatim}
(defflag neg-prim-sub
  (flagtype boolean)
  (default nil)
  (change-fn (lambda (a b c)
	       (declare (ignore a b c))
	       (ini-prim-hashtable)))
  (subjects primsubs)
  (mhelp "When T, one of the primitive substitutions will introduce negation."))

\end{verbatim}

Also put into the file auto.exp the line
\begin{verbatim}
(export '(neg-prim-sub))
\end{verbatim}

There are two ways to update flags. One is to do it manually. This is
supported by function update-flag. The other way is to set flags
automatically. For example, you may have to do this in your .ini
file. If XXX is a flag and you want to set it to YYY, then you can add
a line (set-flag 'XXX YYY) in your .ini file. Sometimes, you may use
(setq XXX YYY), but this is highly discouraged because XXX may have a
"change-fn" associated with it, which should be called whenever you
set XXX. Flag \indexflag{HISTORY-SIZE} is such an example.
(Note that if the variable being set is just a variable, and not a \TPS
flag, then the setq form is correct.)

\section{Symbols as Flag Arguments}

If your new flag accepts symbols as arguments, and only certain symbols 
are acceptable (as in, for example, \indexflag{PRIMSUB-METHOD} or \indexflag{APPLY-MATCH}),
the symbols which can be used should have help messages attached somehow. This can either
be done by defining a new category for the arguments, such as ORDERCOM or DEV-STYLE, or it can
be done using the \indexother{definfo} command: {\tt (definfo foo (mhelp "Help text."))} attaches
the given text to the symbol {\tt foo}.

\section{Synonyms}

It is possible to define two flags with different names which are 
synonymous to each other, using the \indexcommand{defsynonym} macro.
The advantage of this is that it allows the name of a flag to be changed
(from the user's point of view) without requiring either a change in the
code or extensive editing of all the modes saved in the library.

For example:

%\begin{tpsexample}
\begin{verbatim}
(defsynonym SUBNAME
            (synonym TRUENAME)
            (replace-old T)
            (mhelp "SUBNAME is a synonym for the flag TRUENAME."))
\end{verbatim}
%\end{tpsexample}

defines a new synonym for {\tt TRUENAME}. The {\it replace-old} property 
determines whether or not the new synonym is to be regarded as the
"new name" of the flag, if replace-old is {\tt T} 
(and so to be recorded in the library, etc.) 
or merely as an alias, if replace-old is {\tt NIL}.

::::::::::::::
flavors.tex
::::::::::::::
\section{Flavors and Labels}
\label{defflavors}
\label{labels}

It is sometimes desirable to be able to endow a gwff with additional
properties.  For example, one may wish to be able to refer to a
gwff by a short tag, or to specify that a particular gwff is actually
a node in an expansion tree.  For this purpose, \TPS provides the
facility of {\it labels} and {\it flavors}.  
A {\it label} is an object which, as far as \TPS is concerned, is merely
a special case of gwff. Labels thus stand for gwffs, but may have
additional properties and distinct representations.

{\it Flavors} are the classes into which labels are divided.  The definition
of a flavor specifies some common properties of a class
of labels, usually the behavior of wffops and predicates.  Also, a 
flavor's definition should specify what attributes each label of that 
flavor should have, and how it should be printed.

\subsection{Representation}

Each flavor is represented in \TPS by a Lisp structure of type {\tt flavor}, 
which has the following slots: {\tt wffop-hash-table}, which stores the
properties common to each instance of the flavor, in particular, how
wffops are to behave; {\tt constructor-fun}, which
is the name of the function to be called when a new label of the flavor
is to be created; {\tt name}, the flavor's name; and {\tt mhelp}, a description
of the flavor.  The values of these slots are automatically computed when
\TPS reads a {\tt defflavor} declaration.
The flavor structures are stored in a central hash table, called
{\tt *flavor-hash-table*}, keyed on the flavor names.  This also is 
updated automatically whenever a flavor is defined (or redefined).

There are two ways to represent labels (instances of flavors), and the
choice is made during the definition of the flavor.  The first, and more
traditional, way is to have each label be a Lisp symbol, with the attributes
of the label being kept on the symbol's property list.  
The second way is to make each label a Lisp structure.  The type of the
structure is the name of the flavor; thus an object's type can be used to
determine that it is a label of a certain flavor.

If one wishes to have labels be symbols, nothing must be done; this is the
default.  A flavor's labels will be structures only if one of two things
is declared in the {\tt defflavor}.  The first is that the property
{\tt structured} appears.  The second is if another flavor whose instances are
structures is specified to be {\it included} in the new flavor.

When a flavor's labels are to be structures, one will usually wish to
specify the {\tt printfn} property so that the labels will be printed
in a nice way.  This function must be one which is acceptable for use
in a {\tt defstruct}.  It is also required that one specify the slots, or
attributes, the
structures are to have, by including a list of the form
\begin{verbatim}
(instance-attributes (slot1 default1) ... (slotN defaultN))
\end{verbatim}
in the flavor definition.

\subsection{Using Labels}

The function {\tt define-label} is a generic way to create new labels of a
specified flavor.  The function call {\tt (define-label sym flavor-name)}
will do one of two things.  If {\tt flavor-name} is a flavor  whose
labels are symbols, then the property list of {\tt sym} will be updated
with property {\tt FLAVOR} having value {\tt flavor-name}.  If on the 
other hand, {\tt flavor-name} is a flavor having structures for labels,
then {\tt sym} will be setq'd to the value of the result of calling
the constructor function for {\tt flavor-name}, which will create
a structure of type {\tt flavor-name}.

To access the attributes of a label which is a symbol,  use {\tt get},
since all attributes will be on the symbol's property list.  
The attributes of a label which is a structure of type {\tt flavor-name}
can be accessed by using the standard Lisp accessing functions for
structures.  Thus, if one of the label's attributes is {\tt represents},
the attribute can be accessed by calling the function 
{\tt flavor-name-represents}. 

Flavors can be redefined or modified at any time.  This may be done if,
for example, one wished to extend a flavor's definition into a Lisp
package which was not always loaded.  Merely put another {\tt defflavor}
statement into the code.  You need only put the new or changed 
properties in the redefinition.  If, however, you wish to change the
attributes of a flavor which is a structure, you should put in all
of the attributes you desire, not just the new ones, and be sure
to declare any included flavor as well.  Note: it is possible to
change a flavor which uses symbols as labels into one which uses
structures, but if you fail to redefine code which depends on 
property lists, the program will be smashed to flinders.


\subsection{Inheritance and Subflavors}

Some flavors may be similar in many ways; in fact, some flavors may be
more specialized versions of other flavors.  One may wish a new flavor's
labels to be operated upon by most wffops in the same way as an existing
flavor's labels; this we will call inheritance of properties.  In addition,
one may wish a new flavor to actually be a subtype (in Lisp terms)
of an existing flavor, and have the attributes of the existing flavor's 
labels be included in the attributes of the new flavor's labels; this we
will call inclusion of attributes.   The {\tt defflavor} form allows either
or both types of sharing to be used.

Inheritance of properties is signalled in the {\tt defflavor} by a form such
as {\tt (inherit-properties {\it existing-flavor1} ... {\it existing-flavorN})}.
This will cause the properties in the {\tt wffop-hash-table} of the 
existing flavors to be placed into the {\tt wffop-hash-table} of the new
flavor. If any conflict of properties occurs, e.g., if {\it existing-flavorI}
and  {\it existing-flavorJ}, I < J, have a property with the same name, then
the value which {\it existing-flavorJ} has for that property will be 
the one inherited by the new flavor. A new flavor may inherit properties 
from any number of existing flavors.

In contrast, attributes may be included from only one other flavor.  This
can be done by using the form {\tt (include {\it existing-flavor})}. The 
existing flavor must be a flavor whose instances are structures, and the
new flavor's instances will also be structures whose slots include the
attributes of the existing flavor.  Thus the same accessing functions for
those slots will work on labels of both flavors.  To define default
values for those slots, add them to the {\tt include} form as if it were
an {\tt :include} specifier to a {\tt defstruct}; e.g., 
{\tt (include {\it existing-flavor} ({\it slot1 default1}))}.

\subsection{Examples}

Here are some examples of flavor definitions.

%\begin{lispcode}
\begin{verbatim}
(defflavor etree
  (mhelp "Defines common properties of expansion tree nodes.")
  (structured t)
  (instance-attributes
   (name '|| :type symbol)
   components				; a node's children
   (positive nil )			; true if node is positive in the
					;formula
   (junctive nil :type symbol)		; whether node acts as neutral,
					;conjunction, or disjunction
   free-vars				; expansion variables in whose scope
					; node occurs,
					; used for skolemizing
   parent				; parent of the node
   ;;to keep track of nodes from which this node originated when copying a
   ;;subtree
   (predecessor nil)
   (status 1))
   (printfn print-etree)
   (printwff (lambda (wff bracket depth)
	       (if print-nodenames (pp-symbol-space (etree-name wff))
		 (printwff 
		  (if print-deep (get-deep wff)
		    (get-shallow wff))
		  bracket depth))))
   ...many more properties...)
\end{verbatim}
%\end{lispcode}

{\tt \indexData{Etree}} labels will be structures, with several attributes.
The function used to print them will be {\tt print-etree}.

%\begin{lispcode}
\begin{verbatim}
(defflavor leaf
  (mhelp "A leaf label stands for a leaf node of an etree.")
  (inherit-properties etree)
  (instance-attributes
   shallow)
  (include etree (name (intern-str (create-namestring leaf-name))))))
\end{verbatim}
%\end{lispcode}

{\tt \indexData{Leaf}} labels will also be structures, with attributes including
those of {\tt etree}, as well as a new one called {\tt shallow}.  Note that
the {\tt name} attribute is given a default in the {\tt include} form.
{\tt Leaf} inherits all of the properties of {\tt etree}, including, for
example, its print function, unless they are explicitly redefined in
the definition of {\tt leaf}.



\section{Printing Proofs}

Proofs printed in Scribe or \TeX are preceded by preambles which are
defined by the variables \indexflag{SCRIBE-PREAMBLE} and 
\indexflag{VPFORM-TEX-PREAMBLE}. The values of these flags are set
in the {\it tps3.ini} file. Since these preambles source
files in the directory {\it .../doc/lib}, things must be done carefully to
make sure that \indexcommand{SCRIBEPROOF} and \indexcommand{TEXPROOF} 
will insert the appropriate pathname when tps is distributed to other 
locations. Note that the Makefile creates the file {\it tps3.sys}, which contains the variable
sys-dir which shows where the tps was built.
	
When the Scribe preamble was changed to add
\begin{verbatim}
@@LibraryFile(KSets)
@@LibraryFile(Mathematics10)
\end{verbatim}
some of the hacks in {\it tps.mss} may have become obsolete (but harmless).
Mathematics10 is a file from the standard Scribe library; KSets
is a file belonging to \tps.

::::::::::::::
grader.tex
::::::::::::::
\chapter{The Grader Program}

(Programmers should be aware that the GRADER program has its own manual.)

\section{The Startup Switch}

In theory, adding the switch {\tt -grader} to the command line which 
starts up \TPS should start up the Grader program directly. The code which 
implements this is in \indexfile{tps3-save.lisp}.

In practice, some modifications may be needed depending on the particular 
Lisp being used. For example:

\begin{itemize}
\item When starting up in CMUlisp on an IBM RT, the error {\tt "Switch does not exist"}
will be given. This is just Lisp complaining that it doesn't recognize the switch;
it passes the switch on to \TPS anyway, so this is no cause for concern.

\item When using Allegro Lisp version 4.1 or later, a {\tt --} symbol is used to separate
Lisp options from user options. So, on early versions of Allegro Lisp the line to
start up grader is:
{\tt xterm {\it <many xterm switches>} -e /usr/theorem/bin/run-tps -grader \&}
whereas for later versions it is:
{\tt xterm {\it <many xterm switches>} -e /usr/theorem/bin/run-tps -- -grader \&}
\end{itemize}
::::::::::::::
help.tex
::::::::::::::
\chapter{Help and Documentation}
\label{help}

\section{Providing Help}

When the user types the command \indexcommand{HELP object}, \TPS will first
try to determine which category {\tt object} is in (it may be in several, in which case it will 
produce a list of categories and then print the help for each separately).

Recall from the entry about categories (section ~\ref{categories}) that each category has 
{\tt mhelp-line} and {\tt mhelp-fn} properties. The {\tt mhelp-line} is a short phrase that describes 
each object in the category (for example the category {\tt PMPROPSYM} has the mhelp line 
"polymorphic proper symbol"). The mhelp-fn is a function that will print out the help for a specific
object. For many simple categories (e.g. {\tt context}), 
the function {\tt \indexother{princ-mhelp}} is sufficient; this simply 
prints the {\tt mhelp} string attached to the object in question. Other categories need more complex
help (for example {\tt mexpr}), and so have their own specially-defined mhelp functions.

When writing help messages or mhelp functions, keep in mind that the information given 
should contain all the information that a {\it user} would want to know. 
More detailed help for maintainers and programmers should 
be written down and incorporated into this manual, not added into the online documentation.

\subsection{Mhelp and Scribe}
The online documentation can be used to generate a facilities guide, so
it is important that you be aware that the mhelp properties and mhelp functions
you define for new objects or categories will be used to generate Scribe
files.  Take a look at the files {\it mhelp.lisp} and {\it scrdoc.lisp} and see
how this works.  You may need to set things up properly so that the entries you
are introducing are put into the index of such guides.  Look at the
file {\it tpsdoc.mss} in the doc/lib area to see how the indexing is done.

\section{The Info Category}

There is a category of objects called \indexother{INFO} which is used solely for providing
help on symbols that would otherwise not have help messages (for example, the various settings
of some of the flags, such as PR97 or APPLY-MATCH-ALL-FRDPAIRS). You can attach a help message
to any symbol {\it foo} with:

%\begin{tpsexample}
\begin{verbatim}
(definfo foo (mhelp "Help text."))
\end{verbatim}
%\end{tpsexample}

\section{Printed Documentation}

The directories with root {\it /home/theorem} mentioned below are
on gtps.

{\it /home/theorem/project/doc/files.dir} contains information about
\TPS documentation files.

{\it /home/theorem/project/doc/etps/tps-cs.mss} describes how to
access \TPS in the cmu cs cell.

{\it /home/theorem/project/doc/etps/etps-andrew.mss} describes how
to access \ETPS in the andrew cell.

{\it /home/theorem/project/doc/<topic>/manual.mss} is the main file
for the manual on <topic> (one of: {\tt char}, {\tt etps}, {\tt facilities}, 
{\tt grader}, {\tt prog}, {\tt teacher} and {\tt user}).

See the \TPS User Manual for additional information.

When new facilities are added to \ETPS, copy the information
about them from the automatically produced {\it facilities.mss} into the
appropriate \ETPS mss file.

\section{Indexing in the Manuals}

The basic mechanisms are in {\it /home/theorem/project/doc/lib/index.lib}
and {\it /home/theorem/project/doc/lib/indexcat.mss}. Note the comment on
the use of {\tt @IndexCategory} in the former file.  In the \TeX
version of the Programmer's Guide, there are indexing commands defined
which mimic the role of the corresponding Scribe commands.

{\tt @indexother{DIY-TAC}} in the text on page <pagenumber> puts
{\tt DIY-TAC <pagenumber>} into the index.

{\tt @index*X*(WORD}) in the text on page <pagenumber> puts
{\tt WORD, *Y* <pagenumber>} into the index.

Example:
{\tt @indexcommand{DO-GRADES}} in the text on page <pagenumber> puts
{\tt DO-GRADES, System Command <pagenumber>}
into the index. Here is a partial list of 
of possible values for {\tt *X*} and {\tt *Y*}, where the complete
list is in {\it /home/theorem/project/doc/lib/indexcat.mss}.

\begin{itemize}
\item {\tt *X*} = command gives {\tt *Y*} = System Command
 
\item {\tt *X*} = edop gives {\tt *Y*} = Editor Command
 
\item {\tt *X*} = flag gives {\tt *Y*} = flag
 
\item {\tt *X*} = function gives {\tt *Y*} = function
 
\item {\tt *X*} = style gives {\tt *Y*} = style

\item {\tt *X*} = mexpr gives {\tt *Y*} = mexpr
\end{itemize}

See "quitting" in the index of the ETPS manual to see the 
effect of the following lines in the file
{\it /home/theorem/project/doc/etps/system.mss}:

%\begin{tpsexample}
\begin{verbatim}
@@seealso[Primary="Quitting",Other="@{\tt EXIT}"]
@@seealso[Primary="Quitting",Other="@{\tt END-PRFW}"]
@@seealso[Primary="Quitting",Other="@{\tt OK}"]
@@indexentry[key="Quitting",entry="Quitting"]
\end{verbatim}
%\end{tpsexample}

\section{Other commands in the manuals}

Any other Scribe commands may be used in the manuals; for example
we use the {\tt typewriter} font given by @t for command 
names, and the {\it italic} font given by @i for file names.

In the \TeX versions of the manuals, one uses the corresponding
\TeX commands.

We also have @TPS in Scribe (and \verb+\TPS+ in \TeX)
to print the string "\TPS", and @HTPS in Scribe to do the
same in headers.

::::::::::::::
intro.tex
::::::::::::::
\chapter{Introduction}

\begin{quotation}
"The {\it Guide}... is an indispensable
companion to all those who are keen to make sense of life in
an infinitely complex and confusing Universe, for though it
cannot hope to be useful or informative in all matters, it
does at least make the reassuring claim, that where it is
inaccurate it is at least {\it definitively} inaccurate. In cases
of major discrepancy it's always reality that's got it wrong."

Douglas Adams, {\it The Restaurant at the End of the Universe}
\end{quotation}

\section{Guidelines}

This guide assumes that the reader is familiar with Common Lisp, and
does not attempt to explain or summarize information that is available
elsewhere about the workings of that language, in particular, in
Steele's {\it Common Lisp the Language, (2nd ed.)}.

There are three major rules which should be followed whether maintaining
\TPS code, or just fooling around with it:

\begin{enumerate}
\item Always keep a backup copy of the files you are changing, so that when
you realize how badly you goofed, you can put things back the way they were.

\item Don't get too tricky.  Clever hacks may be amusing, and may indeed
give some increase (usually modest) in efficiency, but within weeks
you will have no idea how they work, and others will be even more
mystified.  Those who follow you in your task will curse and despise
you; consequently, your cute programs will probably be completely rewritten
anyway. 

\item Don't panic.
\end{enumerate}

See section ~\ref{writing} for more minor guidelines.

\section{\TPS Conventions}

\subsection{Filenames}

The extension of a filename should indicate what it contains:
{\it .lisp} for Lisp source code; {\it .exp} for export statements
({\it vide infra}); {\it .rules} for deduction rule definitions
({\tt defirule} statements); {\it .mss} for Scribe formatted
documentation; {\it .tex} for \TeX formatted documentation;
{\it .vpw} for vpwindow output; {\it .work} for work files; {\it .prf} for proofs.

Filenames should be descriptive of their contents, without being too long.  For example,
{\it functions.lisp} would be a stupid name, because from the name no
one would know what its purpose was.  If you have several related
files, it is a good idea to give them a common prefix, so that it is
clear just from their names that they are related. 

\subsection{Lisp packages and export files}
\TPS creates and uses several different Lisp packages.  (If you don't
know what I mean by Lisp package, read the chapter on packages in {\it 
Common Lisp the Language, (2nd ed)}.)  These packages are
created when \TPS is compiled or built, by {\tt make-package} forms in
the files {\tt tps-build.lisp} and {\tt tps-compile.lisp}.  The package
structure is set up so that common functions are placed in the package
{\tt CORE}, which is used by each of the other packages.  

The {\tt CORE}
package contains functions such as those dealing with wff parsing and
printing, proof manipulation, and operating system interfaces (such as
basic file operations). It also contains functions for dealing with 
Scribe, vertical paths, editing, windows, review, etc... 
The other packages include: {\tt GRADER}, for functions relating to the Grader
subsystem; {\tt AUTO} for functions relating to automatic proof
procedures such as mating search; {\tt ML} for things specific to the
Math Logic courses, such as exercises and proof rules.

\ETPS contains part of the {\tt CORE} package, part of the {\tt OUTLINE}
package and part of the {\tt RULES} package.

The idea is that only those symbols that are needed by other packages are
exported from their home package. In order to specify which symbols
should be exported, the files {\it core.exp}, {\it auto.exp}, etc.  These
files, one for each Lisp package, are loaded at the beginning of the
compilation process, before any code is loaded.  This way, any package
conflicts are detected immediately.

There is a special export file, called {\it special.exp}.  This file
contains export statements for symbols which may already exist in
certain Lisp implementations.  For example, some implementations already
contain a symbol {\tt EXIT}, while others do not.  Why is this a
problem?  Because if the {\tt CORE} packages uses a package from an
implementation (e.g., Allegro's {\tt EXCL} package), and that package
already exports the symbol {\tt EXIT} (so that {\tt EXIT} is imported by
{\tt CORE}),  then an error will result if we
try to export {\tt EXIT} from the {\tt CORE} package, i.e., you can't
export a symbol from a package other than its {\it home} package.
{\it special.exp} uses the
standard {\tt \#+} and {\tt \#-} macros to specify in which implementations
such symbols should be exported from the {\tt CORE} package.  Generally,
these nuisance symbols are found by trial and error when first porting
\TPS to a Lisp implementation, and some symbols may have to be moved
from {\it core.exp} to {\it special.exp}. (Another symbol in \$t(CORE) that can
cause problems with some Lisps is {\tt date}.)

Note that when \TPS starts up, the {\tt USER} (soon to be
{\tt COMMON-LISP-USER}, as the changes in Common Lisp suggested by the
X3J13 committee are implemented) is the value of the variable
{\tt *package*}.  What this means is that any symbols typed in by the
\TPS user will be interned in the {\tt USER} package.  Thus, any symbols
that could be inputted by the user as, say, a flag value, should be
exported from the package in which they were defined, otherwise \TPS
will  not realize they are supposed to be the same.  As an example,
the flag {\tt RULEP-MAINFN} can be given the value {\tt RULEP-SIMPLE}.
Since {\tt RULEP-SIMPLE} is defined in the {\tt CORE} package, it must be
exported in {\it core.exp}, so that when it is inputted, the symbol
{\tt CORE::RULEP-SIMPLE} is interned, not {\tt USER::RULEP-SIMPLE}.  Of
course, this presumes that the {\tt USER} package uses the {\tt CORE}
package (which it always does in \TPS).

\subsection{Implementation-specific differences}
Not all Lisp implementations are alike.  This is
particularly true in the areas of Common Lisp which are intentionally
unspecified, including things like how the Lisp top level works, how
file pathnames are represented, how the user exits the Lisp or saves
a core image. 

For this reason, certain \TPS source files contain 
{\tt \#+} and {\tt \#-} directives.  We try to keep the number of these
files to a minimum, so that when porting to new implementations, work
is minimized.  When using {\tt \#+} and {\tt \#-}, you should try to use as
specific a feature of the implementation as possible (but avoid using
the machine type unless that is the reason you have to make a change).
For example, the feature {\tt :allegro-v3.1} is probably better than
{\tt :allegro}, as I have found out to my dismay when Allegro 4.0 came
out.  Look at the lisp variable *features* to find what features that
version of lisp recognizes.

There is one feature ({\tt :andrew}) that is added when we compile \TPS/\ETPS for use
on the Andrew workstations (machines in the domain andrew.cmu.edu).
This is because on those machines, lisp implementations have problems
interfacing with the operating system and getting the proper home
directory of a user.  Thus special measures are taken in this case.
This feature is added in the {\tt .sys} files for the Andrew editions.

There are another two features which are added in the relevant {\tt .sys}
files for \ETPS and \TPS; these are \indexother{:TPS} and \indexother{:ETPS}.
This allows programmers to specify slightly different behaviour for the 
two systems (for example, when using the editor, you may have a window that
shows the vpform in \tps, but not in \ETPS).

The files which use {\tt \#+} and {\tt \#-} are principally {\it special.exp},
{\it boot0.lisp}, {\it boot1.lisp}, {\it tops20.lisp}, {\it tps3-save.lisp},
and {\it tps3-error.lisp}.

\subsection{\TPS modules}

\TPS source files are organized into \TPS modules. 
Basically, a \TPS module is just a list of source files, in the
sequence in which they are to be compiled/loaded.  All
\TPS modules are defined in the file {\it defpck.lisp}.  Each source
file should be in a \TPS module, and that module should be indicated
in the file.  (Conceivably, one might define two
different \TPS modules which had files in common, but we have never
done that.)  

In addition to the files it contains, the definition of a \TPS module
also specifies the other modules which must also be present when it
is used.  

The \TPS module structure breaks up the source files into chunks,
each which has some particular purpose or purposes.  Then to build a
version of \TPS which has certain capabilities, one need
only load the modules required.  This is how the files
{\it tps-build.lisp} and {\it tps-compile.lisp} specify \TPS is to be
built. Note that {\it etps-build.lisp} and {\it etps-compile.lisp} just
load fewer modules than the build/compile files for \tps.  Likewise,
there are {\it grader-compile.lisp} and {\it grader-build.lisp} files for
building a Grader core image.

By using the module mechanism, a module may be modified by
adding, deleting, or modifying its constituent files, and other users don't
have to know; all they need to know is what the module provides.

Functions such as {\tt LOAD-MODULE}
are provided to load modules, making sure that any modules they
require are also loaded.

\subsection{File format}

In general, programmers should use only lower-case.  Why? For two
reasons.  It is easier to read, and in case-sensitive operating
systems like Unix, it is easier to use utilities such as fgrep and
gnu-emacs tags ({\it vide infra}) to search for occurrences of symbols.

Each \TPS source file should contain certain common elements. First is
a copyright notice, whose purpose is self-explanatory (just copy it
from another source file).  Make the copyright date current for any
new code.

The first line of the file, however, should be something like:
\begin{verbatim}
;;; -*- Mode:LISP; Package:CORE -*-
\end{verbatim}
The gnu-emacs editor will use this line to put the buffer in Lisp
mode, and if you are using one of the gnu-emacs interfaces to the
lisp, it will use the package information appropriately.  See the
documentation for such gnu-emacs/lisp interfaces.

The first non-comment form of each source file should be an
{\tt in-package} statement, to tell the compiler what package the file
should be loaded in. Recent implementations of lisp will object if
there is an {\tt \indexother{in-package}} command anywhere else in the file.

Next, the \TPS module of the file should be indicated, by a
{\tt part-of} statement, like {\tt (part-of {\it module-name})}.
This should match the entry given in {\it defpck.lisp}.

Don't forget {\tt context} statements.  Basically they just
reset the variable {\tt current-context}, which is used by other
functions to organize the documentation and help messages.

\section{Maintenance}

\subsection{Porting \TPS to a new Lisp}
As discussed above, the lisp-implementation-dependent parts of \TPS are confined to
a few files.  See the discussion above, and the user's manual, for
more details. There are (at least) two places in which the code is currently
not ANSI standard, in that the function {\tt int-char} and the type {\tt string-char}
(which do not exist in ANSI standard lisp) are used. When porting to a new lisp,
one should be aware that it may be necessary to provide substitute definitions 
for these. See {\tt defun int-char} and {\tt deftype tps-string-char} in {\it boot0.lisp}
for an example of how this might be done.

Also, if you are using Kyoto Common Lisp, you will find that the way it
represents directories is a little unusual: all paths are relative unless
specified not to be. So, for example, {\it tps3.sys} should be changed to read:

\begin{verbatim}
(setq news-dir '(:root "usr" "tps"))
(setq source-path '((:root "usr" "tps" "bin") 
                    (:root "usr" "tps" "lisp")))

(setq compiled-dir '(:root "usr" "tps" "bin"))
(setq patch-file-dir '(:root "usr" "tps"))
\end{verbatim}

(assuming the main \TPS directory is {\it /usr/tps/}).

If you are using Allegro Common Lisp version 4.1 or later, {\tt --} is
used to separate user options from lisp options, and hence the standard 
way of starting up the Grader program in X-windows becomes:

{\tt xterm -geometry 80x54--14-2 '\#723+0'  -fn vtsingle -fb vtsymbold-sb -n 
CTPS-Grader -T CTPS-Grader -e /usr/theorem/bin/run-tps -- -grader \&+}

\subsection{Building \TPS}
See the user's manual for a description of how to set up and build a
new version of \tps/\ETPS.

The global variable \indexother{core-name} currently contains "TPS3";
it is defined in \indexfile{tps3.sys}, which is generated by the Makefile.
All files (news, note, ini, sys, patch, exe) use core-name as their
`name'.

File names and extensions should be strings rather than
quoted symbols, to avoid any ambiguity with the package qualifiers.

Changes to the code are put in the patch file {\it \indexfile{tps3.patch}} until 
\TPS is rebuilt. \ETPS and Grader have separate patch files.
When you change the file {\it nat-etr.lisp} (for
example), put the line {\tt (qload "nat-etr")} into {\it tps3.patch}.
In general, don't put {\tt (qload "nat-etr.lisp")} into the patch file, or the
uncompiled version of the file will be loaded. However, the export files
{\it *.exp} do need their extension.

Entries such as {\tt (qload "auto.exp")} which load exp files should
come before those loading lisp files. 
{\tt (qload "core.exp")} should come before loading other export files.
Macro files should come before other files in the same module.

Putting the line
{\tt (setq core::*always-compile-source-if-newer* T)}
near the beginning of the {\it tps3.patch} file, and \\
{\tt (setq core::*always-compile-source-if-newer* NIL)}
at the end of the same file
will cause files to be compiled automatically
whenever appropriate as one is starting up \tps, but then
restores the default value of {\tt *always-compile-source-if-newer*}
so that you will be able to decide whether or not to compile other
files as you load them.

Example: when ms91-6 and ms91-7 were introduced, {\it tps3.patch}
contained:
\begin{verbatim}
(qload "core.exp")
(qload "auto.exp")
(qload "defpck")
(qload "contexts-auto")
(load-module 'ms91)
(qload "diy")
\end{verbatim}

\subsection{Memory Management}

\TPS uses a huge amount of \indexother{memory} in the course of a long search, and it may be necessary 
to rearrange either the internal memory available in your computer or the maximum space
occupied by your version of Lisp. Both of these things vary; the former by system (type {\tt sys} to
find out what system you are using) the latter by the variety of Lisp. You can tell roughly how much memory
is being used in most versions of Lisp by turning on garbage collection messages and watching the numbers
they report.

After a long search, \TPS may fail with an error message that mentions not having enough 
\indexother{heap space}, or \indexother{stack space}, or \indexother{swap space}. Allegro 
Lisp is very good about indicating the real 
cause of the problem. CMU lisp turns off errors while it garbage collects, and unfortunately that's
when most of these errors occur, so if your CMU-based \TPS seizes up in mid-garbage collect and refuses to
stop even for $\hat{}$C, then you've probably run out of memory somewhere. Lucid Lisp turns off garbage 
collection when it approaches the internal memory limits (there is a good reason for this; see 
the Lucid manual), so if you get a message about garbage collection being off then the real
problem is probably a lack of memory. 
(\TPS never switches \indexother{garbage collection} off itself.)

\subsubsection{Heap Size and Stack Size}
On a Unix system, type {\tt limit} into a C-shell (or whatever shell you're using) to see a list 
of the upper limits on various things stored in memory. The ones you're most interested in will
be \indexother{datasize} and \indexother{stacksize}. If you are superuser, you can remove these restrictions
temporarily by typing {\tt unlimit datasize stacksize}, or possibly {\tt unlimit -h datasize stacksize}. 

To increase these limits permanently, you need superuser privileges. You will need to reconfigure the
kernel and reboot your system. On anything except an HP, write to {\tt gripe@cs} and ask them to do it,
unless you're confident about being able to do such things. On an HP, you can use their \indexother{SAM}
program (when nobody else is logged in, since you're going to do a reboot), as follows:
\begin{enumerate}
\item Log in as superuser, and type {\tt sam}.

\item Double-click on "Kernel Configuration"

\item  Double click on "Configurable Parameters"

\item Highlight the parameter "maxdsiz" and select "Modify" from the "Actions" menu.
Increase the value as high as you want. On our machines, it was initially 
0x04000000 and we increased it to 0x7B000000. If you choose too high a number,
it will be rejected and you can try again.

\item Check that the "value pending" column shows your new value for maxdsiz. If not,
pick "Refresh Screen" from the "Options" menu and do the last step again.

\item Now do the same for "maxssiz"; we increased it from 0x00800000 to
0x04FB0000.

\item Choose "Exit" from the "File" menu. You will get a barrage of questions, say
yes to all of them. (They will be something like: create the kernel now? 
replace the old kernel? reboot the system?)
\end{enumerate}

When the reboot is done, type {\tt limit} to check that the values have increased.

\subsubsection{Swap Space}
Swap space is that part of the memory (usually on disk) where the operating system
stores parts of the programs that are supposed to be in memory. This is how you 
can get away with running more programs than your RAM has space for.
Clearly, the amount of \indexother{swap space} you need will depend not only on how big
your \TPS grows, but also on what else is running at the same time.

Again, on anything but an HP it's time to go whining to {\tt gripe@cs} and get them to do it.
On an HP, start SAM as in the last section, and double-click on "Disks and File Systems".
Now double-click on "Swap". There are two sorts of swap space, device ({\tt dev}) and file system
({\tt fs}). The former is faster and should be given priority over the latter.

Here is where I don't quite understand what's going on, so if this information ever becomes
crucial it would be a good idea to check it. I believe that device swap space is simply a 
partition of the internal disk drive, and that it might be possible to create more space simply by
rearranging the partition. I have no idea how to do this.

For the time being, then, we'll restrict ourselves to filesystem swap space. You can mount 
one filesystem swap space on each disk you've got, so take a look at the list that SAM has given you.
If there are no {\tt fs} swap space listed, or there is a disk that doesn't have one, then you can 
create one by selecting "Add Filesystem Swap" from the "Actions" menu. Give it a reasonable 
number (you can use {\tt du} and {\tt df} to find out how much space there is on the disk at the moment,
and then choose some large fraction of that), and allocate a priority that is lower (which is to say,
a larger number; 0 is highest-priority) than the priorities of the {\tt dev} swapspace (so that you 
will use the fast swap space before the slow one). New swap space takes effect right away.

If you already have {\tt fs} swapspace on all disks, you can highlight the one you want to change 
and then choose "Modify Swap Space" from the "Actions" menu. Increase the size as you want.
Modifications only take place after the next reboot, but it is not necessary to reboot right away
as it is for the heap and stack space.

\subsubsection{Internal Limits in Lisp}
As if all that wasn't enough, your version of Lisp may also have some constraints on how large it can grow.
\begin{enumerate}
\item CMU Lisp has no such limits, as far as I know.

\item Lucid Lisp has them, and they are user-modifiable; type {\tt (room t)} into a Lucid \TPS to see 
what the current settings are. Look for "Memory Growth Limit"; if it seems too small, type
(for example)
{\tt (change-memory-management :growth-limit 2048)} into the \TPS to allocate 128Mb (2048 64kb segments).
You can also make this permanent by adding {\tt \#lucid(change-memory-management :growth-limit 2048+)}
to your {\it tps3.ini} file. Other parameters besides the overall size limit can also be changed; see the
Lucid manual for details.

\item Allegro Lisp also has a limit, but in this case it is set at the initial building of Lisp. Here you'll
have to retrieve the build directory for allegro (which is {\it /afs/cs/misc/allegro/build/} followed by 
the name of your system). We have a copy of this on {\it tps-1}, called {\it allegro4.2hp\_huge}, but it
requires some hacking to make it build properly. Follow the instructions in the README to build yourself
a new Lisp core image with more than the standard 60Mb data limit. If you aren't up to the 
hacking, once again the solution is to whine at {\tt gripe@cs}, who will forward your mail to the 
Allegro maintainer.
\end{enumerate}

\subsection{Making tar tapes of \TPS for distribution}
\input{tar}


\section{How to \indexother{locate} something?}\label{finding}
Sometimes you will be looking at code, and will come across a function
or variable whose purpose is not familiar to you.  If it is not a standard Common
Lisp function, for which the Lisp functions {\tt documentation} and
{\tt apropos} may be useful, as well as reference books and user manuals,
there are three ways to \indexother{find} where it is defined.

The first method uses the gnu-emacs tags mechanism. Periodically, we run the 
{\tt etags} program on the
{\it .lisp} files in the source directory. One does this by entering the \TPS
lisp directory and then running the \indexother{etags} program; usually, 
this is done by typing {\tt M-x shell-command etags *.lisp}.  
This generates a file called
{\it \indexfile{TAGS}}, with entries for each line of code which begins with
{\tt (def...}. Then you can use the gnu-emacs {\tt find-tag} function (ESC-. , 
unless you've rearranged the emacs keys) to look
for the first occurrence of the symbol, and the {\tt tags-loop-continue} function
(ESC-,) to find the each subsequent occurrence.  
This can be slow if there are many symbols which begin with the
prefix for which you are searching, or if the symbol is overloaded by 
defining it for different purposes (e.g., {\tt LEAVE} is a
matingsearch command, a review command, a unification toplevel command).  
See the gnu-emacs documentation.

Certain functions, such as eproof-statuses, are defined implicitly,
and you won't find their definitions using the tags mechanism. If you
look at the definition of the structure \indexother{eproof} in the file
etrees-flags.lisp, however you will find:
\begin{verbatim}
(defstruct (eproof (:print-function print-eproof))
...
  (statuses (make-hash-table :test #'eq))
\end{verbatim}
This defines the function eproof-statuses.

The second method is to use the \TPS export files. Try
examining the files with a {\it .exp} extension.  Generally, comments 
tell which file each symbol comes from.  This method will fail,
however, if the symbol is not exported, or if the symbol has been
moved from the file in which it was originally defined without the
{\it .exp} having been updated.

\TPS has many global lists; the master list is called 
\indexother{global-definelist}, and in general each sort of \TPS object 
will have an associated global list. 

The last method is to use operating system utilities like grep and
fgrep to find all occurrences of the symbol.


\section{Utilities}
Utilities are commonly-used Lisp functions/macros. The functions (or macros) themselves 
are defined in the normal way, and then a \indexcommand{defutil} command is added into 
the code beside the function definition. The point of adding the {\tt defutil} command is
that utilities have their own \TPS category, you can get online help on them, and their 
help messages are printed into the Facilities Guide; this will help other \TPS 
programmers to find them in the future.
 
Examples are such functions as \indexother{msg} and \indexother{prompt-read}; see the
facilities guide for a complete list.

There aren't really very many \indexother{utilities}
at the minute, although it would be useful if more were defined, since then we could
avoid duplicating code in different places. So, if you write a useful macro or 
function {\it foo}, or discover one already written, please add a utility 
definition next to it in the code. This should look like:

%\begin{tpsexample}
\begin{verbatim}
(defutil foo
   (Form-Type function)
   (Keywords jforms printing)
   (Mhelp "Some useful words of wisdom about the function foo."))
\end{verbatim}
%\end{tpsexample}

{\tt Form-Type} should be either {\tt function} or {\tt macro}. {\tt Keywords} can be anything
you want, since it is currently ignored by \tps. {\tt Mhelp} is, of course, a help message.
Note: if your useful function is actually an operation on wffs, it should be defined as
a wffop or wffrec (recursive wffop) rather than as a utility; utilities are really 
intended to be functions that are useful to \TPS programmers but which do not fall into 
any other \TPS category.

\section{Overloading Commands}

There are certain symbols in \TPS that been {\it overloaded}, that is
they have been defined to have more than one meaning: they may be
simultaneously a matingsearch command, review command, and unification
command. This is done so that same symbol can have similar effect in different
top-levels. For example, {\it LEAVE} should leave the current top-level, 
as opposed to having a different exiting command for each top-level,
which would make things more difficult for the user to remember.

This can cause problems in \TPS unless programmers are careful.  You
see, we currently use the symbol's property list extensively to store
things.  When a matingsearch command (such as {\tt LEAVE}) is defined,
the actions that are to be taken when the user inputs the command are
stored on {\tt LEAVE}'s property list.  It is important, therefore, that
each category use different property names, so that there is never a
clash.  For example, if we used the property {\tt ACTION} for both
review commands and matingsearch commands, then {\tt LEAVE}'s property
list could not hold both simultaneously, but merely one or the other.
Better property names would be {\tt REVIEW-ACTION} and {\tt MATE-ACTION}.

\section{Output}

Some general tips for keeping the output as neat as possible:

\begin{itemize}
\item Avoid using the lisp function \indexfunction{y-or-n-p}, and stick to the \TPS
function \indexfunction{prompt-read}, so that the responses will go into work files
correctly.

\item \indexfunction{msg} and \indexfunction{msgf} (which is like {\tt msg} but adds a linefeed if necessary)
are \TPS functions for producing output. These functions take a sequence
of arguments, and evaluate and print out each argument in an appropriate format;
an argument t means go to a new line. See {\tt defutil msg}.

\item {\tt (msg (gwff1 . gwff))} will print out the correct representation
of the gwff, whereas {\tt (princ gwff1)} will just print its internal
representation.

\item To insert a call to runcount in the code:
{\tt msgf (\indexfunction{runcount})}

\item \indexfunction{stringdt} gives the time and date. \indexfunction{stringdtl} also inserts linefeeds

\item {\tt princ} often puts messages into a buffer.
To get them to print out, add the command {\tt {finish-output}}.
You may also have to do this when you use other
output commands, including msg. 

\item Windows (proofwindows, edwindows, vpwindows) all work by issuing a Unix shell
command which runs an xterm which, in turn, runs the Unix "tail" command 
recursively on an output file that \TPS creates by temporarily redirecting 
{\tt *standard-output*}. (Compare such commands as \indexcommand{SCRIPT} and
\indexcommand{SAVE-WORK}, which {\it permanently} redirect {\tt \indexother{*standard-output*}}.)
See the files 
\indexfile{tops20.lisp}, \indexfile{prfw.lisp}, \indexfile{edtop.lisp} and
\indexfile{vpforms.lisp} for more information.
\end{itemize}

\section{Compiling as much as possible}
In defining new \TPS objects, we often define as a side-effect new functions.
For example, when defining a new argument type, we define a testfn and a
getfn for that type, based on the values for those properties that are
given in the {\tt deftype\%} form. 

Currently, all such functions are compiled, by cleverly defining the definition
macros so that {\tt defun} forms are created during the compilation of a file.
If you define new categories that will create such functions, you will want
to do something similar, so that you aren't always running interpreted code.
See the files {\it argtyp.lisp} and {\it flavoring.lisp} for examples of how
this can be done.

\section{Writing New Code Without Making A Nuisance of Yourself}
\label{writing}

\begin{itemize}
\item Programmers should avoid referencing internal symbols of different
LISP packages.  If you are doing this, think about why it is
necessary.  Perhaps it is better to export the symbols, or rethink the
package structure.

\item Symbols should be exported before files containing them are compiled.
Otherwise you stand the risk of having those symbol-occurrences
interned in the wrong package.

\item Lisp macros can be very useful, but it is easy to overuse them.  It
can be very difficult to debug code that uses many macros, and because
there is no guarantee that macros will not be expanded when code is
loaded (and they are always expanded when compiled), modifying a macro
means recompiling every file in which it appears, which is quite a nuisance.

\item There are a multitude of functions in \tps, so one must be careful not
to inadvertently redefine a function or macro.  With the Lisp function
{\tt APROPOS}, you can check to see
whether a function name is already being used.
Use the TAGS table. See section ~\ref{finding}, above.

\item Try not to re-invent the wheel; look in all the likely places to see if some
of the code you need has already been written. If your new construct is
similar to an existing one, use {\tt grep -i} in the directory 
{\it /afs/andrew/mcs/math/TPS/lisp/} to find and examine all uses of the existing 
construct.

\item Remember that rules of inference should be written as {\it .rules} files and compiled 
with the \indexcommand{ASSEMBLE-RULE} command; if you modify the {\it .lisp} files 
directly, you run the risk of having your modifications accidentally overwritten
by future users.

\item When modifying copies of existing files, prior to installing them, 
rename the file temporarily (for example, preface the filename with your 
initials) so that if you compile your new code it won't overwrite the 
existing compiled file.

\item Don't install code until you've tested it! After installation, keep backup 
copies of the old files
in the {\it /home/theorem/project/old-source/} directory on gtps, and change their
extensions from {\it .lisp} to {\it .lisp-to-1997-jan-3} (or whatever). Delete
all Emacs backup files from the main lisp directory. Compile new code using
the CMU Common Lisp version of \TPS since that compiler is fussier than most.

\item Try to make sure that online documentation is included in all user
functions, argument types, etc. that you define. Also, you should at the very least 
put comments in your code; better yet, write some documentation for the manuals.

\item When a new part of \TPS is developed, an appropriate module should be defined in
{\it /afs/andrew/mcs/math/TPS/lisp/defpck.lisp}. If a new file is being added to 
an existing module, just add it to the list in defpck.lisp, make sure the correct
heading is on the file, and export the filename from 
{\it /afs/andrew/mcs/math/TPS/lisp/<package>.exp}. (Actually, the exporting should 
be done automatically by \tps, but it won't hurt to do it manually as well.)

\item If a new package or module has been added, it must go into all the build and
compile files for \ETPS and \tps. (See, for example,
{\it /afs/andrew/mcs/math/TPS/common/tps-compile.lisp}.)  In general, it should
go into the ends of the list of modules, so that definitions it
depends upon will be loaded first. If a new module is added, be sure to add it 
to the {\it \indexfile{facilities.lisp}} and {\it \indexfile{facilities-short.lisp}}
files, otherwise it won't show up in the facilities guide.

\item After installing new code, remember to change the patch files, the {\it tpsjobs} 
file and the {\it tpsjobs-done} file, and to send a mail message to the other 
people working on the program.
\end{itemize}

\section{\indexother{Debugging} Hints}

\begin{itemize}
\item Insert print commands in a temporary version of a file to see either 
which parts of the code are being used or what the current values of some
variables are.

\item Compile the file in several common lisps, especially in
cmulisp (or tps3cmu), and see if the error messages are helpful.

\item Try to reproduce the bug in a simpler form.

\item See in how many different contexts (such as different
matingsearch procedures) it arises, so you can isolate its essential
features.

\item Use the debugging features of your version of lisp (e.g. step and trace).

\item Change the values of the flags \indexflag{QUERY-USER}, \indexflag{MATING-VERBOSE},
\indexflag{UNIFY-VERBOSE}, \indexflag{TACTIC-VERBOSE}, 
\indexflag{OPTIONS-VERBOSE}, etc... to get more output.

\item Use the monitor. (See chapter ~\ref{monitor}.)

\item Use the lisp function {\tt plist} to inspect the property list of an object.
Use {\tt inspect} to see the values of the slots in a structure.

\item Errors in translation, or errors during verification of a merged jform ("The formula is not provable 
as there is no connection on the following path") are usually caused by merging. See the chapter on
merging, and in particular the note about using merge-debug.

\item The code in the file \indexfile{etrees-debug} can be useful for tracking
down bugs involving etrees.  (See subsection ~\ref{etrees}.)

\item Errors of the form "Wff operation <wffop> cannot be applied to labels of flavor <label>" are almost always
caused by attempting to use a wffop on a flavor for which the corresponding property is undefined. See the section
on flavors for more details.

\item Errors in structure-slot-accessor are often of the form "Structure for accessor <foo-slot> is not a <foo>".
For every structure <foo>, there is a test <foo-p>; use it! Of course, you should also work out how 
something that wasn't a <foo> managed to turn up at that point in the program anyway; often, it's an exceptional
case that you forgot to handle.

\item If the bug is new (for example, if you know it wasn't there last month), don't forget 
that the {\it tpsjobs-done} file lists all of the files which have been changed, along 
with the reasons for each change and the date of each change. The {\it tps/tps/old-source/} directory
should contain backup copies of the changed files. Failing that, snapshots of the entire 
lisp directory (in the form of gzipped tar files made after each rebuild) are stored 
in the {\it tps/tps/tarfiles/} directory. Use cload to restore the old copies of the most likely
culprit files into a core image, until the bug disappears; then use ediff to
compare the old and new files.
\end{itemize}

\section{Miscellaneous}

\subsection{Counting Flags}

One can count the number of flags in \TPS as follows:

%\begin{tpsexample}
\begin{verbatim}
[btps]/afs/andrew/mcs/math/TPS/lisp% grep -i defflag *.lisp > flagcount
[btps]/afs/andrew/mcs/math/TPS/lisp% ls -l flagcount
{\it Edit the file flagcount to eliminate lines which do not define flags}
[btps]/afs/andrew/mcs/math/TPS/lisp% wc flagcount
     210     421    8327 flagcount
{\it The number of lines (210} is the number of flags.)
[btps]/afs/andrew/mcs/math/TPS/lisp% rm flagcount
\end{verbatim}
%\end{tpsexample}

The above counts the number of flags defined in the source code. The
number currently present in a particular version of \TPS can be found as follows:

%\begin{tpsexample}
\begin{verbatim}
(defun discard (list)
	(if (null list) nil
	  (if (or (listp (car list)) (memq (car list) (cdr list)))
;; if it's a list, it's a subject name, and we don't want to count them.
;; if it appears later on, we don't want to count it twice.
;; (may need to use franz:memq rather than memq)
	    (discard (cdr list))
	      (cons (car list) (discard (cdr list))))))

(msg "TPS has " (length (discard global-flaglist)) " flags.")
\end{verbatim}
%\end{tpsexample}

\subsection{Dealing with X Fonts}

\begin{itemize}
\item To enable your computer to find the fonts, put into an appropriate 
(such as {\it .Xclients}) file: {\tt xset }fp /tps/fonts/+,
using the appropriate pathname in place of {\it /tps/fonts/}. The {\tt }fp+
adds the new directory at the start of the path, because we've had trouble 
in the past with old fonts with the same name being earlier on in the path.

\item {\tt xset q} shows the fonts.

\item {\tt xset fp-} takes them out of the fontpath.

\item {\tt xlsfonts} lists the fonts available
Because the font list is usually very long,
you may prefer to use
{\tt xlsfonts | grep <fontname>} to check whether the font <fontname> 
is available.

\item {\tt xfd -fn <fontname> \&} shows all the characters in <fontname>
\end{itemize}

Dan Nesmith built the symbol fonts by starting with
the vtsingle font, because xterm requires a font that is
exactly the same size as vtsingle.  However, every
character is now different from the original; they were created by hand-colouring 
the pixels. It is, however, easier to
edit an existing font than to create one from scratch.

The exact duplicates of the Concept ROM fonts are found in symfont1.bdf and
symfont2.bdf, for a total of 256 characters (including a normal-sized epsilon).
Unfortunately, because of the limits below on xterm and lisp, it was necessary to
leave out some of these characters when making a single font, now called
vtsymbold.   The galsymbold font was created by splitting the font vtsymbold
into 128 bitmaps, then using an X10 program that would automatically blow
each bitmap to the proper size, then manually adjusting a few of
the characters.

There is a real limitation of the xterm program in that you get only two
fonts, one for normal text and one for bold text.  We use the bold text
font for symbols, and switch back and forth between the fonts by sending
the appropriate escape symbols.  Though xterm would probably allow using
more than the "printable" characters, this appears very hard to get
lisp to do in general, that is, it's hard to figure out how to get lisp to
send these characters to the terminal.  This will probably never be
completely implementation-independent, because character sets are a very
unstable part of the lisp specification.

In \TPS you will only be able to get the symbols between 32 and 127, basically
because the lisp allows only those (without some kind of great hackery).

A few lines of attack for getting more characters suggest themselves:

\begin{enumerate}
\item Hack xterm to allow more than 2 fonts. (In fact, Dan has
done it to allow one more font, and thinks it would be possible to add up
to two more, for a total of one normal font and three symbol fonts.)
The disadvantage here would be having to distribute the new version of xterm,
and worrying about portability problems (which actually should be minimal,
but with lots of different machines and versions of X out there, not
predictable).  But this approach is fairly easy to get to from the current
state.

\item Give up on xterm.  There is a new version of gnu-emacs, version 19,
which allows the use of more than one font in a buffer.  You can then run
\TPS in a gnu-emacs buffer, and at the same time build in support for
command-completion, hypertext documentation, etc.  You can also get several
windows from a single emacs now, so you can still have the editor stuff
pop up a separate window.  It might also be easier to bind in support
for automatically running tex or scribe and displaying it.
This requires someone who can hack gnu-emacs lisp, but a lot of this
stuff has already been done by somebody, and it's just a matter of putting
it together.   In this case you could probably roll the fonts together into
a single large one.
\end{enumerate}
::::::::::::::
library-rewrite.tex
::::::::::::::
\chapter{Library}

% @comment(the old library documentation, which is misleading, is in library.mss)

The library commands are documented in the user manual.

A library can currently only occupy one directory (i.e. subdirectories
may not be used), although users are given the ability to refer additionally to a
common directory of basic definitions by using the \indexflag{BACKUP-LIB-DIR} flag.

Many library commands are essentially written as two copies of the same function, 
the first of which checks the default library directory and the second of which
checks the backup directory. The second piece of code is surrounded by {\tt unwind-protect}
commands in order to make sure that the \indexflag{DEFAULT-LIB-DIR} and \indexflag{BACKUP-LIB-DIR}
flags always end up correct. Users may not write to the backup directory.

The index for each library directory is stored in the {\it libindex.rec} file in that directory;
this file that is read every time the directory is changed. Objects are removed from the library 
by deleting them from the appropriate {\it .lib} file and removing their entry from the {\it libindex.rec}
file. This may result in a {\it .lib} file of zero length, in which case the file is deleted.

Objects which are loaded by the user are re-defined as \TPS objects; library objects of type 
MODE or MODE1 become \TPS modes, whereas gwffs and abbreviations each become both theorems
(of type {\it library}) and abbreviations. Notice that this blurs the distinction between a gwff and an 
abbreviation. Users are allowed to redefine \TPS theorems of type {\it library}, and their corresponding
abbreviations; theorems of other types may not be redefined (this is to prevent users from accidentally
overwriting standard abbreviations with their own library definitions).

Library definitions are parsed every time they are written, and this involves re-loading all of the
needed objects. Since the needed objects are often abbreviations, this will frequently result in 
their redefinition, and so \lisp will generate warning messages. If a large file is being re-parsed,
this can take a long time and produce a huge number of warnings.

::::::::::::::
macros.tex
::::::::::::::
% ----------------------------------------------------------------------
% Environments
% ----------------------------------------------------------------------

%\newenvironment{lfsig}{%
%   \par\vspace{\abovedisplayskip}\(\begin{array}{lcl@{\hspace{5em}}}}{%
%   \end{array}\)\par\vspace{\belowdisplayskip}\noindent\ignorespaces}
\newenvironment{lfsig}{%
   \par\vspace{\abovedisplayskip}\(\begin{array}{lcl}}{%
   \end{array}\)\par\vspace{\belowdisplayskip}\noindent\ignorespaces}

% \newenvironment{proofquote}{\begin{quote}\small}{\end{quote}}
% \newenvironment{proofquote}{\begin{minipage}{4.5in}}{\end{minipage}}
\long\def\proofbox#1{\[\fbox{\parbox{4.5in}{\small #1}}\]}

\newenvironment{progexample}{%
   \par\vspace{\abovedisplayskip}\(\begin{array}{@{\hspace*{2em}}l}}{%
   \end{array}\)\par\vspace{\belowdisplayskip}\noindent\ignorespaces}

% ----------------------------------------------------------------------
% Symbols
% ----------------------------------------------------------------------

\def\of{\mathrel{::}}

% --------------------
% Mini-ML (Chapter 1)
% --------------------

\def\z{\mbox{\bldf z\/}} % \def\z{\mbox{\cf z\/}}
\def\s{\mbox{\bldf s\/}} % \def\s{\mbox{\cf s\/}}
% \def\pred{\mbox{\cf pred\/}}
% \def\zerop{\mbox{\cf zerop\/}}
\def\fst{\mbox{\bldf fst\/}}
\def\snd{\mbox{\bldf snd\/}}
\def\llam{\mbox{\bldf lam\/}}
\def\llet{\mbox{\bldf let\/}}
\def\lletn{\mbox{\bldf let\/}\,\mbox{\bldf name}}
\def\lletv{\mbox{\bldf let\/}\,\mbox{\bldf val}}
\def\iin{\mbox{\bldf in\/}}
\def\fix{\mbox{\bldf fix\/}}

% evaluation

\def\evalsto{\mathrel{\hookrightarrow}}
% \def\ev#1{\mbox{\cf ev\_#1}}

% value proof

\def\value{\mbox{\it Value}}
% \def\val#1{\mbox{\cf val\_#1}}

% types

\def\nat{\mbox{\cf nat\/}}
\def\bool{\mbox{\cf bool\/}}
\def\cross{\times}
% \def\arrow{\mathrel{\rightarrow}} % in fpstd.

% typing rules

\def\vdml{\mathrel{\triangleright}}
\def\gvdml{\Gamma \vdml}
% \def\tp#1{\mbox{\cf tp\_#1}}

% conversion

% \def\conv{\simeq}
\def\conv{\equiv}
\def\freein{\mathrel{\mbox{free in}}}
\def\notfreein{\mathrel{\mbox{not free in}}}

% in exercises and language extensions

\def\ccase{\mbox{\bldf case\/}}
\def\oof{\mbox{\bldf of\/}}
\def\bar{\mathrel{\mbox{\tt |}}}
\def\to{\Rightarrow}
% \def\pair{\mbox{\bldf pair\/}}
\def\split{\mbox{\bldf split\/}}
\def\as{\mbox{\bldf as\/}}
\def\inl{\mbox{\bldf inl\/}}
\def\inr{\mbox{\bldf inr\/}}

\def\double{\mbox{\it double}}

\def\vnat{v_{\mbox{\scriptsize \rm nat}}}     % \sf not available this size!
\def\enat{e_{\mbox{\scriptsize \rm nat}}}     % \sf not available this size!
\def\mn#1{[\![#1]\!]}

% ---------------
% LF (Chapter 2)
% ---------------

\def\rep#1{\lceil #1 \rceil} % AMS symbol improvement:
% \def\rep#1{\ulcorner #1\urcorner}  % I don't have ulcorner and urcorner -- ceb
% \def\drep#1{\ulcorner\renewcommand{\arraystretch}{1}\arraycolsep 5pt
% \begin{array}[t]{c}#1\end{array}\urcorner}
\def\lf#1{\mbox{\rm #1}}
\def\type{\mbox{\rm type}}
\def\vdlf{\vd} % \def\vdlf{\vd^{\!\!\rm LF}}
\def\gvdlf{\Gamma \vdlf}
\def\vdslf{\vdlf_{\!\!\scriptscriptstyle \Sigma}}
\def\gvdslf{\Gamma \vdslf}
\def\isakind{\mbox{\it Kind}}
\def\isactx{\mbox{\it Ctx}}
\def\isasig{\mbox{\it Sig}}
% \def\valid{\mbox{\it Valid}}
% \def\betaconv{=_\beta}

\def\exp{\lf{exp}}
% \def\canon{\mathrel{:_c}} % \def\canon{\mathrel{\Rightarrow}}
% \def\atm{\mathrel{:_a}}   % \def\atm{\mathrel{\leadsto}}
\def\canon{\Uparrow}
\def\can{\canon}
\def\atm{\downarrow}

\def\vdsub#1{\vd_{\!\! \scriptscriptstyle #1}}
\def\gvdsub{\Gamma \vdsub}
\def\unrep#1{\llcorner #1\lrcorner}

\def\eval{\lf{eval}}
\def\tp{\lf{tp}}
\def\dvdml{\Delta \vdml}

\def\lfvalue{\lf{value}}
\def\redv{\Longrightarrow}
\def\vs{\lf{vs}}
\def\redtriv{\stackrel{\it triv}{\redv}}

\def\closed{\mbox{\it Closed}}
\def\open{\mbox{\it Open}}
\def\lfclosed{\lf{closed}}

\def\limplies{\mathbin{\supset}}

\def\inst{\preceq}
\def\vdsml{\mathrel{\triangleright\!\triangleright}}

% -----------------------
% Compilation (Chapter 6)
% -----------------------

\def\shift{\mathord{\uparrow}}

\def\trans{\mathrel{\leftrightarrow}}
\def\vtrans{\mathrel{\Leftrightarrow}}
\def\fevalsto{\evalsto}

\def\lletnp{\mbox{\bldf let\/}'\,\mbox{\bldf name}}
\def\lletvp{\mbox{\bldf let\/}'\,\mbox{\bldf val}}

\def\ssucc{\mbox{\cf succ\/}}

\def\KS{{\it KS}}
\def\amp{\mathop{\&}}
\def\done{{\it done}}
\def\St{{\it St}}
\def\goesto{\mathrel{\Longrightarrow}}
\def\goestos{\stackrel{*}{\Longrightarrow}}
\def\goestoss{\stackrel{*}{\makebox[0pt][l]{$\Longrightarrow$}\Longrightarrow}}
% \def\evl{\mbox{\it Evl}}

\def\return{\mbox{\bldf return}}
\def\answer{\mbox{\bldf answer}}
\def\init{\mbox{\bldf init}}
\def\cpm{\mathop{\diamond}}
\def\app{\mbox{\bldf app}}
\def\cevalsto{\stackrel{c}{\hookrightarrow}}
\def\corr{\sim}

% ------------------------------
% Natural Deduction (Chapter 7)
% ------------------------------


\def\andi{\land{\rm I}}
\def\andel{\land{\rm E}_{\rm L}}
\def\ander{\land{\rm E}_{\rm R}}
\def\oril{\lor{\rm I}_{\rm L}}
\def\orir{\lor{\rm I}_{\rm R}}
\def\ore{\lor{\rm E}}
\def\impi{\mathord{\limplies{\rm I}}}
\def\impe{\mathord{\limplies{\rm E}}}
\def\noti{\mathord{\lnot{\rm I}}}
\def\note{\mathord{\lnot{\rm E}}}
\def\topi{\top{\rm I}}
\def\bote{\mathord{\bot{\rm E}}}
\def\foralli{\forall{\rm I}}
\def\foralle{\forall{\rm E}}
\def\existsi{\exists{\rm I}}
\def\existse{\exists{\rm E}}

\def\botc{\mathord{\bot_C}}
\def\eec{\lnot\lnot_C}
\def\exm{{\rm XM}}

\def\ndredl{\quad\Longrightarrow_{L}\quad}
\def\rf{\quad\leadsto\quad}
\def\rfand{\quad\mbox{\it and}\quad}
\def\rfdone{\mbox{\it done}}
\def\rfs{\quad\stackrel{*}{\leadsto}\quad}

\def\lfi{\lf{i}}
\def\lfo{\lf{o}}
\def\lfnd{\lf{nd}}
\def\lfnj{\lf{nj}}
\def\lfnk{\lf{nk}}
\def\repA{\rep{A}}
\def\repB{\rep{B}}

\def\pfof{\mathrel{:\!\cdot}}

\def\lredl{\longrightarrow_{L}}
\def\lred{\longrightarrow}
\def\Arrow{\Rightarrow}
\def\abort{\mbox{\bf abort}}

\def\tpj{\mathrel{\triangleright}}
\def\gtpj{\Gamma \tpj}

\newsavebox{\tempsrc}

\def\bigrep#1{\savebox{\tempsrc}{$\displaystyle
#1$}\raisebox{\ht\tempsrc}{$\ulcorner$}\;\usebox{\tempsrc}\;\raisebox{\ht\tempsrc}{$\urcorner$}}

\def\nati{{\rm NI}}
\def\nate{{\rm NE}}
\def\prim{\mbox{\bldf prim\/}}

\def\ttrue{\mbox{\cf true\/}}
\def\ffalse{\mbox{\cf false\/}}
\def\iif{\mbox{\bldf if\/}}
\def\tthen{\mbox{\bldf then\/}}
\def\eelse{\mbox{\bldf else\/}}

\def\ue{\langle\, \rangle}
\def\extr#1{\left|#1\right|}
\def\iso{\cong}

% ------------------------------
% Logic Programming (Chapter 8)
% ------------------------------

\def\whr{\stackrel{whr}{\longrightarrow}}
\def\dvd{\Delta \vd}
\def\id{{\it id}}

\def\seq{\mathrel{\longrightarrow}}
\def\imm{\gg}
\def\sequ{\stackrel{u}{\seq}}
\def\dsequ{\Delta \sequ}

% \def\GL{{\cal GL}}
% \def\IM{{\cal IM}}
\def\SS{{\cal S}}
\def\II{{\cal I}}
\def\RR{{\cal R}}

\def\rsd{\mathrel{\backslash}}
\def\seqr{\stackrel{r}{\seq}}
\def\dseqr{\Delta \seqr}

\def\dseqc{D \stackrel{c}{\seq}}

% \def\gsequ{\Gamma \sequ}
% \def\gseqr{\Gamma \seqr}

% ----------------------------------------------------------------------
% Deductions
% ----------------------------------------------------------------------

\def\CC{{\cal C}}
\def\DD{{\cal D}}
\def\EE{{\cal E}}
\def\PP{{\cal P}}
\def\QQ{{\cal Q}}
\def\UU{{\cal U}}

\def\above#1#2{\begin{array}[b]{c}\relax #1\\ \relax #2\end{array}}
\def\abovec#1#2{\begin{array}{c}\relax #1\\ \relax #2\end{array}}

\def\cian#1#2#3{\ctr{\ianc{#1}{#2}{#3}}}
\def\cibn#1#2#3#4{\ctr{\ibnc{#1}{#2}{#3}{#4}}}

\def\hypo#1#2{\begin{array}[b]{c}\relax #1\\ \vdots \\ \relax #2\end{array}}
\def\hypoc#1#2{\begin{array}{c}\relax #1\\ \vdots \\ \relax #2\end{array}}
\def\hypol#1#2#3{\begin{array}[b]{c}\relax #1\\ #2 \\ \relax #3\end{array}}
\def\hypolc#1#2#3{\begin{array}{c}\relax #1\\ #2 \\ \relax #3\end{array}}

\def\ctr#1{\begin{array}{c} #1\end{array}}

\newsavebox{\tempded}
\newsavebox{\tempdedA}
\newsavebox{\tempdedB}

% ----------------------------------------------------------------------
% Indexing and Cross-Referencing
% ----------------------------------------------------------------------


% Chad's Macros
\def\kolm{\rightarrow}
\def\kolmand{\land{\rm K}}
\def\kolmimp{\limplies{\rm K}}
\def\kolmor{\lor{\rm K}}
\def\kolmnot{\lnot{\rm K}}
\def\kolmtrue{\top{\rm K}}
\def\kolmfalse{\bot{\rm K}}
\def\kolmforall{\forall{\rm K}}
\def\kolmexists{\exists{\rm K}}

\def\dne{\lnot\lnot{\rm R}} 
\def\dni{\lnot\lnot{\rm X}}
\def\tnr{\lnot\lnot\lnot{\rm R}}
\def\dnfe{\lnot\lnot\bot{\rm R}}

\def\nk{\vdash_{\rm NK}} 
\def\nj{\vdash_{\rm NJ}} 
\def\nd{\vdash}

\def\ail{{a_1,\ldots,a_n}}
\def\pol{{p_1,\ldots,p_m}}
%\def\kolmhyp{p_1\kolm \lnot\lnot p_1,\ldots,p_m\kolm \lnot\lnot p_m}
\def\hyl{H_1,\ldots,H_k}
\def\hypl{\nk H_1,\ldots,\nk H_k}
\def\h\\begin{array}{c}\\relax #1\\\\ \\relax #2\\end{array}}"
""
"\\def\\cian#1#2#3{\\ctr{\\ianc{#1}{#2}{#3}}}"
"\\def\\cibn#1#2#3#4{\\ctr{\\ibnc{#1}{#2}{#3}{#4}}}"
""
"\\def\\hypo#1#2{\\begin{array}[b]{c}\\relax #1\\\\ \\vdots \\\\ \\relax #2\\end{array}}"
"\\def\\hypoc#1#2{\\begin{array}{c}\\relax #1\\\\ \\vdots \\\\ \\relax #2\\end{array}}"
"\\def\\hypol#1#2#3{\\begin{array}[b]{c}\\relax #1\\\\ #2 \\\\ \\relax #3\\end{array}}"
"\\def\\hypolc#1#2#3{\\begin{array}{c}\\relax #1\\\\ #2 \\\\ \\relax #3\\end{array}}"
""
"\\def\\ctr#1{\\begin{array}{c} #1\\end{array}}"
""
"\\newsavebox{\\tempded}"
"\\newsavebox{\\tempdedA}"
"\\newsavebox{\\tempdedB}"
""
"% ----------------------------------------------------------------------"
"% Indexing and Cross-Referencing"
"% ----------------------------------------------------------------------"
""
""
"% Chad's Macros"
"\\def\\kolm{\\rightarrow}"
"\\def\\kolmand{\\land{\\rm K}}"
"\\def\\kolmimp{\\limplies{\\rm K}}"
"\\def\\kolmor{\\lor{\\rm K}}"
"\\def\\kolmnot{\\lnot{\\rm K}}"
"\\def\\kolmtrue{\\top{\\rm K}}"
"\\def\\kolmfalse{\\bot{\\rm K}}"
"\\def\\kolmforall{\\forall{\\rm K}}"
"\\def\\kolmexists{\\exists{\\rm K}}"
""
"\\def\\dne{\\lnot\\lnot{\\rm R}} "
"\\def\\dni{\\lnot\\lnot{\\rm X}}"
"\\def\\tnr{\\lnot\\lnot\\lnot{\\rm R}}"
"\\def\\dnfe{\\lnot\\lnot\\bot{\\rm R}}"
""
"\\def\\nk{\\vdash_{\\rm NK}} "
"\\def\\nj{\\vdash_{\\rm NJ}} "
"\\def\\nd{\\vdash}"
""
"\\def\\ail{{a_1,\\ldots,a_n}}"
"\\def\\pol{{p_1,\\ldots,p_m}}"
"%\\def\\kolmhyp{p_1\\kolm \\lnot\\lnot p_1,\\ldots,p_m\\k::::::::::::::
manual.tex
::::::::::::::
% The outline for this is copied from Matt Bishop's Thesis.
\documentclass[11pt]{amsbook}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage[T1]{fontenc} % encoding for european computer modern, loaded below
\usepackage[all]{xy} % commutative diagrams
\usepackage{bbold} % blackboard bold font
\usepackage{named} % the named bibliography style
\newfont{\bsc}{ecxc1200} % european computer modern, used for bold small caps in appendix a.
\makeindex % generate index data
\input tps.tex %requires TPSINPUTS to have the right path; these are in $tps/doc/lib
\input ndmacros.tex
\input vpd.tex
\newtheorem{definition}{Definition}[section] % a definition is labelled Definition and is numbered by section
\newtheorem{lemma}[definition]{Lemma} % lemmas are labelled Lemma and use the same numbering as definitions.
\newtheorem{example}[definition]{Example} % etc...
\newtheorem{theorem}[definition]{Theorem}
\newtheorem{algorithm}[definition]{Algorithm}
\newtheorem{proposition}[definition]{Proposition}
\newtheorem{conjecture}[definition]{Conjecture}
\numberwithin{figure}{chapter} % figures are 3.1, 3.2, 5.1,... rather than 1, 2, 3,...
\newenvironment{notation}{{\sc Notation}\it\ }{\rm}
\newenvironment{remark}{{\sc Remark}\ }{}
\newcommand{\missing}{\par{\bf *****INSERT PROOF HERE*****}\par} % a little reminder that something's missing...
\newcommand{\reml}{[\![}        % Left valuation-bracket, meaning ``remove this''
\newcommand{\remr}{]\!]}        % Right ditto
\newcommand{\uglylabel}[1]{{\hspace{-0.35in}\mbox{\small\bf #1}\hspace*{0.3in}}}
\newcommand{\fixme}{\marginpar[{\bf fix me! $\longrightarrow$}]{{\bf $\longleftarrow$ fix me!}}} % arrows in the margins...
\newcommand{\nchoose}[2]{$($\raisebox{-2pt}{$\stackrel{#1}{\scriptstyle #2}$}$)$} % in-line printing for `n choose k'.
\newcommand{\cal}{\mathfrak} % calligraphic font is really fraktur.
\newcommand{\foobar}[1]{\makebox[1.5em]{\rule[-.5ex]{0cm}{2ex}#1}} % used to label lattices in the results section
\newcommand{\comment}[1]{} % mimic the Scribe `comment' command.
\newcommand{\alg}[1]{{\em #1}\index{#1 algorithm}} % how to print the name of an algorithm and index it as well
\newcommand{\mmm}[2]{\alg{Merge}$(#1,#2)$} % \mmm{a}{b} prints Merge(a,b).
\newcommand{\flag}[1]{{\sc #1}\index{#1}} % flags are in small caps, and indexed
\newcommand{\bflag}[1]{{\bsc #1}\index{#1}} % bold flags, for appendix a.
\newcommand{\fval}[1]{{\sc #1}} % flag values in small caps...
\newcommand{\bfval}[1]{{\bsc #1}} % ...or bold ditto, in {description} contexts
\newcommand{\ftyp}[1]{{\sc #1}} % flag types, also in small caps.
\newcommand{\mexpr}[1]{{\tt #1}\index{#1}} % how to index a command (\command is already taken)
\newcommand{\simj}{\sim_{\!_J}} % twiddle-J.
\newcommand{\ssim}{\sim\!\!} % not, but with less space after it.
\newcommand{\nconv}{\stackrel{\sim}{\longrightarrow}}
\newcommand{\bconv}{\stackrel{\scriptscriptstyle\beta}{\displaystyle\longrightarrow}}
\newcommand{\nquiv}{\stackrel{\scriptscriptstyle\beta\sim}{\displaystyle =}}
\newcommand{\bquiv}{\stackrel{\beta}{\displaystyle =}}
\newcommand{\dquiv}{\stackrel{\scriptscriptstyle {\rm def}}{\displaystyle =}}
\newcommand{\rquiv}[1]{\stackrel{\scriptscriptstyle #1}{\displaystyle \longrightarrow}}
\newcommand{\cons}{\!^\frown} % list cons, like the ^ in a^(b)
\newcommand{\mystrut}{\rule[-0.75ex]{0ex}{2.5ex}}
\def\TPS{{\sc Tps3 }}
\def\ETPS{{\sc Etps }}
\def\OMEGA{\mbox{$\Omega${\sc mega}}}
\def\tps{{\sc Tps3}}
\def\LOUI{{\sc L$\Omega$UI}}
\def\SPASS{{\sc Spass}}
\def\OTTER{{\sc Otter}}
\def\PROTEIN{{\sc Protein}}
\def\CLAM{{\rm CL\raise.5ex\hbox{\sc a}M}}

% This is for Scribe compatibility, so that for some
% Scribe commands we can simply change the @ to a \
\def\wt{\sf}
\def\w{\sf}
\def\lisp{{\sc lisp}}
\newcommand{\indexother}[1]{#1\index{#1}}
\newcommand{\indexflag}[1]{#1\index{#1, Flag}}
\newcommand{\indexcommand}[1]{#1\index{#1, Command}}
\newcommand{\indexedop}[1]{#1\index{#1, EdOp}}
\newcommand{\indexmexpr}[1]{#1\index{#1, MExpr}}
\newcommand{\indexparameter}[1]{#1\index{#1, Parameter}}
\newcommand{\indexdata}[1]{#1\index{#1, Data}}
\newcommand{\indexfile}[1]{#1\index{#1, File}}
\newcommand{\indexfunction}[1]{#1\index{#1, Function}}
\newcommand{\indexstyle}[1]{#1\index{#1, Style}}
\newcommand{\indextypes}[1]{#1\index{#1, Type}}
\newcommand{\indexargtypes}[1]{#1\index{#1, Argument Type}}
\newcommand{\indexSyntax}[1]{#1\index{#1, Syntax}}
\newcommand{\indexProperty}[1]{#1\index{#1, Property}}
\newcommand{\indexData}[1]{#1\index{#1, Data}}
\newenvironment{Example}{ \\}{\\}

\begin{document}
\bibliographystyle{alpha}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%List of NSF grants below copied from tpsdoc.lib
\frontmatter
\title{\TPS Programmer's Guide (\TeX Version)\thanks{
copyright \copyright 2000. Carnegie Mellon University.  All rights reserved.)
This manual is based upon work supported by
NSF grants MCS81-02870, DCR-8402532, CCR-8702699, 
CCR-9002546, CCR-9201893, CCR-9502878, CCR-9624683, CCR-9732312,
and a grant from the Center for Design of Educational Computing,
Carnegie Mellon University. Any opinions, findings, and conclusions or
recommendations are those of the author(s) and do not necessarily reflect
the views of the National Science Foundation.}
}
\author{Peter B. Andrews \\
Dan Nesmith \\
Frank Pfenning \\
Sunil Issar \\
Hongwei Xi \\
Matthew Bishop \\
Chad E. Brown}
\date{Working Edition \\
\today}

\maketitle

\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\mainmatter
%\input{results}
%\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter*{Preface}
\pagestyle{plain}
The following is a \TeX (actually, \LaTeX) version of the \TPS Programmer's Guide.
The original version is in Scribe format.
\mainmatter\pagestyle{headings}
\input{intro} 
\input{tps-struct}
\input{top-levels}
\input{mexprs}
\input{wffrep}
\input{wffprint}
\input{vpforms}
\input{flavors}
\input{wffops}
\input{help}
\input{flags}
\input{monitor}
\input{rules}
\input{etps}
\input{mating}
\input{unification}
\input{tactics}
\input{translation}
\input{library-rewrite}
\input{teach}
\input{grader}

\backmatter

\bibliography{logictex} %requires BIBINPUTS to point to $tps/doc/lib
\printindex

\end{document}

::::::::::::::
mating.tex
::::::::::::::
\chapter{Mating-Search}

The top level files for matingsearch are:
{\it mating-dir.lisp} for ms88, 
{\it ms90-3-top.lisp} for ms90-3,
{\it option-tree-search.lisp} for ms89 and ms90-9, {\it ms91-search.lisp} for
ms91-6 and ms91-7, and {\it ms92-9-top.lisp} for ms92-9 and {\it ms93-1.lisp} 
for ms93-1.  The lisp files with prefix {\it ms98} are those used by ms98-1.
The code for \indexcommand{GO} in {\it mating-top.lisp} shows 
what the main functions are.

There are a lot of comments about the workings of the code embedded in the
lisp files; in particular there is an outline of ms90-3 at the top of
{\it ms90-3-top.lisp}.

\section{Data Structures}

See the section on flavors and labels (section ~\ref{labels}) for a discussion
of some relevant information about the data structures below. Among other things, 
that section has the definition of the flavor "etree".

\subsection{Expansion Tree}
\label{etrees}

The data structure \indexData{etree}, defined in 
\indexfile{etrees-labels.lisp}, has the following properties:

\begin{enumerate}
\item \indexother{name}: the name of the etree. We can use this attribute to identify
which kind of structure an etree is.

\item \indexother{components}: is a list which contains all children of the etree. 
The children of an etree are also etrees. We could use this attribute to check whether an 
etree is a leaf, true, or false. 

\item \indexother{positive}: 
tells us whether the formula which an etree represents (which is the formula 
given by \indexfunction{get-shallow}) appears positively or negatively in the 
whole formula. This will be used to compute the vpform of the whole formula.
(The vpform of a subformula may be not the same as the corresponding part of it 
in the whole formula because the "positive" property
of the subformula is dependent on the context.)

\item \indexother{junctive}: can be used for printing the vpform. 
This attribute is linked tightly with the "positive" attribute, and has to do with
whether the node acts as neutral, conjunction or disjunction.

\item \indexother{free-vars}: is a list, containing the free variables in whose scope 
the node appears. When you skolemize a formula, you should use this attribute.
 
\item \indexother{parent}: is the parent of this etree.

\item \indexother{predecessor}: this slot tells you the leaf name from which the current
etree was deepened. It is mainly used for handling skolem constants.

\item \indexother{status}: has little to do with the system as currently 
implemented, but you should be
careful when you are creating commands which will change the variable 
{\tt current-topnode}. You have two choices:
\begin{enumerate}
\item Change the value of {\tt auto::ignore-status} to nil. Then you need
not worry about this attribute. Of course,
what you are doing may then not be compatible with the future versions
of the system. This is highly discouraged.

\item When you want to create new nodes or change some nodes in the {\tt current-topnode}
make the corresponding changes in the attribute {\tt statuses} of {\tt current-eproof}, 
which is a hash-table. Don't forget this, otherwise your new commands won't work.
\end{enumerate}
\end{enumerate}

The file \indexfile{etrees-debug} contains functions useful for debugging
code dealing with etrees.  
The function \indexfunction{check-etree-structure}
recursively checks structural properties of an etree,
and the function \indexfunction{check-etree-structure-break}
calls \indexfunction{check-etree-structure} and calls a break
if the etree fails the structural test.
The idea is that one can temporarily insert 
\begin{verbatim}
(check-etree-structure-break <etree>) 
\end{verbatim}
in suspicious parts of the code
to find out when an etree loses its integrity.  If the etree does not
have structural integrity, a break is called, sending the user (programmer)
to the debugger.  If one wants to insert this in several places in the code,
one may want to include a message as in
\begin{verbatim}
(check-etree-structure-break <etree> "unique identifying message")
\end{verbatim}
to identify which caused the break.

\subsection{The Expansion Proof}
In the mate toplevel, we have an \indexother{expansion proof} stored in the 
special variable
\indexother{current-eproof}, which is an \indexData{eproof}-structure. 
\indexother{current-eproof}
has a attribute \indexData{etree}, whose value is often used to update
variable \indexother{current-topnode}.

Actually, a whole formula is represented by a tree, each node of which is an 
\indexData{etree}.
At first, \indexother{current-topnode} is the root of the tree. 
Each node in the tree can be one of the following structures,
all of which are derived from the structure \indexData{etree}, described 
above. We note only the differences between these structures and etrees.

\begin{enumerate}
\item \indexother{econjunction} is just an etree without 
any additional new attributes. \indexother{components} is a list containing two 
elements, and \indexother{junctive} should be {\tt dis} or {\tt con}.

\item \indexother{edisjunction} is like \indexother{econjunction}.

\item \indexother{implication} is like \indexother{econjunction}

\item \indexother{negation} is just an etree. \indexother{components} contains one element
and \indexother{junctive} is {\tt neutral}.

\item \indexother{skolem} is an etree with two additional attributes:
\begin{enumerate}
\item \indexother{shallow}: contains the shallow formula that the attribute 
\indexother{skolem} represents.
Never forget to make the corresponding changes in it if you have changed some 
other parts of this node; otherwise the proof cannot be transformed into natural 
deduction style by \indexcommand{etree-nat}
since the function \indexfunction{get-shallow} would not work normally.

\item \indexother{terms}: is a \indexData{skolem-term} structure, containing
a term replacing the original variable, and something else.
\end{enumerate}

\item \indexother{expansion} is an etree with three additional properties:
\begin{enumerate}
\item \indexother{shallow}: is the same as in \indexother{skolem}.
\item \indexother{terms}: is an \indexData{exp-var} structure, containing the 
expansion variable for this expansion.
\item \indexother{prim-vars}
\end{enumerate}

\item \indexother{leaf} is an etree with the additional attribute \indexother{shallow},
as in \indexother{skolem}, above. The \indexother{components}, \indexother{junctive}
and \indexother{predecessor} attributes of \indexother{leaf} are all {\tt nil}.

\item \indexother{true}

\item \indexother{false}
\end{enumerate}


\subsection{Other Structures}
\begin{itemize}
\item A \indexData{mating} has certain attributes:
\begin{enumerate}
\item A set of connections

\item A unification tree
\end{enumerate}

\item A \indexData{unification tree} is a tree of nodes.

\item A \indexData{uni-term} is an attribute of a node (which is a structure); it is 
a set of disagreement pairs.

\item A \indexData{failure record} is a hashtable. 
MS88 (and the other non-path-focused procedures) uses the failure 
record.  MS90-3 (and the other path-focused procedures) does not use it (this is one reason
why, when \TPS abandons a mating and later returns to the partially completed 
eproof, ms91-6 continues approximately where it left off and ms91-7 does not).
Links (which all occur in the connection graph) are represented as numbers,
so sets of links are just ordered lists of numbers, and one can
efficiently test for subsets. 
Given a new partial mating M, \TPS just
looks at all the entries in the failure record to see if any of them
are subsets of M. 

\end{itemize}

\section{Skolemization}

There are three \indexother{skolemization} procedures in \TPS; SK1, SK3 and NIL.
(Actually, the latter is not skolemization at all, but the selection nodes method from Miller's thesis. 
However, it still uses skolem constants internally.) 
The flag \indexflag{SKOLEM-DEFAULT} decides which one will be used in a proof, and
the help message for that flag explains the difference.

We assume familiarity with the way that SK1 is handled in TPS. SK3 is broadly 
similar; the only difference between the two is in the function \indexfunction{create-skolem-node}, where
the skolem variables are chosen differently.

NIL, the selection node method, is very different. Selections are represented as Skolem
constants with no arguments, and we now describe the additional machinery needed to make the search 
procedure work in this case.

During simplification (in unification), the requirement that a certain relation should be acyclic
is checked. The exact statement of this relation is given in Miller's thesis; we implement it
(roughly speaking) as a requirement that no substitution term for an expansion variable should 
contain any of the selections which occur below that variable.

Extra slots, called {\tt exp-var-selected} on expansion variables and {\tt universal-selected} on 
universal jforms, are used to record all of the 
selections below each quantifier. This is used in the unification check, and more crucially
in path-focused duplication (since skolem terms are stripped out of the jform, this is our only 
way to remember where they were).

In SK1 and SK3, duplicating a quantifier above a skolem term produces a new skolem term 
consisting of the same skolem constant applied to different variables. In NIL, we obviously 
can't use the same skolem constant everywhere (consider EXISTS X FORALL Y . P X IMPLIES P Y ; 
if we persistently select the same Y every time we duplicate X, the proof will fail). This 
has two major consequences:

\begin{itemize}
\item Path-focused duplication has to be changed. We can no longer duplicate implicitly by changing the
name of the quantified variable; we must now make a copy of the entire scope of the 
quantifier and descend into it, renaming all the selections as we go. These copies are stored
in a chain using the universal-dup slot of the jform (so the universal-dup of the top jform
contains the first copy, whose universal-dup contains the second copy, and so on). These
duplications are preserved during backtracking, in case they are needed again later; we 
use universal-dup-mark to remember how many of them are "really" there.

\item The procedure for expanding the etree after ms90-3 finishes a search has to be completely
replaced. The old procedure relied on the fact that the names of skolem constants never changed,
and so it was possible to attach all of the expansion terms to the jform and then duplicate and
deepen the etree while applying the appropriate substitutions. The names of selections {\it do}
change; this makes the substitutions incorrect (because they will contain the names of old selections).
So we use ms90-3-mating and dup-record to duplicate the etree directly, and then add the
correct connections to it using ADD-CONN. This procedure is probably still buggy.
\end{itemize}

\section{Expansion Tree to Jform Conversion}
In {\it ms90-3-node}, the jform is computed directly from the etree without using 
the jform which may be stored with the etree. (It is not clear where or whether
that jform is used; it might be part of the interactive system.)

\indexfunction{msearch} does the search. It returns 
{\tt (dup-record ms90-3-mating unif-prob)}.
{\tt unif-prob} represents the solution to the unification problem, perhaps
as a substitution stack.
This triple is then handed to the processes that translate things back to
an expansion proof, call merging, and then translate to 
a natural deduction proof.
\indexfunction{msearch} looks at the flag \indexflag{order-components}; read
the help message for this flag for more information.

Each literal is a jform. One of its attributes is a counter
which gets adjusted to count how many mates that literal has; this is
compared with \indexflag{max-mates}.
The current jform being worked on is
essentially represented as a stack which is passed around as an 
argument. Indices are associated
with outermost variables which are implicitly duplicated. These
indices are also associated with literals in the scope of these
quantifiers to keep track of what copy of the literal is being mated.
It is only when unification is called that these indices are actually
attached to the variables to construct the terms unification must work
on.  (The functions \indexfunction{check-conn} and \indexfunction{conn-unif-p} in 
the file {\it ms90-3-path-enum}, and
related functions in that file, may be relevant here.)

\section{Path-Enumerator}

\subsection{Duplication Order}
Along a path the procedure stops at the first eligible universal
jform. A slot {\it dup-t} in a universal jform tells whether it is
eligible. Then it starts from there to find the innermost
universal jform on the path under the currently picked one, and 
uses the innermost one as its candidate for next duplication. This
is fulfilled by calling function {\it find-next-dup}.

When testing, please set flag max-dup-paths to an appropriate value
so that you can suppress some unnecessary quantifier duplications.
It may save a lot of your searching time and make you aware if
you are on the right track. Always duplicating innermost quantifiers
has the following advantages.
1) producing shorter and clearer proofs, and
2) lowering the values of flags max-search-depth, max-mates, and num-of-dups,
sometimes.

\subsection{Backtracking}
When bactracking starts, the search procedure removes the
last added connection. A path attached to the connection tells
the procedure where it should pick up the search. This works
efficiently since the following claim is almost always true:
With the help of disjunction heuristic, the number of paths
used to block jform is often a very small fraction of the
whole paths in the jform. This means that it is not a big
burden to carry the paths around all the time during searching.
The advantage is that the procedure knows exactly where it is
without having to do heavy computation by using the information
given by the current mating. To make this work, also carried with a path
is an environment, which stores the indices and (partial) substitutions
for the variables in the path.

\section{Propositional Case}
In the file {\it mating-dir.lisp}, you can see that the function 
\indexfunction{ms-director}
checks whether there are any free variables in \indexother{current-eproof}
(seealso eproof, current-eproof)
in order to decide whether to call \indexfunction{ms} or 
\indexfunction{ms-propositional};
if there are no free variables in \indexother{current-eproof}, ms-propositional is
called.

\subsection{Sunil's Propositional Theorem Prover}

The original files for Sunil's fast propositional calculus theorem-prover
are in \\
{\it /home/theorem/project/tps-variants/si-prop/}. {\tt qload} these files, go into the
editor, and make the edwff the example you wish to run. Within the editor
{\tt (test)} runs the program using edwff as argument. When it is done,
{\tt (test1)} shows the mating it found.

Most of the code in the above directory is now a permanent part of \TPS;
the function \indexfunction{prop-msearch} can be called from the {\tt mate} top 
level, and will display a correct mating for propositional jforms. The 
way to call it is: {\tt (auto::prop-msearch (auto::cr-eproof-jform))} (the latter
function is the internal name for \indexcommand{CJFORM}). For some reason, 
the propositional theorem prover is never used, except to reconstruct the 
mating after a path-focused duplication procedure has found it.

\section{Control Structure and Interface to Unification}

The non-path-focused-duplication (npfd) search procedures have
a \indexother{connection graph}, but the pfd procedures do not; the latter just
apply simpl to decide whether literals may be mated.
Sunil's disjunction heuristic (see below) is implemented for pfd search
procedures, but not for npfd.

The non-path-focused-duplication search procedures break a
jform with top-level disjuncts into separate problems, but the
path-focused-duplication search procedures do not.

When searching for a way to span a path, \TPS runs down the path
from the top, and considers each literal. As a mate for that literal, it
considers each literal which precedes it on the path.

When \TPS considers adding an essentially ffpair (pair of
literals which each start with a variable when one ignores any
negations) to the mating, it simultaneously considers both
orientations (choices for which literal will be negative and which
positive) of the ffpair.  Roughly speaking, it does this by putting a
disagreement pair corresponding to the ffpair into the leaves of the
unification tree, and proceeding with the unification process.  If
this process encounters a disagreement pair of the form <{\bf A},
\verb+~+{\bf B}>, where {\bf A} starts with a constant but {\bf B} does not, it
replaces this pair with {\w <\verb+~+{\bf A}, {\bf B}>} and continues. In this
way it finds whichever substitution works in a very economical
fashion.  When a success node is found for a complete mating, the
associated substitution determines the orientation of the ffpair in
the mating.

Here is some more detail about how this is actually implemented.
When TPS decides to mate a pair {L, K} of literals (which it considers	
as an unordered pair), it seeks to unify $\verb+~+\;$L with K, where L 
occurred before K on the path. Whenever the unification process
encounters a double negation, it deletes it. (Thus, in the case of a
first-order problem, TPS quickly starts to unify the atoms of the mated
literals.)

When the unification process encounters a flexible-rigid pair
(which we designate by <...f... , ...$\verb+~+\;$H...>)
where the flexible term has head variable f and the rigid term has
a head of the form $\verb+~+\;$H, the following substitutions for f are
generated:

\begin{enumerate}
\item Projections

\item $\lambda w^{1}\cdots\lambda w^{k}. \verb+~+ f^{1} \ldots$, where information is attached
to $f^{1}$ which does not permit a substitution of this same type (i.e.,
introducing a negation) to be applied to $f^{1}$.

\item $\lambda w^{1}\cdots \lambda w^{k}. f^{2} \ldots$, where information is attached
to $f^{2}$ which does not permit the first two of these types of substitution
to be applied to it. 
\end{enumerate}

(The information is stored by putting the variables into the lists
neg-h-var-list and  imitation-h-var-list.)
The restrictions on $f^{2}$ assure that the dpair which is essentially
<...$f^{2}$... , ...\verb+~+ H...> can only be used to generate new substitutions
for $f^{2}$ if other substitutions reduce \verb+~+H to a form which does not
start with a negation. 

\subsection{Sunil's Disjunction Heuristic}

\begin{enumerate}
\item If a matrix contains $[A \lor B]$, and $A$ has no mate, then no
mate for $B$ will be sought.

\item If a matrix contains  $[A \lor B]$, and $A$ has a mate, but no
mate for $B$ can be found, then the search will backtrack,
throwing out the mate for A and all links which were subsequently
added to the mating.
\end{enumerate}

{\bf Remark:}  This heuristic is also used by Matt Bishop's search
procedure ms98-1.  See his thesis~\cite{Bishop99a} for more details.

\section{After a Mating is Found}

Here is the sequence of events for pfd:

\begin{enumerate}
\item An expansion proof has been found.
A record (probably called DUP-RECORD) of indices for duplicated
variables, leaves, and connections is maintained by the search process.
The unification tree associated with the mating has a record of the
substitutions for variables.

\item Construct jform with final substitutions applied.
This uses all copies of variables needed for the final mating.

\item Duplicate expansion tree from the jform

\item Attach expansion terms to the expansion tree

\item Call propositional search to reconstruct the mating

\item Reorganize mating in ms88 form

\item Merge expansion proof

\item Translate expansion proof
\end{enumerate}

\section{How MIN-QUANT-ETREE Works}

After a proof is found, \TPS constructs an expansion proof tree.
The implementation of flag \indexflag{MIN-QUANT-ETREE} consists of the
following steps.


\begin{enumerate}
\item \TPS searches through the expansion proof tree to find if
there are primsubs which are not in minimized-scope form.
If \TPS finds some, it goes to step (2).

\item First, \TPS transform all the primsubs into their minimized-scope 
forms. In order to make sure that the expansion proof
tree is still a correct one, \TPS has to modify it. This is
done by calling two functions, namely, \indexfunction{one-step-mqe-bd} and
\indexfunction{one-step-mqe-infix}. Now \TPS goes to step (3).

\item Since the expansion proof tree is still a correct one, \TPS
can use a propositional proof checker to search for a mating.
This mating will be used to construct a proof in natural
deduction style.
\end{enumerate}

There are still potential bugs in the procedure, since
various rewrite nodes in an expansion proof can interfere with
flag \indexflag{MIN-QUANT-ETREE}. This has to be dealt with case by case.

\section{Merging}

Note: merging still contains bugs, although not very many. If a correct mating
is merged and produces a translation error, or a message of the form "The formula is not provable
as there's no connection on the following path: <path>", then it's likely that a bug in merging is
the culprit. Within merging, the routines for REMOVE-LEIBNIZ and PRETTIFY are the most likely 
causes of problems. The former only applies for formulae with equality, and can be checked by
trying again with the flag REMOVE-LEIBNIZ set to NIL. For the latter, you need to use \indexother{merge-debug}; 
type {\tt setq auto::merge-debug t} before calling merging, and you can step 
through the process, inspecting the etree at each step and omitting the optional steps. 
This can be a great help in discovering which part of the merging process is causing the bug.

Once a complete mating is found, we enter a merging process, in which
each expansion is checked to see
if it is really needed in the proof.  That is, it is temporarily removed
from the etree, and the resulting etree is checked to see if the mating still
spans all its paths.  This begins in \indexfunction{PRUNE-UNMATED-BRANCHES} (a misleading name),
which calls \indexfunction{UNNEEDED-NODE-P}, which calls \indexfunction{SPANS}, which calls
\indexfunction{SPANNING-CLIST-PATH}, which calls \indexfunction{FIND-CHEAPEST-CLIST-SPANNING-PATH}, which
\indexfunction{FIND-ALT-CHEAPEST-CLIST-SPANNING-PATH}.   Note that even in x5207, which is
relatively small, 12 calls to \indexfunction{UNNEEDED-NODE-P} result in 295 calls to
\indexfunction{FIND-CHEAPEST-CLIST-SPANNING-PATH}; these functions are the main reason why 
merging can be so slow, especially in proofs created 
by MS90-3 or MS90-9.

You could possibly (as was done at one time) not test this spanning 
condition, and just check to see if every expansion actually has a connection 
below it.  The problem here is that in ms90-3, by the time we get to the merging
process, we have mated every possible pair in the tree, whether the connection
is necessary or not.  That is why unneeded-node-p was modified to be more 
rigorous, because otherwise it was almost useless.  Additionally, there may be
embedded falsehood nodes below it, which are required to close some paths,
even if there are no mated nodes below it. 

A better spanning function should be used, though actually the one used
is already propositional, but of an earlier generation than Sunil's
propositional search function.  One should
realize, however, that the procedure should use the mating provided (and not 
the eager "mate-everything", because our mating might {\it not} be that big).  
In fact, Dan wrote such a \indexfunction{SPANS} that uses a variant of \indexfunction{PROP-MSEARCH},
and the time used in X5207 by \indexfunction{SPANS} went from 1 second to about .3 sec. 
Unfortunately, \indexfunction{PROP-MSEARCH} (or rather, \indexfunction{PROP-FIND-CHEAPEST-PATH}) appears to 
have the "empty disjunction causes confusion" bug.  (Try MS90-3 on
the formula "falsehood implies A"). 

More drastic changes were tougher to implement. There were a few suggestions:
\begin{itemize}
\item What this is doing is a lot of duplicated effort, so perhaps it would be possible to cache some results.
This would be pretty space-intensive; e.g. THM131 has an astronomical number of vpaths
when it begins merging. It turned out that attempts to make SPANS better by caching the results were pretty silly, 
because the way it is invoked, you can't tell the difference between
sets of arguments.  The differences are made by changing the status of
various lower-level expansion nodes.  So that attempt was abandoned.

\item Perhaps it would be possible to check the paths which the suspect node was on.
It's not clear how to do this.

\item Of course, it might be possible to avoid some of the calls to spans
in the first place (though possibly not with MS90-3), but even eliminating 
half would only save 3 days in the wolf-goat-cabbage problem, without changing what it does. 
\end{itemize}

In the end, the solution used was as follows: when path-focused duplication has been
used, the expansion proof will often have a great deal of redundancy
in the sense that the same expansion term will be used for a given variable
many times. More precisely, if one defines an expansion branch by
looking at sequences of nested expansion nodes, attaching one expansion
term to each expansion node in the sequence, there will be many identical
expansion branches. So one can start by merging the tree in the
sense of eliminating this redundancy, and then apply to this much simpler
tree the procedure for deleting unnecessary expansion terms which
we think is using so much time. It turned out to be easiest to do this
by throwing away the mating, and reconstructing it by propositional search
after the tree has been cut down to size. Of course, one could also
preserve the original mating by "merging" it appropriately as one collapsed
the tree.

The precise way in which this was done, in the file {\it mating-merge.lisp}, was:
\begin{enumerate}
\item Don't do pruning of unnecessary nodes at the beginning of the merge,
when the tree is its greatest size. 

\item Instead, {\it do} prune all branches that couldn't possibly have been used 
They are those that have a zero status. This is probably not necessary,
but certainly makes debugging easier and doesn't cost much.

\item After merging of identical expansions has been done, call the original
pruning function.
\end{enumerate}

Note that the merge process does (or should, anyway) merge the mating 
appropriately as the tree collapses.
On THM131 this takes the time spent on merging from 7 days down to 12 minutes.
This is not so surprising, because it begins with 113 million paths, and after
the merging of duplicate expansions, it's down to around 442 thousand. 

The matingstree top level has its own approach to merging, which is essentially
step (2) above, in which all unused 
expansions are simply thrown away, followed by a regular merge as detailed above. 
Putting step (2) first here 
is necessary because the master expansion tree has many nodes which are irrelevant to any particular proof.

\section{Printing}
::::::::::::::
mexprs.tex
::::::::::::::
\chapter{MExpr's} \label{defmexprs}
\TPS provides its own top-level.  It allows for default arguments
and provides a way of giving arguments (e.g. wffs) in some external
representation which is converted before the "real" function is called.
All this is also available in an interactive mode, where the user is
prompted for arguments after he has been told what the defaults are
and which alternatives are open.  The way all this has been implemented
is through MExpr's, which constitute special functional objects analogous
to Expr's or FExpr's in LISP.  Every \TPS command should be an MExpr
so that the facilities of \TPS' top-level can be utilized. 

\section{Defining MExpr's} \label{mexprargs}
Mexprs are special functional objects that are
recognized by the top level of \tps.
They can be defined with the {\tt defmexpr} macro, which has
a number of optional arguments.
The general format is ({\tt {}} indicate optional arguments)
\begin{verbatim}
(defmexpr {\it name}
	 {(ArgTypes {\it type1} {\it type2} ...)}
	 {(ArgNames {\it name1} {\it name2} ...)}
	 {(ArgHelp {\it help1} {\it help2} ...)}
	 {(DefaultFns  {\it fnspec1} {\it fnspec2} ...)}
         {(EnterFns {\it fnspec1} {\it fnspec2} ...)}
	 {(MainFns {\it fnspec1} {\it fnspec2} ...)}
	 {(CloseFns {\it fnspec1} {\it fnspec2} ...)}
         {(Print-Command {\it boolean})}
         {(Dont-Restore {\it boolean})}
	 {(MHelp "{\it comment}")}
\end{verbatim}

There are actually two other possible entries, {\tt Wffop-Typelist} and {\tt WffArgTypes}; these
are only used in mexprs which are generated automatically by the Rules package.

In the following a {\it function specification} is either a symbol naming
a function, or an expression of the form {\tt (Lambda {\it arglist} . {\it body})}.
We also assume that the main function which is to perform the command has
{\it n} arguments.  Then the phrases in the above definition have the
following meaning.
\begin{description}
\item [{\it name}\index{{\it name}}]
 This is the name of the MExpr as called by the user.

\item [{\tt ArgTypes}] This is a list which must have as many elements as the
function arguments, i.e. {\it n}.  {\it type1}, {\it type2}, ..., {\it typen} have
to be valid types, which means that they
have to have a non-{\tt NIL} {\tt ArgType} property.  Each argument supplied by the
user on the command line will be processed first by the corresponding
{\tt GetFn}.  In case an FExpr is to be called, each element of the
argument list is presupposed to be of the same type.  This type is
specified in parentheses.  If {\tt ArgTypes} is omitted, the function has
no arguments.

\item [{\tt ArgHelp}] This has to be a list of length {\it n}.  Each element is
a string describing the argument, or {\tt NIL}.  These quick helps
for arguments can be accessed via the {\tt ?} when being prompted
for the argument value.  For an FExpr, there should be only one string.

\item [{\tt DefaultFns}]
The {\it fnspecs} declared in this place are being processed in a
left-to-right order, where the result of one {\it fnspec} is passed on
to next.  A {\it fnspec} can signal an error (a {\tt THROW} with a
{\tt FAIL} label) if the arguments seem to be contradictory (e.g.  if a
planned line and a term is supplied for a {\tt P}-rule, but the term does
not appear in the proof), but it can count on the arguments being of
the correct type and in internal representation. 

In detail, each default {\it fnspec} must be either a symbol denoting
a function of {\it n} arguments, where {\it n} is the number of mexpr
arguments, or else a lambda expression of {\it n} arguments.
Each {\it fnspec} must return a list of
length {\it n}.  This list will then be handed on and processed by the next
{\it fnspec} as if it were the list of arguments supplied by the user.
Any entry which is not a {\tt \$} should be left unchanged.  The function
is not allowed to have side-effects.
As a general convention, the arguments which are not used by
a {\it fnspec} are not written out with their name, but replaced by
{\tt \%}{\it i}.  This makes it easier to see at one glance which defaults
are filled in by a certain {\it defaultspec}. 

\item [{\tt EnterFns}] {\it fnspec1}, {\it fnspec2}, ... is an arbitrary list of function
specifications.  They are called in succession with the value list
returned by the last default {\it fnspec}, before the {\tt MainFns} are called.

\item [{\tt MainFns}] {\it fnspec1}, {\it fnspec2}, ... is an arbitrary list of function
specifications.  They are called in succession with the value list
returned by the last default {\it fnspec}.  If none are specified, it is assumed
that there is a function named {\it name}, which can be called.
Notice that at this stage, no defaulted arguments may be left.
{\tt ComDeCode} (the command processing function) will refuse to call
any function, unless all the defaults are determined.  This clearly
divides the responsibilities between {\tt GetFn}'s, {\tt DefaultFn}'s and
{\tt MainFn}'s. Any {\it fnspec} may abort with an error by doing a
{\tt THROW} with a {\tt FAIL} label.  A {\tt THROW} with a {\tt TryNext} label
will be handled like a normal return.  A {\tt THROW} with a {\tt CutShort} label
means that none of the remaining {\tt MainFn}'s will be executed and the 
value of the {\tt THROW} will be handed on to the {\tt CloseFn}'s.

\item [{\tt CloseFns}] {\it fnspec1}, {\it fnspec2}, ... is a list of function
specifications.  They are called in succession with the value returned
by the last {\tt MainFn}.  Even if the {\tt MainFn}'s were FExpr's, each
{\it fnspec} has to describe an Expr.

\item [{\tt Dont-Restore}] {\it boolean} determines whether or not this command will be
restored, if it is saved using SAVE-WORK. For example, commands like HELP and ? should not
be restored.

\item [{\tt Print-Command}] {\it boolean} is used by RESTORE-WORK and EXECUTE-FILE, which 
both ask "Execute Print-Commands?"; this is how they know which commands are print 
commands.

\item [{\tt MHelp}] This has to be a string and will be available through
{\tt UserHelp} and the {\tt ??} if no {\tt QuickHelp} is available.

\end{description}

\section{Argument Types}
At the top-level of \TPS
explicitly declared argument types are available.  Many of the more important ones are
all declared in the file {\tt argtyp.lisp}.  They can be recognized by their
{\tt ArgType} property value, which is {\tt T}.  Each of argument type
has at least three properties, {\tt GetFn}, {\tt TestFn}, and {\tt PrintFn}.
{\tt GetFn} is responsible for translating the user's value
into internal representation, {\tt TestFn} tests if some object is of the given
type, and {\tt PrintFn} makes the internal
representation intelligible to the user.  

The defining command for the category {\tt argtype} is actually {\tt deftype\%\%}, but 
all definitions of argtypes should be made through the secondary macro {\tt DefType\%}.
Its format is as follows
({\tt {}} enclose optional arguments):
\begin{verbatim}
(DefType% {\it name}
	 (GetFn {\it fnspec})
	 (TestFn {\it fnspec})
	 (PrintFn {\it fnspec})
	{(Short-Prompt {\it boolean})}
	{(MHelp "{\it comment}")}
	{({\it property1} {\it value1}) ({\it property2} {\it value2}) ...})
\end{verbatim}
In the above a {\it fnspec} is either the name of a one-argument function,
or a list of forms which are to be evaluated as an implicit progn.
In the latter case, {\it name} stands for the argument supplied.
\begin{description}
\item [{\it name}\index{{\it name}}] 
The name of the argument type.  It will get a property value of {\tt T}
for the property {\tt ArgType} when the {\tt DefType\%} has been executed.


\item [{\tt GetFn}] Here {\it fnspec} defines the function used to process the argument
as supplied by the user on the command line.  The value returned by it
is then handed on to the main function executing the command.  No {\tt GetFn}
will ever receive a {\tt \$}.  It is simply not called, if the corresponding
argument in the command line is defaulted.  A {\tt GetFn} should signal an
error if the argument is not
of the correct type.  This will be implemented (as it is right now)
as a {\tt THROW} with the label {\tt FAIL}.  A special case of {\it fnspec}
for a {\tt GetFn} is {\tt TestFn}.  This means the {\tt GetFn} will
test if the supplied argument is of the correct type.  If yes, the argument
will simply be returned, otherwise an error will be signaled.
A {\tt GetFn} may have side-effects, but this has to be declared
under {\tt Side-Effects}.

\item [{\tt PrintFn}] Here {\it fnspec} should print the external representation
of its only argument.  It can expect this argument to be of the correct
type.  The value returned is ignored.  A {\tt PrintFn} may signal an error
if printing is not possible (e.g. if the current style does not
have a representation of the given data type).

\item [{\tt TestFn}] Here {\it fnspec} should return {\tt NIL} if its argument is not of
type {\it name}, and return something not {\tt NIL} otherwise.

\item [{\tt Short-Prompt}] {\it boolean} is only used in {\it otl-typ.lisp}, but I can't 
work out what for.

% \begin{comment}
% This was removed...
% {\tt Side-Effects}\\ {\it value1}, {\it value2}, ... constitute a list of identifiers
% which cause the {\tt GetFn} to have side-effects.  {\w {\tt (Side-Effects T)}}
% means that any argument to {\tt GetFn} will lead to side-effects.  This is
% necessary for instance in the {\tt RWff} argument type, since {\tt RDC}, {\tt RD} ,or
% {\tt PAD} have side-effects and thus should not be called more than once.
% It is useful however, to call a {\tt GetFn} on the same argument more than
% once to figure out defaults as well as possible.
% \end{comment}
\item [{\tt MHelp}] This is an optional documentation and is accessed during {\tt MHelp}
or after a {\tt ?} while the user supplies command arguments interactively.

\item [({\it property} {\it value})] Pairs like this allow for more information about the type.
\footnote{More properties may become useful, so
the {\tt Deftype\%} macro allows arbitrary property names.  Possibilities here
include {\tt EdFn} (for editing this argument type) or {\tt OutputFn} (to be able to
read back a data object of the specified type.)}
\end{description}
For example
\begin{verbatim}
(deftype% anything
  (getfn (lambda (anything) anything))
  (testfn (lambda (anything) (declare (ignore anything)) t))
  (printfn princ)
  (mhelp "Any legal LISP object."))

(deftype% integer+
  (getfn testfn)
  (testfn (and (integerp integer+) (> integer+ -1)))
  (printfn princ)
  (mhelp "A nonnegative integer."))

(deftype% boolean
  (getfn (cond (boolean t) (t nil)))
  (testfn (or (eq boolean t) (eq boolean nil)))
  (printfn (if boolean (princ t) (princ nil)))
  (mhelp "A Boolean value (NIL for false, T for true)."))
\end{verbatim}

No {\tt TestFn} or {\tt PrintFn} is allowed to have any
side-effects, since they may be called arbitrarily often.  No {\tt GetFn}
needs to expect
{\tt \$} as an argument, since defaults are now figured out elsewhere.
This avoids conflicts between different defaults for the same argument
type in different functions.  Hence {\tt GetFn} never computes the
default. 

\subsection{List Types}

The macro {\tt deflisttype} defines a list from an existing type:

%\begin{lispcode}
\begin{verbatim}
(deflisttype filespeclist filespec)
\end{verbatim}
%\end{lispcode}

This takes an existing type, {\tt filespec}, and produces a type of lists of filespecs.
It is also possible to specify other properties (the same properties as for {\tt deftype\%}),
in which case these properties override those of the original type. This is typically 
used to give the list type a different help message from the original type.

\subsection{Consed Types}

The macro {\tt defconstype} defines a type as a cons of two existing types:

%\begin{lispcode}
\begin{verbatim}
(defconstype subst-pair gvar gwff
  (mhelp "Means substitute gwff for gvar."))
\end{verbatim}
%\end{lispcode}

This takes two existing types, {\tt gvar} and {\tt gwff}, and produces a type {\tt subst-pair} of 
consed pairs {\tt (gvar . gwff)}.
It is also possible to specify other properties (the same properties as for {\tt deftype\%}),
in which case these properties override those of the original type. This is typically 
used to give the cons type a different help message from the original type.






% \begin<comment>
% Which other properties of argument types are allowed is determined
% by the global variable {\tt ArgTyProps}.
% {\tt ArgTyProps} is very similar to {\tt GrinProps} and serves the same
% purpose.  It contains a list of pairs
% whose first element is the name of a property associated with the
% argument type and whose second element is either {\tt LIST} or {\tt CONS}.
% A value of {\tt LIST} indicates that the property to be stored is just
% a single element, and hence has to be written out using
% @w[{\tt (LIST {\it property} (GET {\it type} {\it property}))}].  {\tt CONS} on the other
% hand means that the property will be a list and thus has to be written
% using
% @w[{\tt (CONS {\it property} (GET {\it type} {\it property}))}].  
% As an example consider the following definition made in the file
% {\tt BASICS}.
% \begin{verbatim}
% (DV ARGTYPROPS
%     ((GETFN . LIST)
%      (TESTFN . LIST)
%      (PRINTFN . LIST)
%      (SIDE-EFFECTS . CONS)
%      (MHELP . LIST)))
% \end{verbatim}
% \end<comment>

% \begin<comment>
% \section{The Tactic mechanism}
% {\it Carl's comments}
% Tactics and tacticals, though inspired by proof methods, are implemented
% in a general way for \tps. Basically, anything which can described as
% a goal-satisfying process using some fixed kind of object or state, can
% form a base for tactics. We define four entities to implement tactics.
% 
% \begin{description}
% \item [Tacticals] These are functions which take formal arguments, either parameters,
% tactic functions, tactic expressions or lists of these, and with additional
% arguments
% indicating the goals, object and bindings for parameters, return the same
% values as tactics. The application of a tactical to its formal arguments
% is a tactic expression. For example, {\tt (or deduct same)} is a
% tactic expression, with {\tt or} as the tactical.
% 
% \item [Base] A base determines how a tactic expression consisting of a non-tactic
% atom is to be interpreted. Besides the recognition function ({\tt isfn})
% and the function to "run" the atom, producing the same output as a tactic,
% there are properties to ensure the correct operation of certain tacticals.
% For example, {\tt fork} needs a process function in order for the forking
% of processes to make sense in a particular base (and to hack the setting
% of global variables). Much of the work of developing tactics for, say,
% rules consists of defining the base correctly. This makes
% a great deal more sense than to change the general procedure to adapt to
% changes in the implementation of rules.
% 
% \item [Tactic functions] These are functions made into objects for tactics.
% This allows them to be documented, as would be necessary for functions
% accessing an expression tree, and limited, as teachers allowing rule
% tactics might wish.
% 
% \item [Tactics] A Tactic is an object with a tactic expression in its {\tt defn}
% property. If uncompiled, the tactic interpreter, {\tt run-tactic-exp},
% will interpret that tactic expression. If compiled, its {\tt compile}
% property should contain a function to achieve the same effect as
% when it is uncompiled. [Compilation has not been implemented.]
% A tactic has arguments, listed in it {\tt mouths} property, which the 
% {\tt feed} tactical may fill using a feed function for the base.
% This provides a more general kind of argument-handling.
% 
% \end{description}
% \end<comment>
::::::::::::::
monitor.tex
::::::::::::::
\chapter{The Monitor}
\label{monitor}

The \indexother{monitor} is designed to be called during automatic proof searches; its basic
operation is described in the User Manual. There are three basic steps required to 
write a new monitor function, which are described below, using the monitor function 
\indexother{monitor-check} as an example. More examples are in the file {\it monitor.lisp}.

\section{The Defmonitor Command}

The command \indexcommand{defmonitor} behaves just like {\tt defmexpr}, the only difference being
that the function it defines does not appear in the list when the user types {\tt ?}. This command
will be called by the user before the search is begun, and should be able to accept any required 
parameters (or to calculate them from globally accessible variables at the time the command is
called).

So, for example, the {\tt defmonitor} part of \indexother{monitor-check} looks like this:

%\begin{tpsexample}
\begin{verbatim}
(defmonitor monitor-check
  (argtypes string)
  (argnames prefix)
  (arghelp "Marker string")
  (mainfns monitor-chk)
  (mhelp "Prints out the given string every time the monitor is called, 
followed by the place from which it was called."))

(defun monitor-chk (string)
  (setq *current-monitorfn* 'monitor-check)
  (setq *current-monitorfn-params* string)
  (setq *monitorfn-params-print* 'msg))
\end{verbatim}
%\end{tpsexample}

Note that this accepts a marker string as input from the user (other monitor functions may 
look for a list of connections, or flags, or the name of an option set; it may be necessary 
to define a new data type to accommodate the desired input). It then calls a secondary 
function, which in this case needs to do very little further processing in order to 
establish the three parameters which are {\it required} for every such function: {\tt *current-monitorfn*}
contains a symbol corresponding to the name of the monitor function, {\tt *current-monitorfn-params*} 
contains the user-supplied parameters (in any form you like, since your function will be the only 
place where they are used) and {\tt *monitorfn-params-print*} contains the name of a function that can 
print out {\tt *current-monitorfn-params*} in a readable way, for use by the commands \indexcommand{monitor}
and \indexcommand{nomonitor}. The latter should be set to {\tt nil} if you can't be bothered to write such 
a function.

\section{The Breakpoints}

In the relevant parts of the mating search code, you should insert breakpoints of the form:

%\begin{tpsexample}
\begin{verbatim}
(if monitorflag 
    (funcall (symbol-function *current-monitorfn-params*) 
             <place> <alist>))
\end{verbatim}
%\end{tpsexample}

The value of {\it place} should reflect what part of the code the breakpoint is at. So, for example,
it might be {\tt 'new-mating}, {\tt 'added-conn} or {\tt 'duplicating}.

The value of {\it alist} should be an association list of local variables and things that your monitor
function will need. For example, {\it alist} might be {\tt (('mating . active-mating) ('pfd . nil))}; it might 
equally well be just {\tt nil}.

All breakpoints should have exactly this pattern. By typing {\it grep "(if monitorflag (funcall" *.lisp} in
the {\it tpslisp} directory, you can get a listing of all the currently defined breakpoints.

\section{The Actual Function}

This is the function which will actually be called during mating search. By convention, it has the
same name as the {\tt defmonitor} function. Normally, it will first check the value of {\it place}, to
see if it has been called from the correct place; it can then use the {\tt assoc} command to retrieve the
relevant entries from {\it alist}. Theoretically, it should be completely non-destructive so as to ensure 
that the mating search continues properly; of course, you may be as destructive as you like, provided 
you understand what you're doing...

The function for {\tt monitor-check} is as follows; notice that this does not check {\it place} since it 
is intended to act at every single breakpoint.

%\begin{tpsexample}
\begin{verbatim}
(defun monitor-check (place alist)
  (declare (ignore alist))
  (msg *current-monitorfn-params* place t)) 
\end{verbatim}
%\end{tpsexample}

::::::::::::::
nat-etr.tex
::::::::::::::
There are three versions of \indexcommand{NAT-ETREE},
the command for translating natural deductions into
expansion tree proofs.  The user can choose between
the three by setting the flag \indexflag{NAT-ETREE-VERSION}
to one of the following values:
\begin{enumerate}
\item {\bf OLD}  (the original version)
\item {\bf HX} (Hongwei Xi's version, written in the early
to mid 1990's)
\item {\bf CEB} (Chad E. Brown's version, written in early 2000)
\end{enumerate}
Also, note that setting the flag \indexflag{NATREE-DEBUG}
to T is useful for debugging the {\bf HX} and {\bf CEB}
versions.

More details about each of these are contained in the
following subsections.

\subsection{Chad's Nat-Etree}

To use this version of \indexcommand{NAT-ETREE}, set
\indexflag{NAT-ETREE-VERSION} to {\bf CEB}.
The main functions for this version are in the
file \indexfile{ceb-nat-etr.lisp}.  The relevant
functions are:
\begin{description}
\item [\indexfunction{ceb-nat-etree}]  This is the main function.  It preprocesses
the proof to remove applications of {\it Subst=} and {\it Sym=},
calls \indexfunction{ceb-proof-to-natree} to build
the natree version of the natural deduction proof, calls
\indexfunction{ceb-natree-to-etree} to build the etree (if the
proof is normal, failing otherwise), and finally builds the mating.
\item [\indexfunction{ceb-proof-to-natree}]  This is a modification
of Hongwei's \indexfunction{proof-to-natree} (see \indexfile{hx-natree-top.lisp}).
This function builds the natree, changing some justifications to $RuleP$
or $RuleQ$, and changing the variables in applications of $UGen$ and $RuleC$
so they are unique in the entire proof (i.e., the natree rules satisfy a global
eigenvariable condition, since the etree selection variables must be distinct).
\item [\indexfunction{ceb-natree-to-etree}]  This initializes the
eproof and calls \indexfunction{natree-to-etree-normal} on the current-natree.
\item [\indexfunction{natree-to-etree-normal}, \indexfunction{natree-to-etree-extraction}]
These functions are mutually recursive and provide the main algorithm for constructing
the etree.  A more description of the algorithm is below.  The function \indexfunction{natree-to-etree-normal}
is called on natree nodes which are considered normal.  (These would be annotated with a $\Uparrow$.)
The function \indexfunction{natree-to-etree-extraction} is called on natree nodes which
are considered extractions.  (These would be annotated with a $\downarrow$.)
\end{description}

This version only works for
normal natural deduction proofs.  Frank Pfenning's ATP
class contained notes on annotating (intuitionistic first-order)
normal natural deduction proofs, and gave a constructive proof
(algorithm) that every normal natural deduction derivation
translates into a cut-free sequent calculus proof.  The
idea of annotations carries over to classical higher-order
logic, and the same idea for an algorithm gives an algorithm for
converting such natural deduction derivations to expansion tree
proofs.

Of course, many natural deduction proofs are not normal.
We may try to write code to normalize proofs (see later in this section),
although the code may not always terminate.

\subsubsection{Normal Deductions}

The idea of a normal deduction is that the proof works down
using elimination rules, and up using introduction rules,
meeting in the middle.  We can formalize this idea by saying
a natural deduction proof is normal if it can be annotated,
where the rules for constructing proofs carry annotations.
Technically, we are defining normal natural deductions by mutually
defining normal deductions ($\Uparrow$) and extraction deductions ($\downarrow$).

\subsubsection{Annotations of the Rules of Inference}

First, the basic rules which allow one to infer normal deductions ($\Uparrow$).
$$ \ianc{A\Uparrow}{\forall x A\Uparrow}{UGen}$$
where $x$ is not free in any hypotheses.
$$ \ibnc{\exists y A\downarrow}{\hypo{\ian{}{[x/y]A\downarrow}{}}{C\Uparrow}}{C\Uparrow}{RuleC}$$
where $x$ is not free in any hypotheses.
$$ \ianc{[t/x]A\Uparrow}{\exists x A\Uparrow}{EGen}$$
$$ \ianc{A\Uparrow}{A\lor B\Uparrow}{IDisj-L} \;
 \ianc{B\Uparrow}{A\lor B\Uparrow}{IDisj-R}$$
$$ \ibnc{A\Uparrow}{B\Uparrow}{A \land B\Uparrow}{Conj} $$
$$ \ianc{\bot\downarrow}{A\Uparrow}{Absurd}$$
$$ \ianc{\hypo{\ian{}{\neg A\downarrow}{}}{\bot\Uparrow}}{A\Uparrow}{Indirect} \;
 \ianc{\hypo{\ian{}{A\downarrow}{}}{\bot\Uparrow}}{\neg A\Uparrow}{NegIntro} \;
 \ianc{\hypo{\ian{}{A\downarrow}{}}{B\Uparrow}}{A\supset B\Uparrow}{Deduct}$$
$$ \ibnc{\neg A\downarrow}{A\Uparrow}{C\Uparrow}{NegElim}$$
$$ \ibnc{A \vee B\downarrow}{\hypo{\ian{}{A\downarrow}{}}{C\Uparrow}\quad\qquad\hypo{\ian{}{B\downarrow}{}}{C\Uparrow}}{C\Uparrow}{Cases}$$

TPS also has rules {\it Cases3} and {\it Cases4} which may be used to
eliminate disjunctions with three or four disjuncts, resp.  These are
annotated in a manner analogous to the {\it Cases} rule.

Next, the basic rules which allow one to infer extraction deductions ($\downarrow$).
$$ \ianc{A \land B\downarrow}{A\downarrow}{Conj} \hspace{2em} \ianc{A \land B\downarrow}{B\downarrow}{Conj}$$
$$ \ianc{\forall x A\downarrow}{[t/x]A\downarrow}{UI}$$
$$ \ibnc{A \limplies B\downarrow}{A\Uparrow}{B\downarrow}{MP}$$

Notice that hypothesis lines are always considered extraction derivations.
Such lines may be justified by any of the following:
$Hyp$, $Choose$, $Assume negation$, $Case 1$, $Case 2$, $Case 3$, $Case 4$.

We need a coercion rule, as every extraction is a normal derivation:
$$ \ianc{A\downarrow}{A\Uparrow}{coercion}$$
In a TPS natural deduction style proof, this coercion step will not usually be
explicit.  Instead, a single line will be given the property of being
a coercion, in which case we know it has both annotations $\downarrow$
and $\Uparrow$, and that these annotations were assigned in a way consistent
with the coercion rule above.  Often, when interactively constructing
a natural deduction proof in TPS, one finds that a planned line is the
same as a support line, and finishes the subgoal using SAME.  This would
correspond to the coercion rule above.

The backward coercion rule 
$$ \ianc{A\Uparrow}{A\downarrow}{bcoercion}$$
is not allowed in normal natural deductions.  In fact, we
can consider backward coercions to be instances of cut.
A separate, interesting project in TPS would be to
program a normalization procedure (see a later subsection).  
Such a procedure would
find instances of the backward coercion rule when
annotating a proof, identify to what kind of ``redex''
the backward coercion rule corresponds, and perform the
reduction.  For this to work we would need to define the
notion of redex so that every proof which needs the backward
coercion rule to be annotated (proofs that are not normal)
must have a redex.  Also, we would need to prove that reduction
terminates -- a task equivalent to constructively proving 
cut-elimination in classical higher-order logic -- a highly
nontrivial task.

\subsubsection{Some Nonstandard ND Rules}

$RuleP$ allows us to make arbitrary propositional inferences.  One might
suspect that any proof using $RuleP$ cannot be normal.  However, we
can view $RuleP$ as a kind of coercion rule:
$$ \ianc{A_1\downarrow,\; A_2\downarrow,\;\ldots,\; A_n\downarrow}{B\Uparrow}{RuleP}$$
where $A_1 \land A_2 \land \cdots \land A_n \limplies B$ is a
propositional tautology.  In fact, with respect to
interactively constructing a natural deduction in TPS,
this annotation of $RuleP$ corresponds to using $RuleP$ when
the planned line follows via propositional reasoning from
some support lines.

There are a variety of rules: $Neg$, $Assoc$, $Imp-Disj$, etc.
which allow the user to make selected propositional inferences.
The quick and easy way to handle these is to change them to
applications of $RuleP$, and that is just what the code does.
A more systematic way of handling them would be to replace
the application with a short subderivation, but this would
not change the annotations (though such subderivations could
be vital when we start trying to normalize).  
Any use of these rules is very likely to destroy normality.

Actually, $Neg$ can be used to make small first-order
inferences (from $\neg \forall x . A$ to $\exists x . \neg A$, etc.).
We can replace these by the more general $RuleQ$.
We annotate $RuleQ$ just like $RuleP$.  {\it However,} 
the translation process will fail on a proof with $RuleQ$
{\it UNLESS} all the instantiations for the quantifiers
are trivial.  Officially, we can only say that $RuleQ$ is
not fully supported in the code.

Of course, the $Same$ rule just propagates the annotation.
So, with respect to annotations, there are two versions of this rule:
$$\ianc{A\downarrow}{A\downarrow}{Same As}\hspace{2em}
\ianc{A\Uparrow}{A\Uparrow}{Same As}$$

When converting normal natural deduction to expansion tree
proofs, we only consider formulas up to $\alpha$-conversion,
so we can ignore the corresponding ND rule.  But effectively,
we allow this rule to be annotated in either of two ways,
as with the $Same$ rule:
$$\ianc{\forall x A\downarrow}{\forall y [y/x]A\downarrow}{AB}\hspace{2em}
\ianc{\forall x A\Uparrow}{\forall y [y/x]A\Uparrow}{AB}$$

Generally, $Assert$ will not appear in a normal proof
(as a ``cut'' is usually thought of as a lemma).
But we will allow $Assert$ (as a normal derivation) if we work backwards from
a goal to get a previously known theorem.  This
will probably rarely occur, except with reflexivity.
$$\ianc{}{A\Uparrow}{Assert}$$
where $A$ is a previously proven theorem.

Definitions can be eliminated or introduced, and the annotations
reflect this.  Also, elimination and introduction of definitions includes
some $\beta$-reduction.  Suppose the abbreviation $A$ is defined to
be $\lambda x_1\cdots\lambda x_n . \psi [x_1,\ldots, x_n]$ in the following
annotated rule schemas.
$$\ianc{\phi[A\; B_1\; \cdots \; B_n]\downarrow}{\phi[\psi[B_1,\ldots,B_n]]\downarrow}{Defn}$$
$$\ianc{\phi[\psi[B_1,\ldots,B_n]]\Uparrow}{\phi[A\; B_1\; \cdots \; B_n]\Uparrow}{Defn}$$

How to annotate the lambda rules is questionable.  A case could be made
that the direction should correspond to normalization.  But I have opted
to be very unrestrictive and allow any lambda rule without affecting the
annotations:
$$\ianc{A\downarrow}{B\downarrow}{Lambda}\hspace{2em}
\ianc{A\Uparrow}{B\Uparrow}{Lambda}$$
where $A$ and $B$ are $\beta\eta$-convertible.

\subsubsection{Equality Rules}

We assume that the proof has been preprocessed to remove
applications of substitution of equals, and applications of
symmetry, so there is no need to annotate these rules (for now).

Reflexivity a normal deduction, treated like any other $Assert$.
$$\ianc{}{A = A\Uparrow}{Refl=}$$
Intuitively, we work backwards until we get to an instance of reflexivity.

There are two ways to apply extensionality consistent with the
idea of annotations (both correspond to expanding an equation using
extensionality in the corresponding expansion tree).  Also, there
are two kinds of extensionality (functional and propositional).
$$\ianc{f_{\greekb\greeka} = g_{\greekb\greeka}\downarrow}{\forall x_\greeka . f x = g x\downarrow}{Ext=}
\hspace{2em}
\ianc{P_\greeko = Q_\greeko\downarrow}{P_\greeko \equiv Q_\greeko \downarrow}{Ext=}$$
$$\ianc{\forall x_\greeka . f x = g x\Uparrow}{f_{\greekb\greeka} = g_{\greekb\greeka}\Uparrow}{Ext=}
\hspace{2em}
\ianc{P_\greeko \equiv Q_\greeko\Uparrow}{P_\greeko = Q_\greeko\Uparrow}{Ext=}$$

Leibniz equality is handled just like definition expansion.
$$\ianc{A_\greeka = B_\greeka\downarrow}{\forall q_{\greeko\greeka} . q A \limplies q B\downarrow}{Equiv-eq}$$
$$\ianc{\forall q_{\greeko\greeka} . q A \limplies q B\Uparrow}{A_\greeka = B_\greeka\Uparrow}{Equiv-eq}$$

\subsubsection{Converting Normal Natural Deductions to Expansion Tree Proofs}

Normal natural deductions are converted to expansion tree proofs
via two mutually algorithms:

\begin{enumerate}
\item  Suppose we are given a line $A_1,\ldots, A_n \vdash C\Uparrow$.
Then we can compute
\begin{enumerate}
\item positive expansion trees $A_i^*$ with shallow
formulas $A_i$, and
\item a negative expansion tree $C^*$ with shallow formula $C$
\end{enumerate}
such that the expansion tree
$$A_1^* \land \cdots \land A_n^* \limplies C^*$$
has a mating.
\item  Given 
\begin{enumerate}
\item a line $A_1,\ldots, A_n \vdash B\downarrow$,
\item positive expansion trees $A_i^{**}$ with shallow formulas $A_i$,
\item a negative expansion tree $B^*$ with shallow formula $B$,
\item and a positive expansion tree $C^*$
\end{enumerate}
such that the expansion tree
$$A_1^{**}\land\cdots \land A_n^{**}\land B^* \limplies C^*$$
has a mating, we can compute
positive expansions trees $A_i^{*}$ with shallow formulas $A_i$,
such that the expansion tree
$$A_1^{*}\land\cdots \land A_n^{*} \limplies C^*$$
has a mating.
\end{enumerate}

{\bf Remark:}  We only check equality of wff's up to $\alpha$-conversion.
We might consider only checking up to $\alpha$-conversion and negation-normal-form.
In this way, we might be able to handle the $Neg$ rules so that more proofs will be ``normal''.

We can show a few cases to demonstrate how the algorithms
work.

\begin{itemize}
\item [Case:] Coercion.
$$ \ianc{C\downarrow}{C\Uparrow}{coercion}$$
This corresponds to a single line of the form
$$ A_1,\ldots,A_n\vdash C$$
which is both an extraction and a normal derivation.
We need to find positive expansion trees $A_i^*$
with shallow formulas $A_i$,
and a negative expansion tree $C^*$ such that
the expansion tree
$$A_1^* \land \cdots \land A_n^* \limplies C^*$$
has a mating.

We can take any positive expansion trees $A_i^{**}$ with shallow
formulas $A_i$, a positive expansion tree $C^{**}$ and 
a negative expansion tree $C^{*}$ with shallow
formula $C$, so that the expansion tree
$$C^{**} \limplies C^{*}$$
has a mating.  (We can construct $C^{**}$ and $C^{*}$ by
always choosing expansion terms to be the bound variable
of the quantifier -- being careful to maintain that selected
variables are distinct, or by allowing matings between non-leaf
nodes.)  Now, applying the second algorithm to the same
line (with $B = C$ and $B^* = C^{**}$), we have positive
expansion trees $A_i^*$ satisfying the criteria above.

\item [Case:]  RuleP.  This is handled as in the Coercion case.
Suppose the application is of the form
$$ \ianc{\above{\DD_1}{\Gamma\vdash B_1\downarrow}\; \above{\DD_2}{\Gamma\vdash B_2\downarrow}\;\ldots\; \above{\DD_m}{\Gamma\vdash B_m}\downarrow}{\Gamma\vdash C\Uparrow}{RuleP}$$
where 
$$\Gamma = A_1,\ldots, A_n.$$
(We need to use weakening to ensure all the hypothesis sets are the same,
but this is really of no consequence in the algorithm.)
Then we can construct positive expansion trees $A_i^{**}$ and $B_j^{*}$ and a negative expansion
tree $C^*$ by choosing the bound variables as the selected and expansion variables.
The etree
$$B_1^{*} \land \cdots \land B_m^{*} \limplies C^*$$
will have a mating by the same propositional reasoning that
validates the application of $RuleP$ (although pushed to the leaves),
and so will
$$A_1^{**}\land \cdots\land A_n^{**}\land B_1^{*} \cdots\land B_m^{*} \limplies C^*$$
Now, we apply the second algorithm $m$ times using $\DD_j$ to remove the $B_j^{*}$'s, obtaining
trees $A_i^*$'s such that
$$A_1^{*}\land \cdots\land A_n^{*} \limplies C^*$$
has a mating.

{\bf A Potential Bug:}  Again, we should make sure all selected variables are distinct,
but unlike the coercion case it is not so easy to ensure this, since we don't know
the propositional reasoning involved in advance.  For now, the code simply doesn't
ensure that selected variables that lie ``underneath'' the $A_i^*$ and $C^*$ nodes
in applications of $RuleP$.  Probably the way to fix this is to preprocess a natural
deduction proof to remove applications of $RuleP$.  We probably need to do this
for normalization later anyway.

\item [Case:]  Hyp.  Suppose the line is
$$A_1,\ldots, A_n \vdash A_j.$$
Since hypotheses are extractions,
we must be applying the second algorithm.  So, we must 
already have positive expansion trees $A_i^{**}$ with
shallow formulas $A_i$, a positive expansion tree $B^{*}$
with shallow formula $A_j$, and a negative expansion tree
$C^*$ such that
$$A_1^{**} \land \cdots \land A_n^{**} \land B^* \limplies C^*$$
has a mating.  Since $A_j^{**}$ and $B^*$ have the same shallow
formula and the same sign, we can merge these expansion trees to form the
positive expansion tree $A_j^*$ with shallow formula $A_j$.
For $i\neq j$, let $A_i^*$ be $A_i^{**}$.  Merging guarantees
that the expansion tree
$$A_1^{*}\land\cdots \land A_n^{*} \limplies C^*$$
will have a mating.  (See Miller's thesis, Lemma 2.5.4,
although his definition of expansion tree is simpler than
the actual implementation.)

\item [Case:]  Deduct.  This case is easy, as are most of the ``introduction''
rules.  Suppose we have
$$\ianc{\above{\DD}{\Gamma, A\vdash B\Uparrow}}{\Gamma \vdash A\supset B\Uparrow}{Deduct}$$
where
$$\Gamma = A_1,\ldots, A_n.$$
The first algorithm applied to $\DD$ gives positive etrees $A_i^*$ and $A^*$
and a negative etree $B^*$ such that
$$A_1^{*}\land\cdots \land A_n^{*} \land A^* \limplies B^*$$
has a mating.
So, of course we can let $[A\limplies B]^*$ be $A^*\limplies B^*$
and
$$A_1^{*}\land\cdots \land A_n^{*} \limplies [A \limplies B]^*$$
has a mating.

\item [Case:]  MP.  This case is interesting, because a naive
algorithm would be forced to treat this case like a ``cut'' in
the sequent calculus.  Suppose we have
$$ \ibnc{\above{\DD}{\Gamma \vdash A \limplies B\downarrow}}{\above\EE{\Gamma \vdash A\Uparrow}}{\Gamma \vdash B\downarrow}{MP}$$
where 
$$\Gamma = A_1,\ldots, A_n.$$
Since this is an extraction, we must be given negative etrees $A_i^{**}$ and
a positive etree $B^*$ such that
$$A_1^{**}\land \ldots \land A_n^{**} \limplies B^{*}$$
has a mating.

The first algorithm applied to $\EE$ gives positive expansion trees $A_i^{***}$
and a negative expansion tree $A^*$ such that
$$A_1^{***}\land \ldots \land A_n^{***} \limplies A^{*}$$
has a mating.  We can merge each $A_i^{**}$ with $A_i^{***}$ to obtain
$A_i^{+}$ so that
$$A_1^{+}\land \ldots \land A_n^{+} \land [A^{*}\limplies B^*] \limplies B^{*}$$
has a mating (combining the two matings we have, and connecting the leaves in the
two copies of $B^*$).

Finally, calling the second algorithm with $\DD$, we can remove the $[A^{*}\limplies B^*]$
obtaining positive etrees $A_i^*$ such that
$$A_1^{*}\land \ldots \land A_n^{*} \limplies B^{*}$$
has a mating.
\end{itemize}

{\bf Note:}  If \indexflag{NATREE-DEBUG} is set to T, then at each step,
the code double checks that a mating exists whenever it should, and that
all trees have the appropriate shallow formula (up to $\alpha$-conversion).

% {\bf Example:}
% 
% Consider the following natural deduction proof of
% $$\forall \,x [ \,P \,x \supset \,P . \,f \,x] \supset . \,P \,a \supset \,P . \,f . \,f \,a$$
% 
% {\it Show the proof.}
% 
% This is a normal proof, as it can be annotated as follows (without using backward coercion):
% 
% {\it Fill in the rest of this example and work through the conversion to an expansion
% tree proof step by step.}

\subsubsection{Normalization of Proofs}

In order to normalize a natural deduction proof, we must identify
possible redexes (pairs of rule applications which must use
backward coercion to be annotated), and show how to reduce these.
There are many such redexes.  The following is a typical example:

$$ \ianc{\ibnc{\above\DD{A}}{\above\EE{B}}{A \land B\downarrow}{Conj}}{A\downarrow}{Conj} \rightarrow \above\DD{A}$$

In first order logic, one can show that some measure on the proof reduces
when a redex is reduced, so that the process will terminate with
a normal proof.  In higher order logic, showing termination is equivalent
to showing termination of cut-elimination.

Actually carrying this out is a future project.

\subsection{Hongwei's Nat-Etree}

This is a brief description of Hongwei's code
for \indexcommand{NAT-ETREE}.  To use this code,
set \indexflag{NAT-ETREE-VERSION} to {\bf HX}.

{\bf Note:  Hongwei wanted this code to work for all natural deductions,
whether normal or not.  So, given a natural deduction proof which
is not normal, try this one, and you might get lucky.}

\indexfunction{ATTACH-DUP-INFO-TO-NATREE} is the main function, which
is called recursively on the subproofs of a given natural
deduction. The goal of \indexfunction{ATTACH-DUP-INFO-TO-NATREE} is to
construct an expansion tree, with no mating attached,
corresponding to a given natural deduction. The constructed
expansion tree contains all the correct duplications done on
quantifiers and all substitutions done on variables.
A propositional search will be called on the generated expansion
tree to recover the mating and generate an expansion proof.
Then \indexcommand{ETREE-NAT} can produce a natural deduction corresponding
to the constructed expansion proof.

The following is an oversimplified case.

Given natural deductions N1 and N2 with conclusions
A and B, respectively, and N derived from N1 and N2
by conjunction introduction. \indexfunction{ATTACH-DUP-INFO-TO-NATREE}
called on N generates two recursive calls on N1 and N2,
and get the expansion proofs corresponding to N1 and N2,
respectively, with which it constructs an expansion proof
corresponding to N.

An important feature of \indexfunction{ATTACH-DUP-INFO-TO-NATREE} is that
it can deal with all natural deductions, with or without
cuts in them. This is mainly achieved by substitution and
merge. This essentially corresponds to the idea in Frank
Pfenning's thesis, though his setting is sequent calculus.
On the other hand, the implementation differs significantly
since natural deductions grow in both ways when compared with
sequent calculus. This is reflected in the code of
\indexfunction{ATTACH-DUP-INFO-TO-NATREE} which travels through a natural
deduction twice, from bottom to top and from top to bottom,
to catch all the information needed to duplicate quantifiers
correctly.

Overview of the files:
\begin{itemize} 
\item \indexfile{hx-natree-top} contains the definition of the data structure,
some print functions and top commands.

\item \indexfile{hx-natree-duplication} contains the code of \indexfunction{ATTACH-DUP-INFO-TO-NATREE}
and some auxiliary functions such as \indexfunction{UPWARD-UPDATE-NATREE}. Also many
functions for constructing expansion trees are defined here.

\item \indexfile{hx-natree-rulep} contains the code for handling \indexfunction{RULEP}. This is done
by using hash tables to store positive and negative duplication
information. Then cuts are eliminated by substitution and merge.
The case in \indexfunction{ATTACH-DUP-INFO-TO-NATREE} which deals with implication
is a much simplified version of this strategy, and helps understand
the algorithm.

\item \indexfile{hx-natree-aux} contains the code of merge functions and the ones handling
rewrite nodes. Presumably there are some bugs in handling rewrites, and
this can be found in the comments mixed with the code. Also a new version
of \indexfunction{ETREE-TO-JFORM-REC} is defined here to cope with a modified date structure
\indexother{ETREE}.

\item \indexfile{hx-natree-cleanup} contains the functions which clean up the expansion
proofs before they can be used by \indexcommand{ETREE-NAT}. This is temporary crutch,
and should be replaced by some systematic methods. For instance, one
could construct brand new expansion proofs according to a constructed
one rather than modify it to fit the needs of \indexcommand{ETREE-NAT}. This yields
a better chance to avoid some problems caused by rewrite nodes. 

\item \indexfile{hx-natree-debug} contains some simple debugging facilities such as some
display function and some modified versions of the main functions in the
code. A suggested way is to modify the code using these debugging functions
and trace them. More facilities are needed to eliminate sophisticated bugs.
\end{itemize}

Selection nodes, not Skolem nodes, are used in the constructed expansion
trees. The prevents us from setting the \indexflag{MIN-QUANT-ETREE} flag to simplify a
proof. It is a little daunting task to modify the code for \indexflag{MIN-QUANT-ETREE},
but the benefits are also clear: both \indexcommand{NAT-ETREE} and non-pfd procedures can
take advantage of the modification.

\subsection{The Original Nat-Etree}

{\bf Note: What follows is a description of how NAT-ETREE used to work. 
For now this code can be run using the command NAT-ETREE-OLD.}  To use this
code set \indexflag{NAT-ETREE-VERSION} to {\bf OLD}.

Legend has it that the code was written by Dan Nesmith and
influenced by the ideas of Frank Pfenning.  Frank's thesis
contains ideas for translating from a cut-free sequent calculus
to expansion tree proofs.

\begin{enumerate}
\item Important files: nat-etr (defines functions which are independent
of the particular rules of inference used); ml-nat-etr1 and
ml-nat-etr2 (which define translations for the rules in the standard TPS).

\item There are three global variables which are used throughout the
translation process: DPROOF, which is the nproof to
be translated; LINE-NODE-LIST, which is an association list which
associates each line of the proof to the node which represents it in
the expansion tree which is being constructed; MATE-LIST, which is a
list of connections in the expansion proof which is being constructed.

\item At the beginning of the translation process, the current proof is
copied because modifications will be made to it.  (It is restored when
the translation is complete.)  The copy is stored in the variable
DPROOF.  Next the function SAME-IFY is called.  This attempts to undo
the effects of the CLEANUP function, and to make explicit the
"connections" in the proof.  This is done because, in an nproof, 
a single line can represent more than one node in an
expansion proof.  SAME-IFY tries to add lines to the proof in such a
way that each line corresponds to exactly one expansion tree node.  

\item After the proof has been massaged by SAME-IFY, the initial root
node of the expansion tree is constructed.  This node is merely a
leaf whose shallow formula is the assertion of the last line of the
nproof.  LINE-NODE-LIST is initialized to contain
just the association of this leaf node with the last line of the
proof, and MATE-LIST is set to nil.

\item Next the function NAT-XLATE is called on the last line of the
proof.  NAT-XLATE, depending on the line's justification, calls
auxiliary functions which carry out the translation, and which usually
call NAT-XLATE recursively to translate lines by which the current
line is justified.  When the justification "Same as" is found, this
indicates that the node associated with this line and the node which
is associated with the line it is the same as should be mated in the
expansion proof.

\item Example:  Suppose we have the following nproof:
\begin{verbatim}
(1) 1  !  A            Hyp
(2)    !  A implies A  Deduct: 1

SAME-IFY will construct the new proof:

(1) 1  !  A            Hyp
(2) 1  !  A            Same as: 1 
(3)    !  A implies A  Deduct: 2
\end{verbatim}

Then a leaf node LEAF0 is constructed with shallow formula 
"A implies A", and LINE-NODE-LIST is set to ((3 . LEAF0)). 
NAT-XLATE is called, and because line 3 is justified using the
deduction rule, LEAF0 is deepened to an implication node, say IMP0,
with children LEAF1 and LEAF2.  Then LINE-NODE-LIST is updated to be
((1 . LEAF1) (2 . LEAF2) (3 . IMP0)), and NAT-XLATE is called
recursively on lines 1 and 2.  Since line 1 is justified by "Hyp",
NAT-XLATE does nothing.  Since line 2 is justified by "Same as: 1",
NAT-XLATE updates the value of MATING-LIST to (("LEAF1" . "LEAF2")), a
connection consisting of the nodes which represent lines 1 and 2.

\item In an nproof that is not cut-free, there will exist lines which do
not arise from deepening the expansion tree which represents the last
line of the nproof.  Currently, NAT-XLATE will get very confused and
probably blow up.  The justification "RuleP" causes other
difficulties, because it generally requires that several connections
be made, involving lines whose nodes haven't been deepened to the
literal level yet.  The function XLATE-RULEP attempts to do this, but
does not always succeed.  This is true because RULEP can also be used
to justify a line whose node is actually a child of the justifying
line, e.g.: 
\begin{verbatim}
(45)  ! A and B 
(46)  ! A        RuleP: 45
\end{verbatim}
Though XLATE-RULEP can handle this situation, it cannot handle more
complex ones such as:
\begin{verbatim}
(16)  ! A
(17)  ! A implies B
(18)  ! B            RuleP: 16 17
\end{verbatim}
Ideally, SAME-IFY would identify these situations before the
translation process is begun, but it does not.
\end{enumerate}

::::::::::::::
ndmacros.tex
::::::::::::::
%%% Macros for Typesetting Natural Deductions
%%% A combination of Frank Pfenning's cdsty.sty file (Computation and Deduction style file)
%%% and Frank Pfenning's macros.tex

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Editorials
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\long\def\ednote#1{\footnote{[{\it #1\/}]}\message{ednote!}}
% \long\def\note#1{\begin{quote}[{\it #1\/}]\end{quote}\message{note!}}
\newenvironment{metanote}{\begin{quote}\message{note!}[\begingroup\it}%
	                 {\endgroup]\end{quote}}
\long\def\ignore#1{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Typesetting Deductions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newbox\tempa
\newbox\tempb
\newdimen\tempc
\newbox\tempd

\def\mud#1{\hfil $\displaystyle{#1}$\hfil}
\def\rig#1{\hfil $\displaystyle{#1}$}

\def\inruleanhelp#1#2#3{\setbox\tempa=\hbox{$\displaystyle{\mathstrut #2}$}%
                        \setbox\tempd=\hbox{$\; #3$}%
		        \setbox\tempb=\vbox{\halign{##\cr
	\mud{#1}\cr
	\noalign{\vskip\the\lineskip}%
	\noalign{\hrule height 0pt}%
	\rig{\vbox to 0pt{\vss\hbox to 0pt{\copy\tempd \hss}\vss}}\cr
	\noalign{\hrule}%
	\noalign{\vskip\the\lineskip}%
	\mud{\copy\tempa}\cr}}%
		      \tempc=\wd\tempb
		      \advance\tempc by \wd\tempa
		      \divide\tempc by 2 }

\def\inrulean#1#2#3{{\inruleanhelp{#1}{#2}{#3}%
		     \hbox to \wd\tempa{\hss \box\tempb \hss}}}
\def\inrulebn#1#2#3#4{\inrulean{#1\quad\qquad #2}{#3}{#4}}

\def\ian#1#2#3{{\lineskip 4pt\inrulean{#1}{#2}{#3}}}
\def\ibn#1#2#3#4{{\lineskip 4pt\inrulebn{#1}{#2}{#3}{#4}}}

\def\lowerhalf#1{\hbox{\raise -0.8\baselineskip\hbox{#1}}}

\def\ianc#1#2#3{{\lineskip 4pt\lowerhalf{\inruleanhelp{#1}{#2}{#3}%
		   \box\tempb\hskip\wd\tempd}}}
\def\ibnc#1#2#3#4{{\lineskip 4pt\ianc{#1\quad\qquad #2}{#3}{#4}}}

\def\rulespacing{\renewcommand{\arraystretch}{3} \arraycolsep 5em}
\def\rulestretch{\renewcommand{\arraystretch}{3}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Common abbreviations
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\bnfas{\mathrel{::=}}
\def\bnfalt{\mid}

\def\lam{\lambda}
\def\Lam{\Lambda}
\def\arrow{\rightarrow}
\def\oftp{\mathord{:}}
\def\hastype{\mathrel{:}}

\def\ldot{\mathord{.}\;}
\def\ldott{\mathrel{.}}
\def\oftpp{\mathrel{:}}
\def\dot{\cdot}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Judgments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\vd{\null\mathrel{\vdash}}
\def\gvd{\Gamma \vd}

\def\th{\null\mathrel{\vdash\!\!\!\vdash}}
\def\gth{\Gamma \th}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Code
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\ttuscore{{\tt\char`\_}}
\def\ttlbrace{{\tt\char`\{}}
\def\ttrbrace{{\tt\char`\}}}
% ----------------------------------------------------------------------
% Environments
% ----------------------------------------------------------------------

%\newenvironment{lfsig}{%
%   \par\vspace{\abovedisplayskip}\(\begin{array}{lcl@{\hspace{5em}}}}{%
%   \end{array}\)\par\vspace{\belowdisplayskip}\noindent\ignorespaces}
\newenvironment{lfsig}{%
   \par\vspace{\abovedisplayskip}\(\begin{array}{lcl}}{%
   \end{array}\)\par\vspace{\belowdisplayskip}\noindent\ignorespaces}

% \newenvironment{proofquote}{\begin{quote}\small}{\end{quote}}
% \newenvironment{proofquote}{\begin{minipage}{4.5in}}{\end{minipage}}
\long\def\proofbox#1{\[\fbox{\parbox{4.5in}{\small #1}}\]}

\newenvironment{progexample}{%
   \par\vspace{\abovedisplayskip}\(\begin{array}{@{\hspace*{2em}}l}}{%
   \end{array}\)\par\vspace{\belowdisplayskip}\noindent\ignorespaces}

% ----------------------------------------------------------------------
% Symbols
% ----------------------------------------------------------------------

\def\of{\mathrel{::}}

% --------------------
% Mini-ML (Chapter 1)
% --------------------

\def\z{\mbox{\bldf z\/}} % \def\z{\mbox{\cf z\/}}
\def\s{\mbox{\bldf s\/}} % \def\s{\mbox{\cf s\/}}
% \def\pred{\mbox{\cf pred\/}}
% \def\zerop{\mbox{\cf zerop\/}}
\def\fst{\mbox{\bldf fst\/}}
\def\snd{\mbox{\bldf snd\/}}
\def\llam{\mbox{\bldf lam\/}}
\def\llet{\mbox{\bldf let\/}}
\def\lletn{\mbox{\bldf let\/}\,\mbox{\bldf name}}
\def\lletv{\mbox{\bldf let\/}\,\mbox{\bldf val}}
\def\iin{\mbox{\bldf in\/}}
\def\fix{\mbox{\bldf fix\/}}

% evaluation

\def\evalsto{\mathrel{\hookrightarrow}}
% \def\ev#1{\mbox{\cf ev\_#1}}

% value proof

\def\value{\mbox{\it Value}}
% \def\val#1{\mbox{\cf val\_#1}}

% types

\def\nat{\mbox{\cf nat\/}}
\def\bool{\mbox{\cf bool\/}}
\def\cross{\times}
% \def\arrow{\mathrel{\rightarrow}} % in fpstd.

% typing rules

\def\vdml{\mathrel{\triangleright}}
\def\gvdml{\Gamma \vdml}
% \def\tp#1{\mbox{\cf tp\_#1}}

% conversion

% \def\conv{\simeq}
\def\conv{\equiv}
\def\freein{\mathrel{\mbox{free in}}}
\def\notfreein{\mathrel{\mbox{not free in}}}

% in exercises and language extensions

\def\ccase{\mbox{\bldf case\/}}
\def\oof{\mbox{\bldf of\/}}
\def\bar{\mathrel{\mbox{\tt |}}}
\def\to{\Rightarrow}
% \def\pair{\mbox{\bldf pair\/}}
\def\split{\mbox{\bldf split\/}}
\def\as{\mbox{\bldf as\/}}
\def\inl{\mbox{\bldf inl\/}}
\def\inr{\mbox{\bldf inr\/}}

\def\double{\mbox{\it double}}

\def\vnat{v_{\mbox{\scriptsize \rm nat}}}     % \sf not available this size!
\def\enat{e_{\mbox{\scriptsize \rm nat}}}     % \sf not available this size!
\def\mn#1{[\![#1]\!]}

% ---------------
% LF (Chapter 2)
% ---------------

\def\rep#1{\lceil #1 \rceil} % AMS symbol improvement:
% \def\rep#1{\ulcorner #1\urcorner}  % I don't have ulcorner and urcorner -- ceb
% \def\drep#1{\ulcorner\renewcommand{\arraystretch}{1}\arraycolsep 5pt
% \begin{array}[t]{c}#1\end{array}\urcorner}
\def\lf#1{\mbox{\rm #1}}
\def\type{\mbox{\rm type}}
\def\vdlf{\vd} % \def\vdlf{\vd^{\!\!\rm LF}}
\def\gvdlf{\Gamma \vdlf}
\def\vdslf{\vdlf_{\!\!\scriptscriptstyle \Sigma}}
\def\gvdslf{\Gamma \vdslf}
\def\isakind{\mbox{\it Kind}}
\def\isactx{\mbox{\it Ctx}}
\def\isasig{\mbox{\it Sig}}
% \def\valid{\mbox{\it Valid}}
% \def\betaconv{=_\beta}

\def\exp{\lf{exp}}
% \def\canon{\mathrel{:_c}} % \def\canon{\mathrel{\Rightarrow}}
% \def\atm{\mathrel{:_a}}   % \def\atm{\mathrel{\leadsto}}
\def\canon{\Uparrow}
\def\can{\canon}
\def\atm{\downarrow}

\def\vdsub#1{\vd_{\!\! \scriptscriptstyle #1}}
\def\gvdsub{\Gamma \vdsub}
\def\unrep#1{\llcorner #1\lrcorner}

\def\eval{\lf{eval}}
\def\tp{\lf{tp}}
\def\dvdml{\Delta \vdml}

\def\lfvalue{\lf{value}}
\def\redv{\Longrightarrow}
\def\vs{\lf{vs}}
\def\redtriv{\stackrel{\it triv}{\redv}}

\def\closed{\mbox{\it Closed}}
\def\open{\mbox{\it Open}}
\def\lfclosed{\lf{closed}}

\def\limplies{\mathbin{\supset}}

\def\inst{\preceq}
\def\vdsml{\mathrel{\triangleright\!\triangleright}}

% -----------------------
% Compilation (Chapter 6)
% -----------------------

\def\shift{\mathord{\uparrow}}

\def\trans{\mathrel{\leftrightarrow}}
\def\vtrans{\mathrel{\Leftrightarrow}}
\def\fevalsto{\evalsto}

\def\lletnp{\mbox{\bldf let\/}'\,\mbox{\bldf name}}
\def\lletvp{\mbox{\bldf let\/}'\,\mbox{\bldf val}}

\def\ssucc{\mbox{\cf succ\/}}

\def\KS{{\it KS}}
\def\amp{\mathop{\&}}
\def\done{{\it done}}
\def\St{{\it St}}
\def\goesto{\mathrel{\Longrightarrow}}
\def\goestos{\stackrel{*}{\Longrightarrow}}
\def\goestoss{\stackrel{*}{\makebox[0pt][l]{$\Longrightarrow$}\Longrightarrow}}
% \def\evl{\mbox{\it Evl}}

\def\return{\mbox{\bldf return}}
\def\answer{\mbox{\bldf answer}}
\def\init{\mbox{\bldf init}}
\def\cpm{\mathop{\diamond}}
\def\app{\mbox{\bldf app}}
\def\cevalsto{\stackrel{c}{\hookrightarrow}}
\def\corr{\sim}

% ------------------------------
% Natural Deduction (Chapter 7)
% ------------------------------


\def\andi{\land{\rm I}}
\def\andel{\land{\rm E}_{\rm L}}
\def\ander{\land{\rm E}_{\rm R}}
\def\oril{\lor{\rm I}_{\rm L}}
\def\orir{\lor{\rm I}_{\rm R}}
\def\ore{\lor{\rm E}}
\def\impi{\mathord{\limplies{\rm I}}}
\def\impe{\mathord{\limplies{\rm E}}}
\def\noti{\mathord{\lnot{\rm I}}}
\def\note{\mathord{\lnot{\rm E}}}
\def\topi{\top{\rm I}}
\def\bote{\mathord{\bot{\rm E}}}
\def\foralli{\forall{\rm I}}
\def\foralle{\forall{\rm E}}
\def\existsi{\exists{\rm I}}
\def\existse{\exists{\rm E}}

\def\botc{\mathord{\bot_C}}
\def\eec{\lnot\lnot_C}
\def\exm{{\rm XM}}

\def\ndredl{\quad\Longrightarrow_{L}\quad}
\def\rf{\quad\leadsto\quad}
\def\rfand{\quad\mbox{\it and}\quad}
\def\rfdone{\mbox{\it done}}
\def\rfs{\quad\stackrel{*}{\leadsto}\quad}

\def\lfi{\lf{i}}
\def\lfo{\lf{o}}
\def\lfnd{\lf{nd}}
\def\lfnj{\lf{nj}}
\def\lfnk{\lf{nk}}
\def\repA{\rep{A}}
\def\repB{\rep{B}}

\def\pfof{\mathrel{:\!\cdot}}

\def\lredl{\longrightarrow_{L}}
\def\lred{\longrightarrow}
\def\Arrow{\Rightarrow}
\def\abort{\mbox{\bf abort}}

\def\tpj{\mathrel{\triangleright}}
\def\gtpj{\Gamma \tpj}

\newsavebox{\tempsrc}

\def\bigrep#1{\savebox{\tempsrc}{$\displaystyle
#1$}\raisebox{\ht\tempsrc}{$\ulcorner$}\;\usebox{\tempsrc}\;\raisebox{\ht\tempsrc}{$\urcorner$}}

\def\nati{{\rm NI}}
\def\nate{{\rm NE}}
\def\prim{\mbox{\bldf prim\/}}

\def\ttrue{\mbox{\cf true\/}}
\def\ffalse{\mbox{\cf false\/}}
\def\iif{\mbox{\bldf if\/}}
\def\tthen{\mbox{\bldf then\/}}
\def\eelse{\mbox{\bldf else\/}}

\def\ue{\langle\, \rangle}
\def\extr#1{\left|#1\right|}
\def\iso{\cong}

% ------------------------------
% Logic Programming (Chapter 8)
% ------------------------------

\def\whr{\stackrel{whr}{\longrightarrow}}
\def\dvd{\Delta \vd}
\def\id{{\it id}}

\def\seq{\mathrel{\longrightarrow}}
\def\imm{\gg}
\def\sequ{\stackrel{u}{\seq}}
\def\dsequ{\Delta \sequ}

% \def\GL{{\cal GL}}
% \def\IM{{\cal IM}}
\def\SS{{\cal S}}
\def\II{{\cal I}}
\def\RR{{\cal R}}

\def\rsd{\mathrel{\backslash}}
\def\seqr{\stackrel{r}{\seq}}
\def\dseqr{\Delta \seqr}

\def\dseqc{D \stackrel{c}{\seq}}

% \def\gsequ{\Gamma \sequ}
% \def\gseqr{\Gamma \seqr}

% ----------------------------------------------------------------------
% Deductions
% ----------------------------------------------------------------------

\def\CC{{\cal C}}
\def\DD{{\cal D}}
\def\EE{{\cal E}}
\def\PP{{\cal P}}
\def\QQ{{\cal Q}}
\def\UU{{\cal U}}

\def\above#1#2{\begin{array}[b]{c}\relax #1\\ \relax #2\end{array}}
\def\abovec#1#2{\begin{array}{c}\relax #1\\ \relax #2\end{array}}

\def\cian#1#2#3{\ctr{\ianc{#1}{#2}{#3}}}
\def\cibn#1#2#3#4{\ctr{\ibnc{#1}{#2}{#3}{#4}}}

\def\hypo#1#2{\begin{array}[b]{c}\relax #1\\ \vdots \\ \relax #2\end{array}}
\def\hypoc#1#2{\begin{array}{c}\relax #1\\ \vdots \\ \relax #2\end{array}}
\def\hypol#1#2#3{\begin{array}[b]{c}\relax #1\\ #2 \\ \relax #3\end{array}}
\def\hypolc#1#2#3{\begin{array}{c}\relax #1\\ #2 \\ \relax #3\end{array}}

\def\ctr#1{\begin{array}{c} #1\end{array}}

\newsavebox{\tempded}
\newsavebox{\tempdedA}
\newsavebox{\tempdedB}

% ----------------------------------------------------------------------
% Indexing and Cross-Referencing
% ----------------------------------------------------------------------


% Chad's Macros
\def\kolm{\rightarrow}
\def\kolmand{\land{\rm K}}
\def\kolmimp{\limplies{\rm K}}
\def\kolmor{\lor{\rm K}}
\def\kolmnot{\lnot{\rm K}}
\def\kolmtrue{\top{\rm K}}
\def\kolmfalse{\bot{\rm K}}
\def\kolmforall{\forall{\rm K}}
\def\kolmexists{\exists{\rm K}}

\def\dne{\lnot\lnot{\rm R}} 
\def\dni{\lnot\lnot{\rm X}}
\def\tnr{\lnot\lnot\lnot{\rm R}}
\def\dnfe{\lnot\lnot\bot{\rm R}}

\def\nk{\vdash_{\rm NK}} 
\def\nj{\vdash_{\rm NJ}} 
\def\nd{\vdash}

\def\ail{{a_1,\ldots,a_n}}
\def\pol{{p_1,\ldots,p_m}}
%\def\kolmhyp{p_1\kolm \lnot\lnot p_1,\ldots,p_m\kolm \lnot\lnot p_m}
\def\hyl{H_1,\ldots,H_k}
\def\hypl{\nk H_1,\ldots,\nk H_k}
\def\hypjl{\nj H_1,\ldots,\nj H_k}
%\def\kolmhypl{KH_1::H_1\kolm H_1',\ldots,KH_k::H_k\kolm H_k'}
\def\hyplp{\nj H_1^*,\ldots,\nj H_k^*}


::::::::::::::
outline.tex
::::::::::::::

\section{Defaults for Line Numbers - a Specification}

There will never be an absolutely correct way
of assigning default for line numbers; we can merely make sure that
the result will always be logically correct - the rest is often
a matter of style and the kind of heuristics used.

Below we give a description of the tasks to be done by a function
\indexfunction{LINE-NO-DEFAULTS} which is called during the application of every
inference rule {in interactive mode}.  We will set the stage by giving
some, not necessarily exhaustive, examples of what meaning to assign
to the data structures and what output to expect from the function.

\subsection{The {\it support} data structure}

At each stage in a proof, we have associated with it a {\it \indexother{support}} structure,
which, for any given planned line ({\it \indexother{pline}}), tells us which deduced lines
({\it \indexother{dline}s}), we expect to use in the proof of the {\it pline}.

Thus the support structure is of the form
$$((p_1\; d_{11} \ldots d_{1x_1})\ldots (p_p\; d_{p1}\ldots d_{px_p}))$$

One may assume the following:
\begin{enumerate}
\item The $p_i$ are pairwise distinct.

\item The $d_{ik}$ are pairwise distinct for every fixed $i$ and $1\leq k \leq x_i$.

\item For each $i$, $d_{ik} < p_i$ for all $1\leq k \leq x_i$.

\item The planned lines $p_i$ are ordered such that the ones the user is
expected to work on first appear closer to the front.  In particular,
$p_1$ is the planned line worked on most recently.

\item Similarly, for a given $i$, the $d_{ik}$ are ordered such that the
one the user is expected to use first appear earlier.
\end{enumerate}

With each rule definition, there will be a description of how the {\it support}
structure changes.  This is given as two {\it support} structure templates,
using the name given to the lines in the rule specification.

\subsection{Examples}

The examples below are not complete, in the sense that not the full
description of the rule (for the rules module) is given, we have merely
extracted what is important in our context.  {\it p} and {\it d} are
placeholders for a {\it pline} or any number of {\it dlines}, respectively,
which are found in the support structure of the current proof, but are
merely copied in the application of the particular rule described.

\begin{verbatim}
Rule of Cases

*(D1)  H      !A(O) OR B(O)                                          
 (H2)  H,H2   !A(O)                                Case 1: D1
 (P3)  H,H2   !C(O)                                          
 (H4)  H,H4   !B(O)                                Case 2: D1
 (P5)  H,H4   !C(O)                                          
*(P6)  H      !C(O)                           Cases: D1 P3 P5

Support Transformation: (P6 D1 ss) ==> (P3 H2 ss) (P5 H4 ss) 
\end{verbatim}

Note that the specified support transformation tells \TPS what lines
it expects to be there, when the rule is applied, and which lines
should be new.  In this case, {\tt P} and {\tt Dab} are expected to be new,
the others are to be constructed.  Of course, these are only defaults,
and the user can apply the rule with any combination of lines present or
absent.


\begin{verbatim}
Induction Rule

 (D1) H     ! P 0
 (H2) H,H2  ! P m                           Inductive Assumption on m
 (D3) H,H2  ! P . Succ m
*(P4) H     ! FORALL n . NAT n IMPLIES P n           Induction: D1 D3

Support Transformation: (P4 ss) ==> (D1 ss) (D3 H2 ss)
\end{verbatim}

\begin{verbatim}
  Forward Conjunction Rule

 (P1)  H      !A(O)                                          
 (P2)  H      !B(O)                                          
*(P3)  H      !A(O) AND B(O)                               Conj: P1 P2

Support Transformation: (P3 ss) ==> (P1 ss) (P2 ss) 
\end{verbatim}


\begin{verbatim}
Backward Conjunction Rule

*(D1)  H      !A(O) AND B(O)                                          
 (D2)  H      !A(O)                                  Conj: D1
 (D3)  H      !B(O)                                  Conj: D1

Support Transformation: (pp D1 ss) ==> (pp D2 D3 ss) 
\end{verbatim}


\subsection{The LINE-NO-DEFAULTS functions}

There are two functions whose job it is to determine defaults for line
numbers.  The reason we need two functions is, that some of the lines
which appear on the left-hand side of the {\it support-transformation},
may reappear on the right.  The way we handle these connections, is
that we first determine the defaults for lines which are supposed
to exist (the left-hand side of the {\it support-transformation}), then
substitute those values into the right-hand side and call the second
default function.

The function \indexfunction{LINE-NO-DEFAULTS-FROM} is called with one argument
{\wt {line-no-defaults-from {\it default-exist}}} and \indexfunction{LINE-NO-DEFAULTS-TO}
is called with two arguments
{\wt {line-no-defaults-to {\it default-exist} {\it default-new}}}
where

\begin{description}
\item [{\it default-exist} ]  is the left hand side of the support transformation
specified for the rule, with lines that we need the default replaced
by a {\tt \$}, while the other lines are numbers (which means they either have
been figured out by an earlier default function or specified by the user).
Something which is neither {\tt \$} nor a number is one of the ``variables''
{\it d} or {\it p} standing for other {\it dlines} or {\it plines} in the current
{\it support} structure.  They must simply be returned (in the proper place,
of course).

\item [{\it default-new} ]  is the right hand side of the support transformation
specified for the rule, with the same interpretation as for {\it default-exist}.
\end{description}

The output of {\tt LINE-NO-DEFAULTS-FROM} should be a
{\tt {\it default-exists-figured}}, the output of {\tt LINE-NO-DEFAULTS-TO} is
a list {\wt ({\it default-exists-figured} {\it default-new-figured})} in
which all {\tt \$} of the arguments have been filled in.  These functions may
also do a {\tt THROWFAIL}, if one of the requirements R for logical
correctness cannot be satisfied in the given proof structure.

Also note that all lines in {\it default-exists} have already been determined,
when {\it default-new} is called.

The specification which must be meet by the {\tt LINE-NO-DEFAULTS-{\it x}}
functions can be grouped into three classes: requirements which ensure
the logical correctness of the rule application (R), requirements which
make the defaults sensible for the ``usual'' application of the rule (D)
and should never be deviated from, and desired properties, which need
not be satisfied, but approximate what the user would like to see most
of the time.

Note that the scope in this function is restricted by the fact that
it does not examine the logical structure assertions or hypotheses
of the lines in the proof.  This is accomplished by a completely different
mechanism and is not the responsibility of the function.  For instance,
it is perfectly sensible for {\tt LINE-NO-DEFAULTS} to suggest the first
{\it pline} in the current support structure for the backwards conjunction
rule, even though it may not be a conjunction at all! [This may cause mayhem
in rule tactics. The latter assumes that if there is a correct default,
the default function will choose it. Since a new line, properly located,
is always a correct, and possibly a useful default, tactics may miss
an opportunity to apply a rule.]

Subsequently, we will assume that
\begin{description}
\item [{\it default-exist} is ]  $((p_1\; d_{11} \ldots d_{1x_1})\ldots (p_p\; d_{p1}\ldots d_{px_p}))$

\item [{\it default-new} is ]  $((q_1\; e_{11} \ldots e_{1y_1})\ldots (q_q\; e_{q1}\ldots e_{qy_q}))$
\end{description}

\begin{center}
{\bf Requirements for Logical Correctness}
\end{center}

\begin{description}
\item [$R_{1}$ ]  $q_j < p_i$ for all $1\leq i \leq p$, $1\leq j \leq q$.

\item [$R_{2}$ ]  $e_{jk} < q_j$ for all $1\leq j \leq q$, $1\leq k \leq y_j$

\item [$R_{3}$ ]  $d_{ik} < p_i$ for all $1 \leq i \leq p$, $1\leq k \leq x_i$
\end{description}

\begin{center}
{\bf Sensible Defaults Requirements}
\end{center}

The requirements below only make sense, if the lines specified by the user
do not already violate them.  In that case, they must be relaxed to
apply only to the remaining unspecified lines.
\begin{description}
\item [$D_{1}$ ]  A {\it plan-support} pair suggested for an element of {\it default-exist}
must always match a {\it plan-support} pair in the current {\it support} structure
of the (incomplete) proof.

\item [$D_{2}$ ]  A {\it plan-support} pair suggested for an element of {\it default-new}
must consist of entirely new lines, and no two lines among all the suggested
defaults may have the same number.
\end{description}

\begin{center}
{\bf Wishful Thinking}
\end{center}

The following are constraints we would like to met, but is of course
not always possible.

\begin{description}
\item [$W_{1}$ ]  $q_j < q_{j+1}$ for all $1 \leq j < q$

\item [$W_{2}$ ]  $q_j < e_{j+1,k}$ for all $1 \leq j < q$ and 
$1 \leq j \leq q_{j+1}$

\item [$W_{3}$ ]  $d_{ik} < e_{jl}$ for all $1\leq i \leq p$,
$1\leq k \leq x_i$, $1\leq j \leq q$, $1\leq l \leq y_j$.

\item [$W_{4}$ ]  $\lnot\exists {\tt L}. \max{e_{jk}} < {\tt L} < q_j$ for
all $1 \leq j \leq q$.

\item [$W_{5}$ ]  Let $gap_{j} = q_j - \max{e_{jk}}$ for $1 \leq j \leq q$, if
$\{e_{jk} | 1 \leq k \leq y_j\}$ is non-empty, otherwise let $gap_{j} =
q_j - \max{\{{\tt L} | {\tt L} < q_j \lor \exists n\neq j . {\tt L} = q_n\}}$.
Then maximize $gap_{j}$, giving equal ``weight'' to all $1\leq j \leq q$.

\item [$W_{6}$ ]  Minimize $b = \min{e_{jl}} - \max{d_{ik}}$, with an alternative
similar to $W_{5}$ in case any of the sets is empty.

\item [$W_{7}$ ]  Minimize $t = \min{p_i} - \max{q_j}$, with an alternative similar
to $W_{5}$ in case any of the sets is empty.
\end{description}


\section{Updating the {\it support} structure}

Part of the execution of a rule application, is updating the plan
structure; this is one of the reasons why with every rule there 
comes a description of how the plan structure should be updated.
Below we will give a description of what the
function \indexfunction{UPDATE-PLAN} is supposed to accomplish, even in cases
when the rule is used in a way different from the defaults.  Again,
we assume the {\tt UPDATE-PLAN} is called as in 

{\tt (update-plan {\it default-exist} {\it default-new})} \\
where

\begin{description}
\item [{\it default-exist} is ]  $((p_1\; d_{11} \ldots d_{1x_1})\ldots (p_p\; d_{p1}\ldots d_{px_p}))$

\item [{\it default-new} is ]  $((q_1\; e_{11} \ldots e_{1y_1})\ldots (q_q\; e_{q1}\ldots e_{qy_q}))$
\end{description}

Recall that there may be variables appearing in place of a line number.
The following restrictions should be noted:

\begin{itemize}
\item For each {\it plan-support} pair
$(p_i\; d_{i1} \ldots  d_{ix_i})$ in {\it default-exist}, there is at most one
occurrence of a variable among $d_{i1},\ldots,d_{ix_i}$.

\item Any occurrence of a variable in {\it default-exist} is unique.
\end{itemize}

When {\tt UPDATE-PLAN} is called, all arguments are filled in, that is each
place is occupied either by a variable or a line number.

\subsection{{\it support} Structure Transformation in the Default Case}

If the rule is used completely in the default direction, i.e. all
{\it plan-support} pairs in {\it default-exist} exist in the current
{\it support} structure and all pairs in {\it default-new} consist of new lines,
then the effect of the rule application on the {\it support} structure is
straightforward:

\begin{itemize}
\item Delete all pairs matching $(p_i\; d_{i1}\ldots d_{ix_i})$ from the
{\it support} structure and attach to the front the pairs 
$(q_j\; e_{j1} \ldots e_{jy})$.

\item A variable in place of a $p_i$ matches {\bf any} {\it plan-support} pair in the
current proof, as long as the $d_{ik}$ match the corresponding support
lines.

\item A variable in place of a $d_{ik}$ matches the lines which are not matched by
any of the line numbers.  If $p_i$ is a variable, every match for $p_i$
produces a corresponding match of $d_{ik}$.

\item A variable in place of $q_j$ must occur as some $p_i$ and as many copies
of $(q_j\; e_{j1}\ldots e_{jy_j})$
are produced as there are matches of $p_i$.

\item A variable in place of $e_{jl}$ must occur as some $d_{ik}$ and the matched
list of lines in filled in.
\end{itemize}

\subsection{What if ...?}

We will go through all cases which differ from the default application
of the rule and specify what should happen to the {\it support} structure.
Of course, \TPS can not always correctly predict what the user had in
mind, when applying a rule, so the following must partly be considered
heuristics, but they will not always implement the user's devious
intentions.

\begin{enumerate}

\item % @tag(backtrack)
{\bf What if . . . } a $p_i$ exists, but is not a {\it pline?}  This case is
delicate and perhaps frequently occurs, if the user does not bother deleting
some lines before backtracking after some previous mistake.  Here execute
a {\tt PLAN-AGAIN} (which may become smart about support lines)\footnote{This is
a small project in itself!}.  This will make $p_i$ into a planned line and
we can handle it the usual way.

\item {\bf What if . . . } a $p_i$ does not exist?  Then, very likely, a rule meant to be used
backwards, was applied in a forward way.  We can't do much here: just ignore
the relevant part of {\it default-exist} completely.

\item {\bf What if . . . } a $p_i$ is a variable, but $d_{ik}$ don't match anything in the current
{\it support} structure.  This is already a special case of something discussed
in the previous section.

\item {\bf What if . . . } a $d_{ik}$ does not exist?  Then we must enter it as a planned line, collecting
$\{d_{il} | l \lneq k\}$ as its support lines.

\item {\bf What if . . . } a $d_{ik}$ does exist, but does not support $p_i$ ($p_i$ not a variable)?  Then
somehow $d_{ik}$ was improperly erased form the supports of $p_i$.
Just treat $d_{ik}$, as if it were supporting $p_i$.

\item {\bf What if . . . } a $q_j$ is a variable (thus exists as a {\it pline})
and matched a line number identical to a $e_{jk}$?  Then we are closing a
gap with a forward rule:  Do not enter the $j^{th}$ {\it plan-support} pair
into the {\it support} structure.

\item {\bf What if . . . } a $q_j$ already exists as a {\it pline}?  In this (probably very rare case)
we are reducing the proof of one planned line to the proof of another planned
line.  Add the $e_{jk}$ as additional support lines (also, of course, pulling it
to the front of the {\it support} structure).

\item {\bf What if . . . } a $q_j$ exists as a {\it dline}?  Here we already proved what we need,
so leave this {\it plan-support} pair out when constructing the new {\it support}
structure.

\item % @tag(dlineexist)
{\bf What if . . . } a $e_{jk}$ exists as a {\it dline}?  Here we may be in a situation similar to
~\ref{backtrack}.  The justification of $e_{jk}$ will be changed according to
the current rule applied.  As far as the {\it support} structure is concerned,
we don't treat it specially.

\item {\bf What if . . . } a $e_{jk}$ exists as a {\it pline}?  Here we are justifying a planned line.
Delete the {\it plan-support} pair for $e_{jk}$ from the current {\it support}
structure.  The justification of $e_{jk}$ will be changed appropriately.

\item {\bf What if . . . } a $e_{jk}$ exists as a {\it hline}?  If {\tt TREAT-HLINES-AS-DLINES} is {\tt T}, do
what you would do to a {\it dlines} (see ~\ref{dlineexist}).  Otherwise, nothing
special is done.

\end{enumerate}

\subsection{Entering Lines into the Proof Outline}

The descriptions in the previous section can, when read carefully, also
serve as a guide to what should happen when entering a line into the
proof outline.  Of course, what should be done is clear, if we are in the
all-default case.  Otherwise we may have to change some justifications
as indicated in the previous section, but otherwise existing lines are left
alone.

Entering lines into the proof could be taken over by the same function, if
we handed it linelabels instead of line numbers in {\it default-exist} and
{\it default-new}.

\section{Defaults for Sets of Hypothesis}

In \tps, the user will rarely ever have to deal explicitly with sets of
hypothesis.  However the detail can be controlled by a flag
called \indexflag{AUTO-GENERATE-HYPS}.  If this flag is {\tt T}, \TPS will not only
generate smart defaults for sets of hypothesis, but make them strong
defaults, which means that the user will never be asked to specify
hypotheses for a line.

There some restrictions on what the user of the {\tt RULES} module may
specify as hypothesis in a rule.  Ignoring for the moment the problem of
fixed hypotheses, like sets representing axioms of extensionality of an
axiom of infinity, the hypotheses for each line $l$ may have the form
$H, s_{1}, \ldots , s_{n}$, where $H$ is a
meta-notation for a set of lines and the $s_i$ are labels for lines
present elsewhere in the rule specification.  Let us use $H_l$ for
this set of specified hypotheses for line $l$.

Note the restriction that there may be only one variable standing for
``arbitrary'' sets of lines in any single rule description.

Defaults {strong or not} for the hypotheses of lines are only calculated
after all line numbers have been specified.  This includes existent and
non-existent lines equally.  The algorithm below will always generate
legal applications of the rule, at the same time generating the ``correct''
set of hypotheses for each line.  The algorithm will almost always be
adequate, in the sense that the user will almost never need to explicitly
add hypotheses to a deduced line or drop hypotheses from a planned line.
There are cases, however, where this may still be necessary (see discussion
below).

\subsection{The Algorithm}

Here, unlike in other parts of the {\tt OUTLINE} modules, we do not need to
refer to the {\it support} structure.  Instead let us view the rule as if
we were to infer the {\it plines} from all the {\it dlines} specified in the rule,
and let us disregard hypothesis lines ({\it hlines}) for the moment.

For a given line {\it l} (in the rule specification) we now let $S_l$ stand
for the set of lines in the hypotheses which were explicitly specified
in the rule description (corresponds to $s_1,\ldots,s_n$
above) and let $L_l$ the actual list of hypotheses for the
line, which must either be matched or constructed (depending on whether
the line existed or not).  Furthermore let $H$ stand for the unique
name for an ``arbitrary'' set of lines which appears in zero or more of
the lines in the rule description.

Let us first consider the case that the hypotheses specified in the rule
description do not contain $H$.  For {\it dlines} {\it d} we must check

$L_d\subseteq S_d$

and for {\it plines} {\it p} we need to check

$S_p \subseteq L_p$

For {\it dlines} $d$ which contain $H$ among their hypotheses,
we must satisfy

$L_d \subseteq H \cup S_d$

and, if we are filling in hypotheses for a new line, we would like to choose
$L_d$ as large as possible, so it satisfies this equation.  From another point
of view, namely when we match existent lines, we find out some constraint on
$H$:

$L_d \setminus S_d \subseteq H$

On the other hand, for any given {\it pline} $p$, we obtain

$H \cup S_p \subseteq L_p$ 

or equivalently 

$H \subseteq L_p$ and $S_p \subseteq L_p$

Here we would like to make $L_p$ as small as possible (the fewer hypotheses
we used, the stronger the statement result).  Alternatively, the second line
can again be viewed as a constraint on $H$ when matching an existent
{\it pline}.

This leads to the following algorithm for determining set of hypothesis:

\begin{enumerate}
\item Let $D_{exist}$ be the set of {\it dlines} which exist in the current proof.
Then set 
$$H_{lower} = \bigcup \{L_d - S_d | d \in D_{exist} \land
H \in H_d\}.$$
  Also let $L_d$ be the strong default for the
hypotheses of line $d$ for each $d\in D_{exists}$.

\item \label{latermodified} % @tag(latermodified)
Let $P_{exist}$ be the set of {\it plines} which exist in the current proof.
Then set 
$$H_{upper} = \bigcap \{L_p |  p \in P_{exist} \land
H\in H_p\}.$$
Also let $L_p$ be the strong default for the
hypotheses of line $p$ for each $p\in P_{exists}$.

\item If {\it not}  $H_{lower} \subseteq H_{upper}$   the application of the inference
rule is illegal. (Do a {\tt THROWFAIL} with proper message.)

\item If both, $H_{lower}$ and $H_{upper}$ are undefined (empty intersection or union,
respectively), do not fill in any further defaults.

\item If exactly one of $H_{lower}$ and $H_{upper}$ is undefined, let $H_{lower} := H_{upper}$
or vice versa.

\item For non-existent {\it dlines} {\it d}, we let $L_d = H_{upper} \cup S_d$.
If {\tt AUTO-GENERATE-HYPS} is {\tt T}, make $L_d$ the strong default for that
argument, otherwise just a regular default.

\item For non-existent {\it plines} {\it p}, we let $L_p = H_{lower} \cup S_p$.
If {\tt AUTO-GENERATE-HYPS} is {\tt T}, make $L_d$ the strong default for that
argument, otherwise just a regular default.
\end{enumerate}

This algorithm is coded in a separate function for each rule.
For the rule {\it rule}, the function is called {\wt {\it rule}-HYP-DEFAULTS}
and is called (when appropriate) form within {\wt {\it rule}-DEFAULTS}.

\subsection{When the Algorithm is not Sufficient}

We must of course consider the case, when a restriction like
``x not free in $H$'' is imposed upon applications of the inference
rule.  Since we fill in $H_{upper}$ for the hypotheses of the {\it dlines} which
do not exist, we must check whether ``x not free in $H_{upper}$''.  It may
be the case, however, that all {\it dlines} actually already existed.  In this
case, it would be sufficient for the validity of the rule application, to check
whether ``x not free in $H_{lower}$''.  To see this may think of the rule as
first a legal application of the inference rule, leaving out the extra
hypotheses, then enlarging the set of hypotheses of the inferred line,
possibly with lines which contain ``x'' free.

This situation can also come up, when  not all the {\it dlines} are specified.
Then we may have been able to make the inference rule application legal,
by leaving out the lines {\tt H} from $H_{upper}$, which violate the condition
``x not free in (the assertion of) {\tt H}''.

This leads to a simple modification of the algorithm above, which would need
much more information about the rule (namely the restrictions), where
we modify the definition of $H_{upper}$ in step ~\ref{latermodified} by 

~\ref{latermodified}$^{*}$.
$H_{upper} = \bigcap \{L_p | p \in P_{exist} \land H\in H_p
\land L_p \;{\rm satisfies\; any\; restriction\; on\; } H\}$.

It seems more reasonable, however, not to place that restriction, but rather
give an error message.  Otherwise the user may only find out much later, that
some of the hypotheses he expected to be able to use, have not been included
in the {\it dlines}, since they violated a restriction.  This makes it necessary,
however, to give the user explicit rules which allow adding hypotheses to
a deduced line or dropping hypotheses from a planned line.

\subsection{Hypothesis Lines}

There are two principal ways hypothesis lines ({\it hlines}) can be treated
in \TPS and since there is very little extra work required, both are
provided for.  The flag \indexflag{TREAT-HLINES-AS-DLINES} controls how hypotheses
lines are handled.

If {\tt TREAT-HLINES-AS-DLINES} is {\tt T}, an {\it hline} may have more
hypotheses than simply {\it hline}.  Also, {\it hlines} may have descriptive
justifications like ``Case {a}'' or ``Ind. Hyp. for n''.  The price you
pay is that hypotheses lines become unique to a subproof and should not
be used elsewhere.  In this case, {\it hlines} are truly treated as
{\it dlines}, and in the above algorithm for determining default for
lines, we mean {\it dline} or {\it hline} whenever we say {\it dline}.

If {\tt TREAT-HLINES-AS-DLINES} is {\tt NIL}, every {\it hline} has exactly
one hypothesis: itself.  Also the justification for any {\it hline} will
be the same, namely the value of the flag \indexflag{HLINE-JUSTIFICATION}
(by default {\tt Hyp}).  What you gain in this case is, that the same
hypothesis line may used many different places in the given proof.  The
default for the hypotheses of an {\it hline} will always be strong and
equal to {\tt ({\it hline})}, anything else will result in an error, even if
perhaps logically correct. Also, in this case, 
if \indexflag{CLEANUP-SAME} is {\tt T}, then \indexcommand{CLEANUP} will
eliminate unnecessary hypotheses.

::::::::::::::
path.tex
::::::::::::::
\begin{enumerate}
\item {\em Natural ordering} for paths based on the jform. Defined by 
induction on jforms. Let paths(J) denote the set of all full paths through 
jform J.
\begin{itemize}
\item $J = \bigvee^{n}_{i=1} jform_{i}$. Let $P_{i} \in paths(jform_{i})$. Then
$P_{i} < P_{j}$ if $i < j$.

\item $J = \bigwedge^{n}_{i=1} jform_{i}$. Let $P_{i} \in paths(jform_{i})$. 
Then \[P_{1}@P_{2}\ldots @P_{n} < P_{1}@P_{2}\ldots%
@P_{j-1}@P^{1}_{j}\ldots @P^{1}_{n} \mbox{\/\rm if\/} P_{j} < P^{1}_{j}\]

\end{itemize}

\item {\em Cheapest Paths}. By Induction on Jforms.
\begin{itemize}
\item $J = \bigvee^{n}_{i=1} jform_{i}$. Cheapest path in $paths(jform_{1})$.

\item $J = \bigwedge^{n}_{i=1} jform_{i}$. Let $P_{i} \in paths(jform_{i})$
such that $P_{i}$ is the cheapest path in $paths(jform_{i})$.
Then $P_{1}@P_{2}\ldots @P_{n}$ is the cheapest path in $J$.

\end{itemize}

\item {\em Finding the next path}. Given a path P, find the next path in
the ordering described above.

\begin{enumerate}
\item Backtracking to the last point at which there is an alternate choice.
This is the last disjunction where another choice can be made. Because we
are using full paths, this just requires removing elements from the path P
until such a disjunction is found.

\item Find the next path through this disjunction.

\item Complete this path wrt the entire jform. This requires starting from
the beginning until disjunction in step 1 is reached and then finding
the cheapest path.
\end{enumerate}
\end{enumerate}
::::::::::::::
prog-tactics.tex
::::::::::::::



\subsection{Implementation of tactics and tacticals}

\begin{enumerate}
\item Main files (in order of importance): tactics-macros, tacticals,
tacticals-macros, tactics-aux.  These files contain tactic-related functions 
of a general nature.  Most tactics are actually contained in other Lisp packages.

\item When a tactic is executed, two global variables affect its
execution: tacuse (the tactic's use), and tacmode (the current mode).
Tacuse determines for what reason the tactic is being called.  Current
uses are etree-nat (translation of \indexother{eproof} to natural deduction) and
nat-ded (construction of a natural deduction proof without any mating
information).  A single tactic may be defined for more than one use.
Tacmode can have the value of either auto or interactive.  Each tactic
should take this value into account during operation.  In general,
this means that when the value is interactive, the user should be
advised that the tactic is about to be applied and should be allowed
to abort it.  When the value is auto, the tactic should just be
carried out if applicable.  

\item For each use, a number of auxiliary functions needed by the
tacticals must be defined.

\begin{enumerate}
\item get-tac-goal: if a goal has not been specified, get the next one,
e.g., the active planned line.

\item copy-tac-goal: copy the current goal into a new goal, so that subsequent
actions can be performed without destroying the current goal.  Allows
later backtracking if necessary.

\item save-tac-goal: put the current goal into a form suitable for
saving.  This is not actually used by current tactics.

\item restore-tac-goal: backtrack to the previous goal. 
This is not actually used by current tactics.

\item update-tac-goal: given the old (saved) goal, and the new goal on which
some progress has been made, update the old goal to reflect the
progress made.
\end{enumerate}

Tacticals must be independent of the value of tacuse.  
They cannot make any assumptions
about the structure of the goals, etc.

The main function used in applying tactics is apply-tactic.  This
is a function that takes a tactic as argument, and allows keyword
arguments of :goal, :use and :mode.  If not specified, the use and
mode default to the global values of tacuse and tacmode.
If they are specified,
the values given then override the global values of tacuse and tacmode.
apply-tactic and (every tactic) returns four values.  
 The first is a
list of goals, the second a string with some kind of message, the
third a token which indicates the result of the tactic and the fourth
a validation, which, if non-nil, should be a function which specifies
how solutions to the returned goals can be combined to solve the original goal.

apply-tactic works as follows:

\begin{enumerate}
\item  Checks that tactic is a valid tactic.

\item If a goal has not been specified, calls get-tac-goal.

\item If the tactic is an atom:
  \begin{enumerate}
\item     gets the tactic's definition for the use.

\item     if the definition is primitive, and the goal is nil, return
(nil "Goals exhausted." 'complete nil).  If the goal is non-nil, apply the 
tactic to the goal.

\item     if the definition is compound, call apply-tactical on the
definition and the goal.
  \end{enumerate}

\item  If the tactic's definition begins with a tactic, call apply-tactic
recursively,  using those optional arguments.

\item  If the tactic begins with a tactical, call apply-tactical.
\end{enumerate}

 Whenever a tactic begins with a tactical, the function
apply-tactical is used.  It takes two arguments, a goal and a tactic.
It is assumed that the tactic begins with a tactical.  
apply-tactical works as follows:


\begin{enumerate}
\item Get the definition for the tactical.

\item If the definition is primitive (a lambda expression), funcall the
definition on the goal and the remainder (cdr) of the tactic. 

\item If the definition is compound (i.e., is defined in terms of other
tacticals and begins with tac-lambda), expand the definition,
substituting the arguments provided in the tactic's definition for the
dummy  arguments in the tactical's definition.  Then call
apply-tactical recursively.

\item Otherwise abort, returning abort as the token (third value
returned).
\end{enumerate}

\item  Validations:  Though the validation mechanism is in place, no use
is made of them in the current tactic uses, since any changes in the
constructed proofs are made immediately, not saved.  Validations must
be modified as tacticals are executed, since during their execution,
the order of goals may be changed.  For example, a tactical may
repeatedly apply a tactic to a goal, then to all the new goals
created, etc., until it fails on all of them.  When it succeeds on a
successor goal, the validation returned must be integrated into the validation
which was returned for the first application of the tactic on the
original goal.  The function make-validation is used for this purpose.
\end{enumerate}
::::::::::::::
rules.tex
::::::::::::::
\chapter{Rules Module}

Information on how to write new rules is in the User Manual.

The chapter about \ETPS (chapter ~\ref{etps}) has some discussion
of how default line numbers and default hypotheses are generated in 
the various {\tt OUTLINE} modules. This should correspond fairly 
closely to the way in which the automatically-generated rules
generate their defaults. The same chapter also has a discussion of
support transformations which illustrates the way in which rules are
defined.

{\bf Do not edit the automatically-generated files.} This means any file 
{\it whatever.lisp} that has a companion file {\it whatever.rules}. If you edit 
the lisp files directly, your changes will be lost if the rules are ever recompiled.

::::::::::::::
tactics.tex
::::::::::::::
\chapter{Tactics and Tacticals}

% I have no idea who wanted to modify this file, or why. - cebrown 2/3/00
{\bf Modify tactics.tex}  

\section{Overview}

Ordinarily in \tps, the user proceeds by performing a series of
atomic actions, each one specified directly.  For example, in constructing
a proof, she may first apply the deduct rule, then the rule of cases, then
the deduct rule again, etc..  These actions are related temporally, but
not necessarily in any other way; the goal which is attacked by 
one action may result in several new goals, yet there is no distinction
between goals produced by one action and those produced by another.  
In addition, this use of small steps prohibits the user from outlining
a general procedure
to be followed.  A complex strategy cannot be expressed in these terms, 
and thus
the user must resign herself to proceeding by plodding along, using simple
(often trivial and tedious) applications of rules.

Tactics offer a way to encode strategies into new commands, using a 
goal-oriented approach.  With the use of tacticals, more complex tactics
(and hence strategies) may be built.  Tactics and tacticals are, in essence,
a programming language in which one may specify techniques for solving
goals.

Tactics are called  partial subgoaling methods by
\cite{Gordon79}.  What this means is that a tactic is a
function which, given a goal to be accomplished,
will return a list of new goals, along with a procedure by which the
original goal can be achieved given that the new goals are first
achieved.  Tactics also may fail, that is, they may not be applicable
to the goal with which they are invoked.

Tacticals operate upon tactics in much the same way that functionals
operate upon functions.  By the use of tacticals, one may create
a tactic that repeatedly carries out a single tactic, or composes
two or more tactics.  This allows one to combine many small tactics
into a large tactic which represents a general strategy for solving goals.

As implemented in \tps, a tactic is a function which takes a goal
as an argument and returns four values: a list of new goals, a message
which tells what the tactic did (or didn't do), a token indicating what
the tactic did, and a validation, which is a lambda expression which takes
as many arguments as the number of new goals, and which, given solutions
for the new goals, combines the solutions into a solution for the original
goal. It is possible that the validation is used nowhere in the code, and
that it should be phased out.

Consider this example.  Suppose we are trying to define tactics which
will convert an arithmetic expression in infix form to one in
prefix form and evaluate it.  One tactic might,
if given a goal of the form "A / B", where A and B are themselves 
arithmetic expressions in infix form, return the list ("A" "B"),
some message, the token "succeed", and the 
validation {\tt (lambda (x y) (/ x y))}.  If now we solve the new goals "A"
and "B" (i.e., find their prefix forms and evaluate them),
and apply the validation as a function to their solutions, we get
a solution to the original goal "A / B".

When we use a tactic, we must know for what purpose the tactic is being
invoked.  We call this purpose the {\it use} of the tactic.  Some examples
of uses are {\tt nat-ded} for carrying out natural deduction proofs, {\tt nat-etree}
for translating natural deduction proofs to expansion proofs (not yet implemented), and {\tt etree-nat}
for translating expansion proofs to natural deductions.  A single tactic may
have definitions for each of these uses.  In contrast to tactics, tacticals
are defined independent of any specific tactic use; some of the auxiliary
functions they use, however, such as copying the current goal, may depend
upon the current tactic use.  For this purpose, the current tactic use
is determined by the flag \indexflag{tacuse}.  Resetting this flag resets
the default tactic use. Though a tactic can be called
with only a single use, that tactic can call other tactics with different
uses.  See the examples in the section "Using Tactics".

Another important parameter used by a tactic is the {\it mode}.  There are
two tactic modes, {\tt auto} and {\tt interactive}.  The definition of
a tactic may make a distinction between these two modes; the current
mode is determined by the flag  \index{tacmode}, and resetting this
flag resets the default tactic mode.  Ideally, a tactic
operating in {\tt auto} mode should require no input from the user, while
a tactic in {\tt interactive} mode may request that the user make some decisions,
e.g., that the tactic actually be carried out.  It may be desirable, however,
that some tactics ignore the mode, compound tactics (those tactics created
by the use of tacticals and other tactics) among them.

One may wish to have tactics print informative messages as they operate;
the flag \indexflag{tactic-verbose} can be set to T to allow this to
occur, and tactics can be defined so that messages are printed when
{\tt tactic-verbose} is so set. Each tactic should call the function {\it tactic-output} 
with two arguments. The first argument should be a string containing the information to be
printed, and the second argument T if the tactic succeeds, and NIL
otherwise.  {\it tactic-output} will, depending on the second argument and
the current value of {\tt tactic-verbose}, either print or not print the
first argument.

\section{Syntax for Tactics and Tacticals}

The \TPS category for tactics is called {\tt tactic}.  The defining 
function for tactics is {\tt deftactic}. The variable {\tt auto::*global-tacticlist*}
contains a list of all tactics. Each tactic definition has
the following form:

%\begin{programexample}
\begin{verbatim}
(deftactic tactic
  {(<tactic-use> <tactic-defn> [<help-string>])}+
)
\end{verbatim}
%\end{programexample}

with components defined below:

% \begin{Format}
% @tabclear
% @tabset(1.5 inches)
\begin{tabular}{ll}
\indexSyntax{tactic-use} ::= & {\tt nat-ded | nat-etree | etree-nat} \\
\indexSyntax{tactic-mode} ::= & {\tt auto | interactive} \\
\indexSyntax{tactic-defn} ::= & {\it primitive-tactic} | {\it compound-tactic} \\
\indexSyntax{primitive-tactic} ::= & {\tt (lambda (goal) {{\it form}}*)} \\
 & This lambda expression should return four values of the form:  \\
 & {\it goal-list msg token validation}. \\
\indexSyntax{compound-tactic}::= & ({\it tactical} {{\it tactic-exp}}*) \\
\indexSyntax{tactic-exp}::= & {\tt tactic}  a tactic which is already defined \\
 & | ({\it tactic} {\tt [:use {\it tactic-use}] [:mode {\it tactic-mode}] [:goal {\it goal}]}) \\
 & | {\it compound-tactic} \\
 & | ({\tt call} {\it command}) ; where {\it command} is a command which could be \\
 & given at the \TPS top level \\
\indexSyntax{goal} ::= & a goal, which depends on the tactic's use,  \\
 & e.g., a planned line when the tactic-use is {\tt nat-ded}. \\
\indexSyntax{goal-list} ::= & ({{\it goal}}*) \\
\indexSyntax{msg} ::= & {\it string} \\
\indexSyntax{token} ::= & {\tt complete}  meaning that all goals have been exhausted \\
 & {\tt | succeed}  meaning that the tactic has succeeded \\
 & {\tt | nil}  meaning that the tactic was called only for side effect \\
 & {\tt | fail}  meaning that the tactic was not applicable  \\
 & {\tt | abort}  meaning that something has gone wrong, such as an undefined  \\
 & tactic \\
\end{tabular}
% \end{format}

Tacticals are kept in the \TPS category {\tt tactical}, with defining
function {\tt deftactical}.  Their definition has the following form:

%\begin{programexample}
\begin{verbatim}
(deftactical tactical
  (defn <tacl-defn>)
  (mhelp <string>))
\end{verbatim}
%\end{programexample}

with 

% \begin{format}@tabclear
% @tabset(1.5 inches)
\begin{tabular}{ll}
\indexSyntax{tacl-defn} ::= & {\it primitive-tacl-defn | compound-tacl-defn} \\
\indexSyntax{primitive-tacl-defn} ::= & {\tt (lambda (goal tac-list) {{\it form}}*)} \\
 & This lambda-expression, where {\tt tac-list} stands for a possibly \\
 & empty list of tactic-exp's, should be independent of the tactic's  \\
 & use and current mode.  It should return values like those returned  \\
 & by a {\it primitive-tac-defn}. \\
\indexSyntax{compound-tacl-defn} ::= & {\tt (tac-lambda ({{\it symbol}}*) {\it tactic-exp})}  \\
 & Here the tactic-exp should use the symbols in the \\
 & tac-lambda-list as dummy variables. \\
\end{tabular}
%\end{format}

\pagebreak
Here is an example of a definition of a primitive tactic.
%\begin{programexample}
\begin{verbatim}
(deftactic finished-p
 (nat-ded 
  (lambda (goal)
    (if (proof-plans dproof)
	(progn
	 (when tactic-verbose (msgf "Proof not complete." t))
	 (values nil "Proof not complete." 'fail))
	(progn
	 (when tactic-verbose (msgf "Proof complete." t))
	 (values nil "Proof complete." 'succeed))))
  "Returns success if all goals have been met, otherwise
returns failure."))
\end{verbatim}
%\end{programexample}

This tactic is defined for just one use, namely {\tt nat-ded}, or natural
deduction.  It merely checks to see whether there are any planned lines
in the current proof, returning failure if any remain, otherwise
returning success.  This tactic is used only as a predicate, so the
goal-list it returns is nil, as is the validation.

As an example of a compound tactic, we have
%\begin{programexample}
\begin{verbatim}
(deftactic make-nice
  (nat-ded
   (sequence (call cleanup) (call squeeze) (call pall))
   "Calls commands to clean up the proof, squeeze the line 
numbers, and then print the result."))
\end{verbatim}
%\end{programexample}

Again, this tactic is defined only for the use {\tt nat-ded}.  {\tt sequence} is
a tactical which calls the tactic expressions given it as arguments
in succession.

Here is an example of a primitive tactical.
%\begin{programexample}
\begin{verbatim}
(deftactical idtac
  (defn
    (lambda (goal tac-list)
      (values (if goal (list goal)) "IDTAC" 'succeed 
	      '(lambda (x) x))))
  (mhelp "Tactical which always succeeds, returns its goal 
unchanged."))
\end{verbatim}
%\end{programexample}

The following is an example of a compound tactical.  {\tt then} and {\tt orelse} are tacticals.

%\begin{programexample}
\begin{verbatim}
(deftactical then*
  (defn 
    (tac-lambda (tac1 tac2)
      (then tac1 (then (orelse tac2 (idtac)) (idtac)))))
  (mhelp "(THEN* tactic1 tactic2) will first apply tactic1; if it
fails then failure is returned, otherwise tactic2 is applied to 
each resulting goal.  If tactic2 fails on any of these goals, 
then the new goals obtained as a result of applying tactic1 are 
returned, otherwise the new goals obtained as the result of 
applying both tactic1 and tactic2 are returned."))
\end{verbatim}
%\end{programexample}

\section{Tacticals}
There are several tacticals available.  Many of them are taken directly from
\cite{Gordon79}.  After the name of each tactical is given
an example of how it is used, followed by a description of the behavior
of the tactical
when called with {\tt goal} as its goal.  The newgoals and validation returned
are described only when the tactical succeeds.


\begin{enumerate}
\item {\tt idtac: (idtac)} \\
Returns {\tt (goal), (lambda (x) x)}.

\item {\tt failtac: (failtac)}\\
Returns failure

\item {\tt call: (call command)}\\
Executes command as if it were entered at top level of \tps.  This is used
only for side-effects.  Returns {\tt (goal), (lambda (x) x)}.

\item {\tt orelse: (orelse tactic1 tactic2 ... tacticN)}\\
If N=0 return failure, else apply {\tt tactic1} to {\tt goal}. If this fails, call {\tt (orelse tactic2 tactic3 ... tacticN)} on {\tt goal}, else return the result of applying {\tt tactic1} to {\tt goal}.\\

\item {\tt then: (then tactic1 tactic2)}\\
Apply {\tt tactic1} to {\tt goal}. If this fails, return failure, else apply {\tt tactic2} to each of the subgoals generated by {\tt tactic1}.\\
If this fails on any subgoal, return failure, else return the list of new subgoals returned from the calls to {\tt tactic2}, and the lambda-expression representing the combination of applying {\tt tactic1} followed by {\tt tactic2}.\\
Note that if {\tt tactic1} returns no subgoals, {\tt tactic2} will not be called.

\item {\tt repeat: (repeat tactic)}\\
Behaves like {\tt (orelse (then tactic (repeat tactic)) (idtac))}.

\item {\tt then*: (then* tactic1 tactic2)}\\
Defined by:\\
{\tt (then tactic1 (then (orelse tactic2 (idtac)) (idtac)))}.  This tactical is taken from \cite{Felty86}.

\item {\tt then**: (then** tactic1 tactic2)}\\
Acts like {\tt then}, except that no copying of the goal or related structures will be done. 

\item {\tt ifthen: (ifthen test tactic1)} or \\
        {\tt (ifthen test tactic1 tactic2)}\\
First evaluates {\tt test}, which may be either a tactic or (if user is an expert) an arbitrary LISP expression.  If test is a tactic and does not fail, or is an arbitrary LISP expression that does not evaluate to nil, then {\tt tactic1} will be called on {\tt goal} and its results returned. Otherwise, if {\tt tactic2} is present, the results of calling {\tt tactic2} on {\tt goal} will be returned, else failure is returned.  {\tt test} should be some kind of predicate; any new subgoals it returns will be ignored by {\tt ifthen}.

\item {\tt sequence: (sequence tactic1 tactic2 ... tacticN)}\\
Applies {\tt tactic1, ... , tacticN} in succession regardless of their success or failure.  Their results are composed.

\item {\tt compose: (compose tactic1 ... tacticN)}\\
Applies {\tt tactic1, ..., tacticN} in succession, composing their results until one of them fails.  Defined by:\\
{\tt (idtac)} if {\tt N}=0\\
{\tt (then* tactic1 (compose tactic2 ... tacticN))} if {\tt N} > 0.

\item {\tt try: (try tactic)}\\
Defined by: {\tt (then tactic (failtac))}.  Succeeds only if tactic returns no new subgoals, in which case it returns the results from applying {\tt tactic}. 

\item {\tt no-goal: (no-goal)}\\
Succeeds iff goal is nil.
\end{enumerate}


\section{Using Tactics}

To use a tactic from the top level, the command \indexmexpr{use-tactic} has
been defined.  Use-tactic takes three arguments: a {\it tactic-exp}, a 
{\it tactic-use},
and a {\it tactic-mode}.  The last two arguments default to the values of
\indexflag{tacuse} and \indexflag{tacmode}, respectively.
Remember that a {\it tactic-exp} can be either the name of
a tactic or a compound tactic.  Here are some examples:

%\begin{group}
\begin{verbatim}
<1> use-tactic propositional nat-ded auto

<2> use-tactic (repeat (orelse same-tac deduct-tac)) 
               $ interactive

<3> use-tactic (sequence (call pall) (call cleanup) (call pall)) !

<4> use-tactic (sequence (foo :use nat-etree :mode auto) 
                         (bar :use nat-ded :mode interactive)) !
\end{verbatim}
%\end{group}
Note that in the fourth example, the default use and mode are overridden
by the keyword specifications in the tactic-exp itself.  Thus during the
execution of this compound tactic, {\tt foo} will be called for one use and
in one mode, then {\tt bar} will be called with a different use and mode.

Remember, setting the value of the flag \indexflag{tactic-verbose} to T will
cause the tactics to send informative messages as they execute.

\input{prog-tactics}
::::::::::::::
tar.tex
::::::::::::::

To make a tar file for distribution via ftp:
\begin{enumerate}

\item Login to gtps.math.cmu.edu.
Delete any old tar files in /home/ftp/pub, or move them to
/home/ftp/pub/old.

\item {\tt cd /home/theorem/project/dist}.

\item Clean up /home/theorem/project/dist and its subdirectories by deleting
each backup, postscript, or dvi file.
% @comment[See /afs/cs/usr/andrews/clean.hlp if you want to automate this.]

\item While in the directory /home/theorem/project/dist,
execute the following command at the Unix prompt:
{\tt tar cvhf /home/ftp/pub/tps3-date.tar .}
(where {\tt date} is the current date).
A list of file names should pass by on the screen.  Just watch until
you get a new Unix prompt.

\item {\tt cd /home/ftp/pub}.

\item If you wish to check the tar file, enter
{\tt tar tvf tps3-date.tar}.
The same list of file names should pass by.  These are the names of
the files which are now in the tar file.

\item {\tt compress tps3-date.tar}. This will create the file
tps3-date.tar.Z, which is ready for people to transfer using ftp.

\end{enumerate}

To make a tar archive onto a big round mag tape (the kind that is seen
in science fiction movies of the sixties and seventies, always
spinning aimlessly, supposedly to suggest the immense computing power
of some behemoth machine):
\begin{enumerate}
\item Go to the CS operator's room on the third floor, at the end of the
Wean Hall 3600 hallway.  

\item Tell the operator that you wish to write a tar tape from the machine
K.GP.  Give her the tape and go back to the terminal room. 

\item Log in on the K.

\item At the Unix prompt, enter {\tt assign mt}.  This gives you control of
the mag tape units.  

\item Enter {\tt cd /home/theorem/project/dist}.  This puts you in the proper
directory.

\item Clean up /home/theorem/project/dist and its subdirectories by deleting
each backup, postscript, or dvi file.

\item Determine which device {\it devname} you wish to use.  This depends on the 
density which you wish to write.  For 6250 bpi, let {\it devname} be
{\tt /dev/rmt16}, for 1600 bpi, let {\it devname} be {\tt /dev/rmt8}, and
for 800 bpi, let {\it devname} be {\tt /dev/rmt0}.  Generally, you can go
for 6250 bpi unless the intended recipient has indicated otherwise.

\item Execute the following command at the Unix prompt:
{\tt tar rvhf {\it devname} .}

\item A list of file names should pass by on the screen.  Just watch until
you get a new Unix prompt.

\item To check the tape, enter
{\tt tar tvf {\it devname}}.
The same list of file names should pass by.  These are the names of
the files which are now on the tape.  If there were already files on
the tape, you will see all of them listed as well.

\item If all is well, call the operator and tell them that you are done with
the tape and that they can dismount it.  Then execute {\tt exit} at the
Unix prompt, to give up control of the tape drive, and log out as usual.

\item Go back to the operator's room and pick up the tape.

\end{enumerate}

To make a tar archive onto a Sun cartridge tape:
\begin{enumerate}
\item Take the tape to the CS operator, and ask her to put it on the machine
O.GP.  That is a Sun3 running Mach.  Go back to a terminal and log in on
any machine.  Again, you want to be in the directory /home/theorem/project/dist.

\item Now, you need to make sure that you can write to the O's tape drive.
You want to check the owner of the file /../o/dev/rst8, and make sure it's
{\tt rfsd}.  If not, call the CS operator and ask them to assign it to rfsd so
that you can make the tar tape.  

\item Now, execute the following command at the Unix prompt:
{\tt tar cvhf /../o/dev/rst8 .}

\item A list of file names should pass by on the screen.  It will be very slow.

\item After you get a new Unix prompt, wait a few seconds for the tape to rewind, 
then check it by entering
{\tt tar tvf /../o/dev/rst8}.
The same list of file names should pass by.  These are the names of
the files which are now on the tape.  

\item Go back to the operator and ask for the tape.

\end{enumerate}
::::::::::::::
teach.tex
::::::::::::::
\chapter{Teaching Records}\label{Teach}

\section{Events in TPS3}

The primary purpose of events in \TPS is to collect information about
the usage of the system.  That includes support of features such as
automatic grading of exercises and keeping statistics on the application of
inference rules.

Events, once defined and initialized, can be signalled from anywhere in
\tps.  Settings of flags, ordinarily collected into modes, control if,
when, and where signalled events are recorded. Siganlling of events can 
be suppressed by changing the values of the flags in subject \indexother{EVENTS}.
Notice that this, of course, only suppresses the signalling, not the events
themselves!

In \ETPS, a basic set of events is predefined, and the events are signalled
automatically whenever appropriate.  If these events are then recorded
depends on your \ETPS profile.

There are some restrictions on events that should be respected, if
you plan to use {\tt REPORT} to extract statistics from the files recording
events.  Most importantly: {\bf No two events should be written to the
same file.}  If you would like to record different things into the
same file, make one event with one template and allow several kinds of
occurrences of the event.  For an example, see the event {\tt PROOF-ACTION}
below.

\subsection{Defining an Event}

If you are using \ETPS, it is unlikely that you need to define an event
yourself.  However, a lot of general information about events is given
in the following description.

Events are defined using the {\tt DEFEVENT} macro.
Its format is

\begin{verbatim}
(defevent <name>
  (event-args <arg1> ... <argn>)
  (template <list>)
  (template-names <list>)
  (signal-hook <hook-function>)
  (write-when <write-when>)
  (write-file <file-parameter>)
  (write-hook <hook-function>)
  (mhelp <help-string>))
\end{verbatim}

\begin{description}

\item [{\tt event-args}]  list of arguments passed on by {\tt SIGNAL-EVENT} for any event
	of this kind.

\item [{\tt template}]  constructs the list to be written.  
        It is not assumed that every event is
	time-stamped or has the user-id.  The template
        must only contain globally evaluable forms and the arguments
	of the particular event signalled.  It could be the source of
        subtle bugs, if some variables are not declared special.

\item [{\tt template-names}]  names for the individual entries in the template.
These names are used by the {\tt REPORT} facility.  As general conventions,
when the template form is a variable, use the same name for the
template name (e.g. {\tt DPROOF}).  If the template form is {\tt (STATUS {\it statusfn})}
use {\it statusfn} as the template name (e.g. {\tt DATE} for {\tt (STATUS DATE)} or
{\tt USERID} for {\tt (STATUS USERID)}).

\item [{\tt signal-hook}]  an optional function to be called whenever the
	the event is signalled.  This should {\bf not} to the writing of
	the information, but may be used to do something else.  If the
        function does a {\tt THROWFAIL}, the calling {\tt SIGNAL-EVENT} will
        return {\tt NIL}, which means failure of the event.  The arguments
        of the function should be the same as {\tt EVENT-ARGS}.

\item [{\tt write-when}]  one of {\tt IMMEDIATE}, {\tt NEVER}, or an integer {\it n}, which means
     to write after an implementation depended period of {\it n}.
     At the moment this will write, whenever the number of inputs = {\it n}
     * {\tt EVENT-CYCLE}, where {\tt EVENT-CYCLE} is a global variable, say 5.

\item [{\tt write-file}]  the name of the global {\tt FLAG} with the filename of the
     file for the message to be appended to.

\item [{\tt write-hook}]  an optional function to be called whenever a number
	(>0) of events are written.  Its first argument is the file it will
        write to, if the write-hook returns.  Its second argument is the
        list of evaluated templates to be written.  If an event is to be
        written immediately, this will always be a list of length 1.

\item [{\tt mhelp}]  The mhelp string for the event.
\end{description}

Remember that an event is ignored, until {\tt (INIT-EVENTS)} or {\tt (INIT-EVENT
{\it event})} has been called.

\subsection{Signalling Events}

\TPS provides a function {\tt SIGNAL-EVENT}, which takes a variable number
of arguments.  The first argument is the kind of event to be signalled,
the rest of the arguments are the event-args for this particular event.
{\tt SIGNAL-EVENT} will return {\tt T} or {\tt NIL}, depending on whether the action
to be taken in case of the event was successful or not.  Note that when
an event is disabled (see below), signalling the event will always be
successful.  There are basically three cases in which an event will be
considered unsuccessful: if the {\tt SIGNAL-HOOK} is specified and does a
{\tt THROWFAIL}, if {\tt WRITE-WHEN} is {\tt IMMEDIATE} and either the {\tt WRITE-HOOK}
(if specify) does a {\tt THROWFAIL}, or if for some reason the writing to
the file fails (if the file does not exists, or is not accessible
because it has the wrong protection, for example).

It is the caller's responsibility to make use of the returned value of
{\tt SIGNAL-EVENT}.  For example, the signalling of {\tt DONE-EXERCISE} below.

If {\tt WRITE-WHEN} is a number, the evaluated templates will be collected
into a list {\it event{\tt -LIST}}.  This list is periodically written out and
cleared.  The interval is determined by {\tt EVENT-CYCLE}, a global flag
(see description of {\tt WRITE-WHEN} above).  The list is also written out
when the function {\tt EXIT} is called, but not if the user exits \TPS with
{\tt $\hat{}$C}.  Note that if events have been signalled, the writing is done
without considering whether the event is disabled or not.  This ensures
that events signalled are always recorded, except for the {\tt $\hat{}$C} safety valve.

Events may be disabled, which means that signalling them will always
be successful, but will not lead to a recordable entry.  This is done
by setting or binding the flag {\it event{\tt -ENABLED}} to {\tt NIL} (initially
set to {\tt T}).  For example, the line {\tt (setq error-enabled nil)} 
in your {\tt .INI} file will make sure that no MacLisp error will be recorded.
For a maintainer using expert mode, this is probably a good idea.

\subsection{Examples}

Here are some examples take from the file {\tt ETPS-EVENTS}.  Interspersed
is also the code from the places where the events are signalled.

%\begin{tpsexample}
\begin{verbatim}

(defflag error-file
  (flagtype filespec)
  (default "etps3.error")
  (subjects events)
  (mhelp "The file recording the events of errors."))

(defevent error
  (event-args error-args)
  (template ((status-userid) error-args))
  (template-names (userid error-args))
  (write-when immediate)
  (write-file error-file)    ; a global variable, eg
			     ; `((tpsrec: *) etps error)
  (signal-hook count-errors) ; count errors to avoid infinite loops
  (mhelp "The event of a Lisp Error."))
\end{verbatim}
{\tt DT} is used to freeze the daytime upon invocation of {\tt DONE-EXC} so that
the code is computed correctly.  The code is computed by {\tt CODE-LIST},
implementing some ``trap-door function''.

\begin{verbatim}
(defvar computed-code 0)

(defvar dt '(0 0 0)) 

(defvar score-file)
(defflag score-file
  (flagtype filespec)
  (default "etps3.scores")
  (subjects events)
  (mhelp "The file recording completed exercises."))

(defevent done-exc
  (event-args numberoflines)
  (template ((status-userid) dproof numberoflines computed-code
			     (status-date) dt))
  (template-names (userid dproof numberoflines computed-code date daytime))
  (signal-hook done-exc-hook)
  (write-when immediate)
  (write-file score-file)
  (mhelp "The event of completing an exercise."))

(defun done-exc-hook (numberoflines)
  ;; The done-exc-hook will compute the code written to the file.
  ;; Freeze the time of day right now.
  (declare (special numberoflines))
  ;; because of the (eval `(list ..)) below.
  (setq dt (status-daytime))
  (setq computed-code 0)
  (setq computed-code (code-list (eval `(list ,@(get 'done-exc 'template))))))

(defflag proof-file
  (flagtype filespec)
  (default "etps3.proof")
  (subjects events)
  (mhelp "The file recording started and completed proofs."))

(defevent proof-action
  (event-args kind)
  (template ((status-userid) kind dproof (status-date) (status-daytime)))
  (template-names (userid kind dproof date daytime))
  (write-when immediate)
  (write-file proof-file)
  (mhelp "The event of completing any proof."))

(defflag advice-file
  (flagtype filespec)
  (default "etps3.advice")
  (subjects events)
  (mhelp "The file recording advice."))

(defevent advice-asked
  (event-args hint-p)
  (template ((status-userid) dproof hint-p))
  (template-names (userid dproof hint-p))
  (write-when 1)
  (write-file advice-file)
  (mhelp "Event of user asking for advice."))

\end{verbatim}
%\end{tpsexample}

Here is how the {\tt DONE-EXC} and {\tt PROOF-ACTION} are used in the code of
the {\tt DONE} command.  We don't care if the {\tt PROOF-ACTION} was successful
(it will usually be), but it's very important that the user knows
when a {\tt DONE-EXC} was unsuccessful, since it is used for automatic
grading.

%\begin{tpsexample}
\begin{verbatim}
(defun done ()
  ...
  (if (funcall (get 'exercise 'testfn) dproof)
      (do ()
	  ((signal-event 'done-exc (length (get dproof 'lines)))
	   (msgf "Score file updated."))
	(msgf "Could not write score file.  Trying again ... (abort with ^G)")
	(sleep 1/2))
      (msgf "You have completed the proof.  Since this is not an assigned exercise,"
	    t "the score file will not be updated."))
  (signal-event 'proof-action 'done))
\end{verbatim}
%\end{tpsexample}

\section{The Report Package}

The \indexother{REPORT} package in \TPS allows the processing of data
from EVENTS. Each report draws on a single event, reading
its data from the record-file of that event. The execution
of a report begins with its BEGIN-FN being run. Then 
the DO-FN is called repetitively on the value of the EVENTARGS
in each record from the record-file of the event, until that
file is exhausted or the special variable DO-STOP is given a non-NIL
value. Finally, the END-FN is called. The arguments
for the report command are given to the BEGIN-FN and END-FN.
The DO-FN can only access these values if they are assigned to
certain PASSED-ARGS, in the BEGIN-FN. Also, all updated values
which need to be used by later iterations of the DO-FN or by
the END-FN should be PASSED-ARGS initialized (if the default NIL
is not acceptable in the BEGIN-FN.

NOTE: The names of PASSED-ARGS should be different from
other arguments (ARGNAMES and EVENTARGS). Also, they should
be different from other variables in those functions where
you use them and from the variables which DEFREPORT always 
introduces into the function for the report: FILE, INP and DO-STOP.

The definition of the category of REPORTCMD, follows:

%\begin{LispCode}
\begin{verbatim}
(defcategory reportcmd
  (define defreport1)
  (properties 
   (source-event single)
   (eventargs multiple)   ;; selected variables in the var-template of event
   (argnames multiple)
   (argtypes multiple)
   (arghelp multiple)
   (passed-args multiple) ;; values needed by DO-FN (init in BEGIN-FN)
   (defaultfns multiplefns)
   (begin-fn singlefn)    ;; args = argnames    
   (do-fn singlefn)       ;; args = eventargs ;; special = passed-args
   (end-fn singlefn)      ;; args = argnames
   (mhelp single))
  (global-list global-reportlist)
  (mhelp-line "report")
  (mhelp-fn princ-mhelp)
  (cat-help "A task to be done by REPORT."))

\end{verbatim}
%\end{LispCode}

	The creation of a new report consists of a DEFREPORT statement
(\indexother{DEFREPORT} is a macro that invokes \indexother{DEFREPORT1})
and the definition of the BEGIN-FN, DO-FN and END-FN. Any PASSED-ARGS
used in these functions should be declared special. It is suggested
that most of the computation be done by general functions which are more
readily usable by other reports. In keeping with this philosophy,
the report EXER-TABLE uses the general function MAKE-TABLE. The latter
takes three arguments as input:  a list of column-indices, a list of
indexed entries (row-index, column-index, entry) and the maximum printing size
of row-indices. With these, it produces a table of the entries.
EXER-TABLE merely calls this on data it extracts from the record file
for the DONE-EXC event. The definition for EXER-TABLE follows:

%\begin{LispCode}
\begin{verbatim}
(defreport exer-table
  (source-event done-exc)
  (eventargs userid dproof numberoflines date)
  (argtypes date)
  (argnames since)
  (defaultfns (lambda (since)
		(cond ((eq since '$) (setq since since-default)))
		(list since-default)))
  (passed-args since1 bin exerlis maxnam)
  (begin-fn exertable-beg)
  (do-fn exertable-do)
  (end-fn exertable-end)
  (mhelp "Constructs table of student performance."))

(defun exertable-beg (since)
  (declare (special since1 maxnam))	;the only non-Nil passed-args
  (setq since1 since)
  (setq maxnam 1))

(defun exertable-do (userid dproof numberoflines date)
  (declare (special since1 bin exerlis maxnam))
  (if (greatdate date since1)
      (progn
       (setq bin (cons (list userid dproof numberoflines) bin))
       (setq exerlis 
	     (if (member dproof exerlis) exerlis (cons dproof exerlis)))    
       (setq maxnam (max (flatc userid) maxnam)))))

(defun exertable-end (since)
  (declare (special bin exerlis maxnam))
  (if bin 
      (progn
       (make-table exerlis bin maxnam)
       (msg t "On exercises completed since ")
       (write-date since)
       (msg "." t))
      (progn
       (msg t "No exercises completed since ")
       (write-date since)
       (msg "." t))))

\end{verbatim}
%\end{LispCode}

::::::::::::::
top-levels.tex
::::::::::::::
\chapter{Top-Levels}\label{toplev}

\section{Defining a Top Level}

Top levels are a \TPS category, whose definition is given in section ~\ref{categories}.
For an example, let's look at the editor top level:

%\begin{lispcode}
\begin{verbatim}
(deftoplevel ed-top
  (top-prompt-fn ed-top-prompt)
  (command-interpreter ed-command-interpreter)
  (print-* ed-print-*)
  (top-level-category edop)
  (top-level-ctree ed-command-ctree)
  (top-cmd-decode opdecode)
  (mhelp "The top level of the formula editor."))
\end{verbatim}
%\end{lispcode}

This says that the top level {\tt ed-top} identifies itself by the function {\tt ed-top-prompt},
which is one of the more complicated prompt functions in \TPS; its only purpose is to 
print the {\tt <ed34>} messages at the start of each line in the editor, but the complications
are necessary because the editor can be entered recursively.

The next line of the toplevel definition gives the name of the command interpreter function.
The {\tt print-*} function is a function that gets called after every line; in this case, it's the
{\tt ed-print-*} function, which prints out the current wff if it has changed due to the last command.
The top level category is {\tt edop}, which is defined as follows:

%\begin{lispcode}
\begin{verbatim}
(defcategory edop
  (define defedop)
  (properties
   (alias single)
   (result-> singlefn)
   (edwff-argname single)
   (defaultfns multiplefns)
   (move-fn singlefn)
   (mhelp single))
  (global-list global-edoplist)
  (shadow t)
  (mhelp-line "editor command")
  (scribe-one-fn
    (lambda (item)
      (maint::scribe-doc-command 
       (format nil "@IndexEdop(~A)" (symbol-name item))
       (remove (get item 'edwff-argname) 
	       (get (get item 'alias) 'argnames))
       (or (cdr (assoc 'edop (get item 'mhelp)))
	   (cdr (assoc 'wffop (get (get item 'alias) 'mhelp)))))))
  (mhelp-fn edop-mhelp)))
\end{verbatim}
%\end{lispcode}

This category defines the sort of command found in the editor top level (compare 
the above definition with that of {\tt mexpr}, for example). So all the commands 
that can only be seen from the editor top level are defined with the {\tt defedop} 
command, as follows:

%\begin{lispcode}
\begin{verbatim}
(defedop o
  (alias invert-printedtflag)
  (mhelp "Invert PRINTEDTFLAG, that is switch automatic recording of wffs
in a file either on or off.  When switching on, the current wff will be
written to the PRINTEDTFILE. Notice that the resulting file will be in 
Scribe format; if you want something you can reload into TPS, then use
the SAVE command."))
\end{verbatim}
%\end{lispcode}

The {\tt top-command-ctree} is used for command completion, and the {\tt mhelp} 
property is obvious. This leaves {\tt top-cmd-decode}, which is the name of the
function that is called by the command interpreter to, for example, fill in the 
default arguments for an edop.

\section{Command Interpreters}

Each top level has its own command interpreter. The actual command interpreters
in much of the code are older versions; the code has since been simplified
considerably. New command interpreters, which may in time replace the older versions,
and which should certainly be used as the models for the command interpreters
of any new top levels, are in the two files \indexfile{command-interpreters-core.lisp} 
and \indexfile{command-interpreters-auto.lisp}.
::::::::::::::
tps-struct.tex
::::::::::::::
\chapter{TPS Structures}

Notice that \TPS has a command \indexmexpr{TLIST} which outputs the
same information as the Lisp command {\it plist}, but formatted more
readably. So, for example, {\tt TLIST X2108} will show all of the slots 
in the structure X2108 (which is a proof).
 
\section{TPS Modules}

See the introductory chapter for a discussion of what \TPS modules are.

\subsection{The Tps3 Module Structure}

All modules are defined in one central file, called {\wt DEFPCK}.
You may want to look at this file to see examples of module definitions
and also a current list of all module known to \tps.

There is a partial order of
modules in \tps.  One whole set of modules called {\tt BARE} is
distinguished from the others.  All files in the module {\tt BARE} and all
of its submodules must always be present in a {\tt TPS3} core image.

When \TPS is built from {\bf Lisp}, some of the files in the {\tt BARE}
module can not be loaded with a module-loading command, since
it has not been defined.  Thus, even though every file for \TPS belongs
to a proper module, not all modules are loaded the same way because of
the ``bootstrapping'' problem.

Another quirk should be mentioned here.  A module called {\tt WFFS}
defines the basic operations of wffs.  The modules {\tt WFF-PRINT}
and {\tt WFF-PARSE} depend on {\tt WFFS.}  The module {\tt WFFS,} however,
cannot exist alone: the modules {\tt WFF-PRINT} and {\tt WFF-PARSE} must
be present also, even though this fact can not be deduced from the
module structure.


\subsection{Defining a New Module}

To define a new module for \tps, use the {\tt DEFMODULE} macro.
Its format is

\begin{verbatim}
(defmodule {\it name}
  (needed-modules {\it module} {\it module} ...)
  (macro-files {\it mfile} {\it mfile} ...)
  (files {\it file} {\it file} ...)
  (mhelp "{\it help-string}"))
\end{verbatim}

\begin{description}
\item [{\tt needed-modules}] These are all modules that must be loaded for the
module {\it name} to work.  Because of the transitive structure of modules
only the direct predecessors of the new module need to be listed.

\item [{\tt macro-files}] These are the files the compiler needs, before it can
compile any of the files in the module.  It is generally a good idea
to make a file with all the macro definitions (e.g. argument types,
flavors of labels, etc.) and separate it from the functions, commands,
etc. in the module.  This means clearer program structure, but also
minimal overhead for the compiler.

\item [{\tt files}] These are the rest of the files in the module.  When the module
is loaded, first the {\tt macro-files} are loaded, then the {\tt files.}
\end{description}

The new module should also be added into {\it defpck.lisp} at an appropriate point, and 
should be added into whichever of {\it tps-build.lisp}, {\it tps-compile.lisp},
{\it etps-build.lisp} and {\it etps-compile.lisp} are appropriate (these files are in the
same directory as the {\it Makefile}, not the main TPS directory).

\section{Categories}
\label{categories}

\TPS categories are in a sense data types. A category is a way to characterize a set of similar
objects which have properties of the same types, use the same auxiliary functions, are acted on 
by the same functions, etc.

Categories are orthogonal to the package/module structure, i.e. a category may have members which are 
defined in many different packages and modules. Categories group objects by functionality 
(how they behave) whereas packages and modules group objects by purpose (why they exist).

Categories are defined using the \indexother{defcategory} macro. For example, the 
definition of the category of \TPS top levels is:
%\begin{lispcode}
\begin{verbatim}
(defcategory toplevel
  (define deftoplevel)
  (properties
   (top-prompt-fn singlefn)
   (command-interpreter singlefn)
   (print-* singlefn)
   (top-level-category singlefn)
   (top-level-ctree singlefn)
   (top-cmd-interpret multiplefns)
   (top-cmd-decode singlefn)
   (mhelp single))
  (global-list global-toplevellist)
  (mhelp-line "top level")
  (mhelp-fn princ-mhelp))
\end{verbatim}
%\end{lispcode}

This shows a category whose individual members are defined with the {\tt deftoplevel} command, and
whose properties include the prompting function, a command interpreter, and so on. There is a 
global list called {\tt global-toplevellist} which will contain a list of all of the top levels 
defined, and an mhelp line "top level" (so that when you type {\tt HELP MATE}, \TPS knows to respond
"MATE is a top level".) The mhelp-fn is the function that will be used to print the help messages
for all the objects in this category. (See chapter ~\ref{help} for more information.)

The chapters of the facilities guide correspond to categories.
Within each chapter, the sections correspond to contexts.
In \tps, \indexother{global-categorylist} contains a list of all the
currently defined categories.

\section{Contexts}

Contexts are used to provide better help messages for the user. Each context is used to partition
the objects in a category into groups with similar tasks. For example, the objects in the 
category {\tt MEXPR} are grouped into contexts such as {\tt PRINTING} and {\tt EQUALITY RULES}.
(Contexts are themselves a category, of course: the definition is in {\it boot0.lisp}.)

New contexts are defined with the \indexother{defcontext} command, and are invoked with the 
single line {\tt (context {\it whatever}}) in the code (all this does is to set a variable 
{\tt current-context} to {\it whatever}). 

Here is a sample use of {\tt defcontext}:
%\begin{lispcode}
\begin{verbatim}
(defcontext tactics
  (short-id "Tactics")
  (order 61.92)
  (mhelp "Tactics and related functions."))
\end{verbatim}
%\end{lispcode}

The only property which is not immediately self-explanatory is {\tt order}; this is used to sort
the contexts into order before displaying them on the screen (or in manuals).

Contexts are used in the facilities guide (for example) to divide
chapters into sections. For example, the line
{\tt (context unification)}
occurs prior to the definition 
{\tt (defflag max-utree-depth ...)}
of the flag MAX-UTREE-DEPTH in the file {\it node.lisp},
and so this flag occurs in the section on unification in the
chapter on flags in the facilities guide.

To see the contexts into which the commands for a given top-level
are divided, just use the ? command at that top-level.
Look at \indexother{global-contextlist} in \TPS to see all the contexts.

\section{Flavors}

Some TPS structures (in particular, all expansion tree nodes, expansion variables, skolem terms and jforms) 
are defined as \indexother{flavors}; see the file {\it flavoring.lisp} for 
the details. These structures have many attached properties which allow 
wffops to be used on them as though they were gwffs; for example, the flavor
{\tt exp-var} in {\it etrees-exp-vars.lisp} has the properties
%\begin{tpsexample}
\begin{verbatim}
  (type (lambda (gwff) (type (exp-var-var gwff))))
  (gwff-p (lambda (gwff) (declare (ignore gwff)) T))
\end{verbatim}
%\end{tpsexample}
which state that the type of an {\tt exp-var} structure is the type of its variable, and all {\tt exp-var}s 
are gwffs. Errors of the form "Wff operation <wffop> cannot be applied to labels of flavor <label>" are almost always
caused by attempting to use a wffop on a flavor for which the corresponding property is undefined; for example,
if we deleted the lines above and recompiled TPS, any attempt to find the type of an expansion variable
would result in the error "Wff operation TYPE cannot be applied to labels of flavor EXP-VAR".

Flavors that are defined within TPS will also have the slot {\tt \indexother{bogus-slot}}; this slot is 
tested for by TPS to confirm that the flavor was defined by TPS, but the contents of this slot are never 
examined. This means that there is always one empty slot in each node of an expansion tree or jform which the
programmer can use to store information while a program is being tested (whereas if you define a new slot, you have to 
recompile all instances of a structure, which can be a nuisance). Obviously, once the new code is working,
you should define a new slot, change all references to bogus-slot and recompile TPS!

::::::::::::::
translation.tex
::::::::::::::
\chapter{Proof Translations}
\section{Data Structures}
\section{EProofs to Nproofs}
\input{epr-npr-translate}
\section{NProofs to Eproofs}
\input{nat-etr}

::::::::::::::
unification.tex
::::::::::::::
\chapter{Unification}

The relevant files are: {\it ms90-3-node.lisp}, {\it ms90-3-unif*.lisp}, 
{\it node.lisp}, {\it unif*.lisp}

\TPS has four unification algorithms, two for first-order logic and
two for the full type theory. Here we are mainly concerned with the two
type theory ones, which differ as follows:

\begin{itemize}
\item UN88 is called by those procedures which do not use
path-focused duplication, and by the \TPS UNIFY top level.
Each variable is a symbol. We use lazy reduction. Head normal form.
General implementation. Can use different strategies for searching the
unification tree. Default breadth-first. Requires storing almost the
entire tree.
When called from mating-search, we search for a success node or generate the
tree to a pre-determined maximum depth.

\item UN90 is called by those procedures which do use
path-focused duplication. No interactive interface exists now.
Each variable has the form (symbol . number)
Terms are reduced to $\lambda$-normal form as in Huet's paper.
Depth-first search. Stores only non-failure leaf node.
Does not store the entire unification tree.
When called from mating-search, we search for the first non-failure node
within the pre-determined maximum depth.  Search for a success node 
only when the mating is complete.
Major drawback: Needs modification to implement subsumption. 
\end{itemize}


\section{Data Structures}
\section{Computing Head Normal Form}
\section{Control Structure}
\section{First-Order Unification}
\section{Subsumption Checking}
There is a subsumption checker for UN88 which uses the slot {\tt subsumed} in 
each node of the unification tree; this is implemented in the file {\it unif-subs.lisp}.
The subsumption-checker is passed the new node and a list of other nodes which 
might subsume it. If \indexflag{SUBSUMPTION-CHECK} is NIL, it returns immediately.
Otherwise, it first checks the flags \indexflag{SUBSUMPTION-NODES} and \indexflag{SUBSUMPTION-DEPTH}
and eliminates all nodes from the list that do not fit the criteria established by these two flags 
(so it might, for example, pick out just those nodes at a depth of less than ten which lie 
either on the path to the new node or at the leaves of the current unification tree). Since
it is possible to add new disagreement pairs to the leaves of the tree under some conditions,
it also rejects any nodes that do not represent the same original set of disagreement pairs
as the new node.

Then it computes a hash function, somewhat similar to Goedel-numbering, by considering each wff
in the set of disagreement pairs at a node. The hash function has to ignore variables, because
we want to catch nodes that are the same up to a change in the h-variables that have been introduced.
These hash numbers are calculated once and then stored
in the {\tt subsumed} slot in the following format: for a dpairset 
%\begin{tpsexample}
\begin{verbatim}
((A1 . B1) (A2 . B2) ...)
\end{verbatim}
%\end{tpsexample}
we first calculate the hash numbers for each wff, and generate the following list:
%\begin{tpsexample}
\begin{verbatim}
(((#A1 . #B1) (A1 . B1)) ((#A2 . #B2) (A2 . B2)) ...)
\end{verbatim}
%\end{tpsexample}
Then, for each disagreement pair, if \#Bi < \#Ai we replace it with ((\#Bi . \#Ai) (Bi . Ai)).
Finally, we sort the list lexicographically by the pairs of hash numbers and store it in the
{\tt subsumed} slot. In future, if we return to this node, we can just read off the hash
function without recalculating it.

Now \TPS compares the dotted pairs of numbers from the hash functions of the new and old node.
If those for the new node are equal to, or a superset of, those for the old node, then we need
to do some more detailed checking. This is the point at which \TPS prints a "?", if \indexflag{UNIFY-VERBOSE}
is not SILENT. Otherwise we know there is no subsumption and proceed to the next node.

If there is still a possibility of subsumption, the next thing to do is to enumerate all the 
ways in which the old node might be considered a subset of the new one. If we are lucky, 
each dotted pair of numbers in a given node will be different from each other and from all other dotted pairs
at that node, and there will only be one way in which this could happen. If we aren't so lucky (if 
there are several disagreement pairs that get the same pair of hash numbers, or if there is a 
disagreement pair where the hash numbers for both wffs are the same), there may
be multiple ways to think about. For each possible way, we output two disagreement pair lists, which 
will be the entire old node and that subset of the new node to which it might correspond, ordered 
so that the nth element of one is supposed to compare to the nth element of the other, for all n.

Next, for each one of these possible ways, we take the two disagreement pair sets given, and begin
to rename the h-variables in them. We start at the left of both sets, and build up a substitution as we move rightwards,
comparing each term to the other symbol-by-symbol.
(Note that we are only replacing variables with other variables.) 
If we reach the end of the term without contradicting ourselves, we output a
"!" and the new node is subsumed. If we fail (because the substitution is inconsistent, or because we reach 
two different variables neither of which is an h-variable), we fail immediately
and go on to the next arrangement, if there is one.

Subsumption-checking can be very slow; set the flag \indexflag{SUBSUMPTION-DEPTH} with care.
Because of this, it was necessary to add time-checking to unification (it was previously only 
done between considering connections). The functions {\tt unify}, {\tt unify-ho-rec} and
{\tt subsumption-check} now check the time if they are called from within a procedure that 
uses time limits (and in order to implement this, many other unification functions have 
been given optional "start-time" and "time-limit" arguments that they do nothing 
with except passing them on to the next function).

\section{Notes}

The code that \TPS uses to handle double-negations is part of the unification 
code. See \indexfunction{imitation-eta} in the file \indexfile{unif-match.lisp}


::::::::::::::
vpforms.tex
::::::::::::::
\section{Simple MetaWffs in TPS3}

Even though in \TPS the principle metalanguage is of course Lisp, it is
often convenient to be able to use simple notations from the metalanguage
and include them directly in the input format for Wffs.  In \TPS this is
achieved by providing a notation for certain kinds of {\tt WFFOPS} inside
an external specification of a wff.  This method is not perfect, but has
other advantages as well, as we shall see.

\subsection{The Notation}

The motivation behind the notation is an analogy to Lisp: we use the
backquote to introduce some Lisp form which is to be evaluated and
inserted into the Wff.  One restriction is that the wffop must return
a {\it gwff} (or a subtype, like a {\it gvar}).  The other is that \TPS must
have certain pieces of knowledge about the {\it wffop} used, in order to be
able to determine the type of the result of applying the {\it wffop}.

Some examples of external format and what they are parsed to:
\begin{verbatim}
"forall x. `(lcontr [[lambda x. P x x x] [f x]])"
\end{verbatim}
to
$$\forall x.P\; [f\; x]\; [f\; x]\; [f\; x]$$

\begin{verbatim}
"forall x exists y.
  `(lexpd z [f x] `(lexpd z [f x] [Q [f x] [f x] y] `(1)) `t)"
\end{verbatim}
to
$$\forall x\exists y.[\lambda z\; [\lambda z^{1}\; Q\; z^{1}\; z\; y]\; z]\; [f\; x]$$

\begin{verbatim}
"`(substitute-types `((A . (O . I))) [P(OA) subset Q(OA)])"
\end{verbatim}
to
$$P_{\greeko(\greeko\greeki)} \subseteq Q_{\greeko(\greeko\greeki)}.$$
(The latter could have been more easily specified as
\begin{verbatim}
(substitute-types (("A" "OI")) "[P(OA) subset Q]")
\end{verbatim}
but that is no longer possible when the formula is to be embedded
in another.)


Here are the general rules:
\begin{itemize}
\item In an ordinary wff, a backquote may precede something of the form
{\wt ({\it wffop} {\it arg} ... {\it arg})}, where {\it wffop} has all the necessary
type information.  The typecase of {\it wffop} is irrelevant.

\item Among {\wt ({\it arg} ... {\it arg})}, each argument is either a gwff (and may
contain other backquoted expressions) or a Lisp expression, which
is considered a constant.  This is necessary to supply arguments which
are not gwffs to a {\it wffop}.  Notice, that it must be the internal
representation of the argument!\footnote{At some point one could work at
removing this restriction, if types are handled properly.}
\end{itemize}

\section{More about Jforms}

Much of the code for handling jforms is in {\it jforms-labels.lisp}; see
{\tt defflavor jform} in this file for the definition.

In the same file we see:

%\begin{tpsexample}
\begin{verbatim}
(eval-when (load compile eval)
  (defflavor disjunction
    (mhelp "A disjunction label stands for a disjunction of wffs.")
    (inherit-properties jform)
    (include jform (type 'disjunction))
\end{verbatim}
%\end{tpsexample}

This tell us that a jform can be a disjunction.




::::::::::::::
wffops.tex
::::::::::::::
\chapter{Well-formed formulae operators}

\section{Operations on Wffs}

By definition, operations on wffs differ from commands in that they
return a meaningful value, usually another wff or a truth value.  While
commands are usually given at the top-level, operations are usually
used inside the editor.  In other respects, operations on wffs are very
similar to commands in \tps.  The types of the arguments and the type of
the result must be specified in the declaration of a {\it wffop}.
Moreover, help for the arguments and help for the wffop itself is
available.  Arguments for wffops may be typed exactly the way arguments
for commands are:  one at a time after a short help message.

You may frequently have to refer to chapter ~\ref{toplev}, since it will
be assumed below that you have a general idea of how the \TPS top-level
interprets commands.

\subsection{Arguments to Wffops}
In principle, arguments to (or results of) wffops can have any type
defined inside \tps.  There are some argument types which are mainly
used for wffops and rarely or not at all for commands.  They are the following
\begin{description}
\item [\indexargtypes{GWFF}] A generalized wff.

\item [\indexargtypes{BOOLEAN}] {\tt NIL} for ``false'', anything else for ``true''.
Internally these are converted {\tt NIL} and {\tt T} first.  In particular,
if a wffop has been declared to return an object of type {\tt BOOLEAN},
this wffop may return anything, but {\tt NIL} is printed as {\tt NIL}, while
everything else is printed as {\tt T}.

\item [\indexargtypes{TYPESYM}] A type symbol (in string representation).  This is
extremely useful for error messages (inside \indexfunction{THROWFAIL}).
For example, the type inference program may contain a line
%\begin{tpsexample}
\begin{verbatim}
(throwfail "Type " (t1.typesym) " does not match " (t2.typesym))
\end{verbatim}
%\end{tpsexample}
For most settings of the \indexflag{STYLE} flag, this will print the types as true
greek subscripts.

\item [\indexargtypes{GVAR}] A general variable.  This is only one of
a whole class of possible subtypes of wffs ({\tt GWFF}).  The {\tt GETFN} for
these special kinds of wffs can easily be described using the function
\indexfunction{GETWFF-SUBTYPE}, which takes a predicate as the first argument,
an {\tt RWFF} as the second.
\end{description}

As an example for the definition of a subtype of {\tt GWFF} serves the definition
of {\tt GVAR}:
\begin{verbatim}
(deftype gvar
  (getfn (getwff-subtype 'gvar-p gvar))
  (testfn gvar-p)
  (printfn printwffhere)
  (side-effects t)
  (no-side-effects edwff)
  (mhelp "	A gwff which must be a logical variable"))
\end{verbatim}

\subsection{Defining Wffops}\label{defwffop}
The format for defining a wffop is very similar to that for defining a
MExpr.  The function that does the definition is called
\indexfunction{DEFWFFOP}.  The general format is ({\tt {}} enclose optional
arguments)
\begin{verbatim}
(DefWffop <name>
	{(ArgTypes <type1> <type2> ...)}
	(ResultType <type>)
	{(ArgNames <name1> <name2> ...)}
	{(ArgHelp <help1> <help2> ...)}
	{(Applicable-Q <fnspec>)}
	{(Applicable-P <fnspec>)}
        {(WffArgTypes <type> ... <type>)}
        {(Wffop-Type <type>)}
        {(Wffop-Typelist (<typesymbol> ... <typesymbol>))}
        {(DefaultFns <fnspec1> <fnspec2> ...)}
        {(MainFns <fnspec1> <fnspec2> ...)}
        {(Replaces <wffop>)}
        {(Print-Op <boolean>)}
        {(Multiple-Recursion <boolean>)}
	{(MHelp "<comment>")})
\end{verbatim}

The keywords {\tt ArgTypes}, {\tt ArgNames}, {\tt ArgHelp}, {\tt DefaultFns},
{\tt MainFns} and {\tt MHelp} have the
same meaning as for commands (MExprs).  See Section ~\ref{mexprargs}.
You have to mention {\tt ArgNames} before {\tt Applicable-P}, if you want to make
use of the argnames without explicitly using lambda.
The other keywords are as follows:

\begin{description}
\item [{\tt RESULTTYPE}] is the only non-optional part of the declaration and
is used for printing the result of the wffop.

\item [{\tt APPLICABLE-Q}] is a ``quick'' predicate (see Section ~\ref{quickslow})
to decide whether the wffop is applicable to a given set of arguments.
If omitted (or explicitly stated to be \indexfunction{TRUEFN}), it means that the
wffop can always be applied.

\item [{\tt APPLICABLE-P}] is a ``slow'' predicate
which is supposed to check thoroughly whether the wffop is applicable.
Again, if one wants to state explicitly that a wffop is always applicable,
use \indexfunction{TRUEFN}.

\item [{\tt WFFARGTYPES}] There must be exactly as many {\it type} entries, as there
are arguments to the {\it wffop}.  Each {\it type} entry may be either a
type (in string format) or {\tt NIL}, which is used for arguments which
are not {\tt gwffs}.

\item [{\tt WFFOP-TYPE}] specifies a {\it type} in string format, which is the
type of the result the {\it wffop}, or {\tt NIL}, if the result is not a
{\it gwff}.

{\tt WFFOP-TYPELIST} \\ A list of type symbols which are to be considered
type variables in the definition of the {\it wffop}.

\item [{\tt REPLACES}] The wffop being defined is to replace some previously defined
wffop. This is used extremely rarely.

\item [{\tt PRINT-OP}] This is set to {\tt T} for printing operations (which are 
usually defined using the macro \indexother{DEFPRTOP}, which sets this 
property automatically). By default, this property has value {\tt NIL}.

\item [{\tt MULTIPLE-RECURSION}]  seems to be set to {\tt T} for most tests of
equality and {\tt NIL} everywhere else. I'm not entirely sure what it's for.
\end{description}

Here are some example which may shed more light onto the subject.

%\begin{tpsexample}
\begin{verbatim}

(defwffop substitute-l-term-var
  (argtypes gwff gvar gwff)
  (resulttype gwff)
  (argnames term var inwff)
  (arghelp "term" "var" "inwff")
  (wffargtypes "A" "A" "B")		; TERM and VAR are of type A
  (wffop-type "B")			; INWFF and result of type B
  (wffop-typelist "A" "B")		; where A and B may be any types.
  (mhelp "..."))

(defwffop lexpd
  (argtypes gvar gwff gwff occ-list)
  (resulttype gwff)
  (argnames var term inwff occurs)
  (arghelp "lambda variable" "term to be extracted" "contracted form"
	   "occurrences to be extracted")
  (wffargtypes "A" "A" "B" NIL)		; TERM and VAR are of type A,
					; INWFF is of type B, OCCURS is not
  (wffop-type "B")			; a gwff, result is of type B,
  (wffop-typelist "A" "B")		; where A and B may be any types.
  (applicable-p (lambda (var term inwff occurs)
		  (declare (ignore inwff occurs))
		  (type-equal term var)))
  (mhelp "..."))

(defwffop substitute-types
  (argtypes typealist gwff)
  (resulttype gwff)
  (argnames alist gwff)
  (arghelp "alist of types" "gwff")
  (mhelp "Substitute for types from list ((old . new) ...) in gwff."))

\end{verbatim}
%\end{tpsexample}

\subsection{Defining Recursive Wffops}

The category \indexother{wffrec\%} is for recursive wff functions. 
Such operations are defined with the \indexfunction{defwffrec} function;
they have only three properties: {\tt ARGNAMES}, {\tt MHELP}
and {\tt MULTIPLE-RECURSION}.

The point of this is that we needed a way of saving the 
{\tt ARGNAME} information for functions which
use an \indexfunction{APPLY-LABEL}, but are not wffops themselves.
These are defined as wffrecs.

Some examples: 

%\begin{tpsexample}
\begin{verbatim}
(defwffrec gwff-q
  (argnames gwff))

(defun gwff-q (gwff)
  (cond ((label-p gwff) (apply-label gwff (gwff-q gwff)))
	((lsymbol-p gwff) t)
	((atom gwff) nil)
	((and (boundwff-p gwff) (gvar-p (caar gwff)) (gwff-q (cdr gwff))))
	((and (gwff-q (car gwff)) (gwff-q (cdr gwff))))))

(defwffrec wffeq-def1
  (argnames wff1 wff2 varstack switch)
  (multiple-recursion t))
 
; the function wffeq-def1 is pages long, so it's not quoted here. Look 
; in file wffequ2.lisp for details.

\end{verbatim}
%\end{tpsexample}

\subsection{Defining a Function Performing a Wffop}
There are some necessary restrictions on how to define proper wffops,
other conventions are simply a matter of style.
The following are general guidelines,
which do not address the definition of flavors (see Section ~\ref{defflavors}).

\begin{enumerate}
\item All arguments to a wffop may be assumed to be of the correct type, when
the function is invoked.  This does not mean, that the function never
should check for an error, but at least the function does not have to check
whether an argument is well-formed, or whether an argument is a logical
variable and not an application.

\item Most user-level wffops get by without using any ``slow'' predicates for
constituents of a gwff.  Use the ``quick'' predicate and assume that
the argument is a gwff.

\item Make the name of a wffop as descriptive as possible.  The user will rarely
have to type this long name, since he will normally invoke wffops in the
editor, where they can be given short aliases.  See section ~\ref{edops}.

\item When using auxiliary functions, make sure their name can be easily related
to the name of the main function.

\item Check the wff operations in the TPS3 Facilities Guide for Programmers and
Users before defining new functions. In particular, you should often use
\indexfunction{GAR} and \indexfunction{GDR} instead of car and cdr to 
manipulate wffs, since the wffs may have labels.

\item Always make sure you are invoking the ``quick'' test in the correct order,
since later tests rely on the fact that earlier tests failed.
\end{enumerate}

\subsection{Quick Test versus Slow Test}\label{quickslow}
Most predicates which test for certain kinds of subformulas come in two
incarnations: as a ``quick'' test and a ``slow'' test.  As a general
convention that should never be violated, both functions have the same
name except for the last character, which is {\tt -Q} for the quick
test and {\tt -P} for the slow test.

As a rule of thumb, quick predicates may assume a very restricted kind
of argument (e.g. a literal atom), but may not work recursively down
into the formula.  Slow predicates, however, may assume nothing about
the argument (they should always work), and often have to do a recursion
to see whether the predicate is true of the argument.

Quick predicates are most useful when in recursive functions that implement
a wffop.  Slow predicates are chiefly called inside the editor to test
that certain transformations or applications will be legal, {\it before they
are performed}.  Speed is usually not important when working in the editor,
but wffops in general should be optimized for speed, since time does make
a difference in automatic mode.

A list of the most useful quick predicates in the order in which they must
be called is supplied here.  See the comments attached to the predicates
in the source file if this list is unclear or ambiguous.

{\bf It is absolutely essential to understand the role of quick
predicates and the order of their invocation to write bug-free code!}

\begin{description}
\item [\indexfunction{LABEL-Q} {\it gwff}] tests for a label.  The standard action in this
case is {\tt (APPLY-LABEL GWFF ({\it wffop} {\it arg1} ... {\it argn}))} where
{\it wffop} is the wffop we are defining and {\it arg1} through {\it argn} are
its arguments. Always call this first, since any given argument may be a label.

\item [\indexfunction{LSYMBOL-Q} {\it gwff}] tests for a logical symbol.  This could either
be a variable, constant, or abbreviation. This must come after the test for {\it label}, 
but does not assume anything else.  There are several subtypes of {\it lsymbol} which
assume that their argument is a {\it lsymbol} and must be called in the
following order:
\begin{description}
\item [\indexfunction{LOGCONST-Q} {\it gwff}] a logical constant, which must have been declared
with {\tt DEF-LOGCONST.}

\item [\indexfunction{PROPSYM-Q} {\it gwff}] a proper symbol, that is something that has not
been declared a constant or abbreviation.

\item [\indexfunction{PMPROPSYM-Q} {\it gwff}] a polymorphic proper symbol (higher-order mode only).

\item [\indexfunction{PMABBREV-Q} {\it gwff}] a polymorphic abbreviation (higher-order mode only).

\item [\indexfunction{ABBREV-Q} {\it gwff} ] an abbreviation.
\end{description}

\item [\indexfunction{BOUNDWFF-Q}] 
Test whether the wff starts with a binder (of any type) and
assumes that we already know that it is neither {\it label} nor
a {\it lsymbol} (in Lisp terms: it must be a {\tt CONS} cell). Access the
bound variable with {\tt CAAR,} the binder with {\tt CDAR,} the scope of the binder
with {\tt CDR.}  Construct a new bound formula with {\tt (CONS (CONS {\it bdvar} {\it binder})
{\it scope})}.

\item [{\tt T}] This is the ``otherwise'' case, i.e. we have an application.
Access the ``function'' part with {\tt CAR,} the ``argument'' part with
{\tt CDR.}  Construct a new application with {\tt (CONS {\it function} {\it argument})}.
Remember also that all functions and predicates are curried.

\end{description}

\begin{center}
{\bf Examples of Wffops}
\end{center}

The following examples are taken from actual code\footnote{As of July 7th, 1994}.
%\begin{verbatim, LineWidth 80, LeftMargin -4}
%\begin{Text, Indent 1inch}
\begin{verbatim}

The following are two different substitution functions
SUBSTITUTE-TERM-VAR (currently in wffsub1.lisp) 
substitutes a term for a variable, but gives
and error if the term is not free for the variable in the wff.
SUBSTITUTE-L-TERM-VAR (currently in wffsub2.lisp) 
also substitutes a term for a variable,
but renames bound variables if a name conflict occurs.
There may be a global variable, say SUBST-FN, whose value is
the function used for substitution by default, or there may be a function
SUBSTITUTE, which checks certain flags to determine which function
to call.

(defwffop substitute-term-var
  (argtypes gwff gvar gwff)
  (wffargtypes "A" "A" "B")
  (resulttype gwff)
  (wffop-type "B")
  (wffop-typelist "A" "B")
  (argnames term var inwff)
  (arghelp "term" "var" "inwff")
  (applicable-p (lambda (term var inwff) (free-for term var inwff)))
  (mhelp
   "Substitute a term for the free occurrences of variable in a gwff."))

(defun substitute-term-var (term var inwff)
  "This function should be used with extreme caution. There's an underlying
  assumption that TERM is free for VAR in INWFF (which is true if TERM is
  a new variable)."
  (or (subst-term-var-rec (intern-subst term var) var inwff)
      inwff))

(defun subst-term-var-rec (term var inwff)
  (cond ((label-q inwff)
	 (apply-label inwff (subst-term-var-rec term var inwff)))
	((lsymbol-q inwff) (if (eq var inwff) term nil))
	((boundwff-q inwff)
	 (if (eq (caar inwff) var) nil
	     (let ((new-wff (subst-term-var-rec term var (cdr inwff))))
	       (if new-wff (cons (car inwff) new-wff) nil))))
	(t (let ((left (or (subst-term-var-rec term var (car inwff))
			   (car inwff)))
		 (right (or (subst-term-var-rec term var (cdr inwff))
			    (cdr inwff))))
	     (unless (and (eq left (car inwff)) (eq right (cdr inwff)))
		     (cons left right))))))

(defwffop substitute-l-term-var
  (argtypes gwff gvar gwff)
  (wffargtypes "A" "A" "B")
  (resulttype gwff)
  (wffop-type "B")
  (wffop-typelist "A" "B")
  (argnames term var inwff)
  (arghelp "term" "var" "inwff")
  (mhelp
   "Substitute a term for the free occurrences of variable in a gwff.
Bound variables may be renamed, using the function in the global
variable REN-VAR-FN."))

(defun substitute-l-term-var (term var inwff)
  (or (subst-l-term-rec (intern-subst term var) var inwff) inwff))

LCONTR (currently in wfflmbd2.lisp)
does a Lambda-contraction.  Notice the use of
THROWFAIL and the use of general predicates like LAMBDA-BD-P
rather than testing directly whether a given wff is bound by
Lambda.  This way, the function works, even if the CAR fo
the application is a label!

(defwffop lcontr
  (argtypes gwff)
  (wffargtypes "A")
  (resulttype gwff)
  (wffop-type "A")
  (wffop-typelist "A")
  (argnames reduct)
  (arghelp "gwff (reduct)")
  (applicable-p reduct-p)
  (mhelp "Lambda-contract a top-level reduct.
Bound variables may be renamed using REN-VAR-FN"))

(defun lcontr (reduct)
  (cond ((label-q reduct) (apply-label reduct (lcontr reduct)))
	((lsymbol-q reduct)
	 (throwfail "Cannot Lambda-contract " (reduct . gwff)
		    ", a logical symbol."))
	((boundwff-q reduct)
	 (throwfail "Cannot Lambda-contract " (reduct . gwff)
		    ", a bound wff."))
	(t (if (lambda-bd-p (car reduct))
	       (substitute-l-term-var (cdr reduct) (gar (car reduct))
				      (gdr (car reduct)))
	       (throwfail "Top-level application " (reduct . gwff)
			  " is not of the form [LAMBDA x A]t.")))))

FREE-FOR is a simple example of a predicate on wffs.
Here, the type of the result is declared to be BOOLEAN.

(defwffop free-for
  (argtypes gwff gvar gwff)
  (resulttype boolean)
  (argnames term var inwff)
  (arghelp "term" "var" "inwff")
  (applicable-q (lambda (term var inwff) (declare (ignore inwff))
			(type-equal term var)))
  (applicable-p (lambda (term var inwff) (declare (ignore inwff))
			(type-equal term var)))
  (mhelp "Tests whether a term is free for a variable in a wff."))

(defun free-for (term var inwff)
  (cond ((label-q inwff)
	 (apply-label inwff (free-for term var inwff)))
  	((lsymbol-q inwff) t)
	((boundwff-q inwff)
	 (cond ((eq (caar inwff) var) t)
	       ((free-in (caar inwff) term)
		(not (free-in var (cdr inwff))))
	       (t (free-for term var (cdr inwff)))))
	(t (and (free-for term var (car inwff))
		(free-for term var (cdr inwff))))))

TYPE (currently in wffprim.lisp) 
returns the type of the argument.  The name is a very
troublesome one and we may eventually need to change it globally so as not
to conflict with Common Lisp.

(defwffop type
	(argtypes gwff)
	(resulttype typesym)
	(argnames gwff)
	(arghelp "gwff")
	(mhelp "Return the type of a gwff."))

(defun type (gwff)
  (cond ((label-q gwff) (apply-label gwff (type gwff)))
	((lsymbol-q gwff) (get gwff 'type))
	((boundwff-q gwff) (boundwfftype gwff))
	(t (type-car (type (car gwff))))))

The following are a sequence of functions which instantiate abbreviations.
One can either instantiate a certain abbreviation everywhere
(INSTANTIATE-DEFN), instantiate all abbreviations (not recursively)
(INSTANTIATE-ALL), or instantiate the first abbreviates, counting
from left to right (INSTANTIATE-1).
The functions are implemented by one master function, one of whose
arguments is a predicate to be applied to an abbreviation.  This
predicate should return something non-NIL, if this occurrence is to be
instantiated, NIL otherwise.
Notice the subcases inside LSYMBOL-Q and the order of the quick
predicates in the OR clause.


(defwffop instantiate-defn
  (argtypes symbol gwff)
  (resulttype gwff)
  (argnames gabbr inwff)
  (arghelp "abbrev" "inwff")
  (applicable-p (lambda (gabbr inwff) (declare (ignore inwff))
			(or (abbrev-p gabbr) (pmabbsym-p gabbr))))
  (mhelp "Instantiate all occurrences of an abbreviation.
The occurrences will be lambda-contracted, but not lambda-normalized."))

(defun instantiate-defn (gabbr inwff)
  (instantiate-definitions 
   inwff #'(lambda (abbsym chkarg) (eq abbsym chkarg)) gabbr))


(defwffop instantiate-all
  (argtypes gwff symbollist)
  (resulttype gwff)
  (argnames inwff exceptions)
  (arghelp "inwff" "exceptions")
  (defaultfns (lambda (&rest rest)
		(mapcar #'(lambda (argdefault arg) 
			    (if (eq arg '$) argdefault arg))
			'($ NIL) rest)))
  (mhelp "Instantiate all definitions, except the ones specified
in the second argument."))

(defun instantiate-all (inwff exceptions)
  (instantiate-definitions
   inwff #'(lambda (abbsym chkarg) (not (memq abbsym chkarg))) exceptions))

(defwffop instantiate-1
  (argtypes gwff)
  (resulttype gwff)
  (argnames inwff)
  (arghelp "inwff")
  (mhelp "Instantiate the first abbreviation, left-to-right."))

(defun instantiate-1 (inwff)
  (let ((oneflag nil))
    (declare (special oneflag))
    (instantiate-definitions
     inwff #'(lambda (abbsym chkarg)
	       (declare (ignore abbsym chkarg) (special oneflag))
	       (prog1 (not oneflag) (setq oneflag t)))
     nil)))

(defwffrec instantiate-definitions
  (argnames inwff chkfn chkarg))

(defun instantiate-definitions (inwff chkfn chkarg)
  (cond ((label-q inwff)
	 (apply-label inwff (instantiate-definitions inwff chkfn chkarg)))
	((lsymbol-q inwff)
	 (cond ((or (logconst-q inwff) (propsym-q inwff) (pmpropsym-q inwff))
		inwff)
	       ((pmabbrev-q inwff)
		(if (funcall chkfn (get inwff 'stands-for) chkarg)
		    (get-pmdefn inwff) inwff))
	       ((abbrev-q inwff)
		(if (funcall chkfn inwff chkarg) (get-defn inwff) inwff))))
	((boundwff-q inwff)
	 (if (and (anyabbrev-q (binding inwff))
		  (funcall chkfn (binding inwff) chkarg))
	     (get-def-binder (binding inwff) (bindvar inwff) (gdr inwff))
	     (cons (car inwff)
		   (instantiate-definitions (gdr inwff) chkfn chkarg))))
	(t (let ((newcar (instantiate-definitions (car inwff) chkfn chkarg)))
	     (if (and (lambda-bd-p newcar) (not (lambda-bd-p (car inwff))))
		 (lcontr (cons newcar
			       (instantiate-definitions (cdr inwff)
							chkfn chkarg)))
		 (cons newcar
		       (instantiate-definitions (cdr inwff) chkfn chkarg)))))))

\end{verbatim}

\section{The formula editor}\label{EDOPS}

The formula editor is in many ways very similar to the top-level of
\tps.  The main difference is that we have an entity called 
``current wff'' or \indexData{edwff}, which can be operated on.
All the regular top-level commands can still be executed, but we
can now also call any {\it wffop} directly.  If we want the {\it wffop} to
act on the {\it edwff}, we can specify {\tt EDWFF} which is a legal
{\it gwff} inside the editor.  

This process is made even easier through the introduction of {\it edops}.
An \indexData{edop} is very similar to a {\it wffop}, but it ties into the
structure of the editor in two very important ways:  One argument can be
singled out, so that it will always be the {\it edwff}, and secondly the
{\it edop} will specify what happens to the result of the operations, which
is often the new {\it edwff}.  This is particularly useful for operations
which take one argument and return one wff as a value, like
lambda-normalization. It helps to give edops and wffops different names;
the name of a wffop should be longer and more descriptive than the name of
the edop for which it is an alias.

\section{Example of Playing with a Jform in the Editor}

%\begin{tpsexample}
\begin{verbatim}
<Ed9>sub x2115
<Ed10>neg
<Ed13>cjform
(AND ((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1))) 
 ((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4)))) 
 ((FORALL w<I>) (OR LIT5 LIT6)) ((EXISTS u<I>) ((FORALL v<I>) (OR LIT7 LIT8))))

<Ed14>edwff
(AND ((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1))) 
 ((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4))))
 ((FORALL w<I>) (OR LIT5 LIT6)) ((EXISTS u<I>) ((FORALL v<I>) (OR LIT7 LIT8))))

<Ed15>(setq aa edwff)
(AND ((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1))) 
 ((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4))))
 ((FORALL w<I>) (OR LIT5 LIT6)) ((EXISTS u<I>) ((FORALL v<I>) (OR LIT7 LIT8))))

<Ed16>aa
(AND ((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1)))
 ((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4))))
 ((FORALL w<I>) (OR LIT5 LIT6)) ((EXISTS u<I>) ((FORALL v<I>) (OR LIT7 LIT8))))

<Ed17>(auto::jform-parent aa)
NIL
vp
<Ed19>(setq bb (auto::conjunction-components aa))
(((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1)))
 ((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4))))
 ((FORALL w<I>) (OR LIT5 LIT6)) ((EXISTS u<I>) ((FORALL v<I>) (OR LIT7 LIT8))))

<Ed20>(length bb)
4
<Ed21>(car bb)
((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1)))

<Ed22>(cadr bb)
((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4))))

<Ed23>(auto::jform-parent (car bb))
(AND ((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1))) 
 ((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4)))) 
 ((FORALL w<I>) (OR LIT5 LIT6)) ((EXISTS u<I>) ((FORALL v<I>) (OR LIT7 LIT8))))

<Ed24>(setq cc '((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1))))
((FORALL X<I>) (OR ((FORALL Y<I>) LIT0) ((FORALL Z<I>) LIT1)))

<Ed26>bb
(((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1)))
 ((FORALL u<I>) ((EXISTS v<I>) (OR LIT2 (AND LIT3 LIT4))))
 ((FORALL w<I>) (OR LIT5 LIT6)) ((EXISTS u<I>) ((FORALL v<I>) (OR LIT7 LIT8))))

<Ed27>(car bb)
((FORALL x<I>) (OR ((FORALL y<I>) LIT0) ((FORALL z<I>) LIT1)))

<Ed28>cc
((FORALL X<I>) (OR ((FORALL Y<I>) LIT0) ((FORALL Z<I>) LIT1)))

(These look the same, but they are quite different.)
\end{verbatim}
%\end{tpsexample}

\section{Defining an EDOP}

An {\it edop} does not define an operation on wffs, it simply
{\bf refers} to one.  Thus typically we have a {\it wffop} associated
with every {\it edop}, and the {\it edop} inherits almost all of its
properties from the associated {\it wffop}, in particular the
help, the argument types, the {\it applicable} predicates etc.

A definition of an {\it edop} itself then looks as follows
({\tt {}} enclose optional arguments)
\begin{verbatim}
(DefEdop <name>
	{(Alias <wffop>)}
	(Result-> <destination>)
	{(Edwff-Argname <name>)}
        {(DefaultFns <fnspec1> <fnspec2> ...)}
        {(Move-Fn <fnspec>)}
	{(MHelp "<comment>")})
\end{verbatim}

In the above definition, the properties have the following meanings:

\begin{description}
\item [{\tt ALIAS}] This is the name of the {\it wffop} this {\it edop} refers to.  It must
be properly declared using the {\tt DEFWFFOP} declaration.

\item [{\tt RESULT->}] This provides part of the added power of {\it edops}.  {\it destination}
indicates what to do with the result of applying the {\it wffop} in {\tt ALIAS}
to the arguments.  {\it destination} can be any of the following:
\begin{description}
\item [{\it omitted}] If omitted, the appropriate print function for the type of
result returned by the {\tt ALIAS} {\it wffop} will be applied to the result.

\item [{\tt EDWFF}] This means that the result of the operation is made the new
current wff ({\it edwff}) in the editor.

\item [{\tt EXECUTE}] This means that the result of the operation is a list of
editor commands which are to be executed.  This may seem strange, but
is actually very useful for commands like {\tt FI} (find the first infix operator),
or {\tt ED?} (move to edit the first ill-formed subpart).  The argument
type \indexargtypes{ED-COMMAND} was introduced for this purpose only.

\item [{\it fnspec}] If the value is none of the above, but is specified, it is
assumed to be an arbitrary function of one argument, which is applied
to the result returned by the {\it edop}.
\end{description}

\item [{\tt EDWFF-ARGNAME}] This is the name of the argument that will be filled with the
{\it edwff}; see the {\tt ARGNAME} property of MExprs, in section ~\ref{mexprargs}, 
for more information.

\item [{\tt DEFAULTFNS}] See the arguments for MExprs, in section ~\ref{mexprargs}.

\item [{\tt MOVE-FN}] This means that the result of the operation will be the new current
wff and moreover that the operation qualifies as a ``move'', namely
that we should store what we currently have before executing the command,
and then use {\it replace-fn} on the value returned after then next {\tt 0} or {\tt $\hat{}$}. 
For example, the editor command {\tt A} moves to the ``function part'' of an 
application.  Moreover, when we return via {\tt 0} or {\tt $\hat{}$}, we need to replace 
this ``function part''.

\end{description}

\section{Useful functions}
A useful function in defining {\it edops} is \indexfunction{EDSEARCH}.
{\wt EDSEARCH {\it gwff} {\it predicate}} will go through {\it gwff} from left
to right and test at every subformula, whether {\it predicate} is
true of that subformula.  If such a subformula is found, {\it EDSEARCH}
will return a list of editor moving commands which will move down
to this subformula.  If the predicate is true of the {\it gwff} itself,
{\tt EDSEARCH} will return {\wt (P)}, the command to print the current wff.
If no subformula satisfying
{\it predicate} is found, {\tt EDSEARCH} will return {\tt NIL}.  For example

\begin{verbatim}

(defedop fb
  (alias find-binder)
  (result-> execute)
  (mhelp "Find the first binder (left to right)")
  (edwff-argname gwff))

(defwffop find-binder
  (argtypes gwff)
  (resulttype edcommand)
  (argnames gwff)
  (arghelp "gwff")
  (mhelp "Find the first binder (left to right)"))

(defun find-binder (gwff) (edsearch gwff (function boundwff-p)))

\end{verbatim}

\section{Examples}

Consider the following examples.\footnote{As taken from the code, 7th July 1994.}

\begin{verbatim}

(defedop ib
  (alias instantiate-binder)
  (result-> edwff)
  (edwff-argname bdwff))

(defwffop instantiate-binder
  (argtypes gwff gwff)
  (resulttype gwff)
  (argnames term bdwff)
  (arghelp "term" "bound wff")
  (applicable-p (lambda (term bdwff)
		  (and (ae-bd-wff-p bdwff) (type-equal (gar bdwff) term))))
  (mhelp
   "Instantiate a top-level universal or existential binder with a term."))

(defun instantiate-binder (term bdwff)
  (cond ((label-q bdwff)
	 (apply-label bdwff (instantiate-binder term bdwff)))
	((lsymbol-q bdwff)
	 (throwfail "Cannot instantiate " (bdwff . gwff)
		    ", a logical symbol."))
	((boundwff-q bdwff)
	 (cond ((ae-bd-wff-p bdwff)
		(substitute-l-term-var term (caar bdwff) (cdr bdwff)))
	       (t
		(throwfail "Instantiate only existential or universal quantifiers," t
			   "not " ((cdar bdwff) . fsym) "."))))
	(t (throwfail "Cannot instantiate an application."))))

(defedop subst
  (alias substitute-l-term-var)
  (result-> edwff)
  (edwff-argname inwff))

(defedop db
  (alias delete-leftmost-binder)
  (result-> execute)
  (edwff-argname gwff))

(defwffop delete-leftmost-binder
  (argtypes gwff)
  (resulttype ed-command)
  (argnames gwff)
  (arghelp "gwff")
  (mhelp "Delete the leftmost binder in a wff."))

(defun delete-leftmost-binder (gwff)
  (let ((bdwff-cmds (find-binder gwff)))
    (append (ldiff bdwff-cmds (member 'p bdwff-cmds))
	    `(sub (delete-binder edwff)))))


(defwffop delete-binder
  (argtypes gwff)
  (resulttype gwff)
  (argnames bdwff)
  (arghelp "bound wff")
  (applicable-q ae-bd-wff-p)
  (applicable-q ae-bd-wff-p)
  (mhelp "Delete a top-level universal or existential binder."))

(defun delete-binder (bdwff)
  (cond ((label-q bdwff)
	 (apply-label bdwff (delete-binder bdwff)))
	((lsymbol-q bdwff)
	 (throwfail "Cannot delete binder from " (bdwff . gwff)
		    ", a logical symbol."))
	((boundwff-q bdwff)
	 (cdr bdwff))
	(t (throwfail "Cannot delete binder from an application."))))

\end{verbatim}

\subsection{Global Parameters and Flags}
The following are the flags and parameters controlling the output 
of the editing session. Note that there are also editor windows,
which have separate flags; type {\tt SEARCH "EDWIN" T} to see a list of these.

\begin{description}
\item [\indexflag{PRINTEDTFILE}]  The name of the file in which wffs are recorded.

\item [\indexflag{PRINTEDTFLAG}] 
If {\tt T}, a copy of the current editing in {\tt ED} will be printed into
the file given by {\tt PRINTEDTFILE}. The prompt will also be changed to {\tt -ED}
or {\tt }ED+.

\item [\indexflag{PRINTEDTFLAG-SLIDES}]  As {\tt PRINTEDTFLAG}, but the output is
in Scribe 18-point style.

\item [\indexflag{PRINTEDTOPS}]  contains the name of a function which tests whether
or not to print a particular wff to the {\tt PRINTEDTFILE}.

\item [\indexflag{VPD-FILENAME}]  is the equivalent of {\tt PRINTEDTFILE} for vertical
path diagrams.

\item [\indexflag{PRINTVPDFLAG}]  is the equivalent of {\tt PRINTEDTFLAG} for
vertical path diagrams.
\end{description}
The flags and parameters listed below
are the counterparts of flags described in full detail on page
~\ref{PrintFlag}.  They have the identical meaning, except that they
are effective in the editor, while their counterparts are effective on
the top-level of \tps.
\begin{description}
\item [\indexflag{EDPPWFFLAG}] 
If {\tt T}, wffs in the editor will generally be pretty-printed.  Default is {\tt NIL}.

\item [\indexflag{EDPRINTDEPTH}] 
The value used as \indexflag{PRINTDEPTH} within the formula editor.  It is 
initialized to {\tt 0}.

\end{description}

\section{The formula parser}

\subsection{Data Structures}
\begin{description}

\item [\indexData{ByteStream}]
This list stores essentially the printing characters which are in its
input.  {\tt CR}, {\tt LF}, and {\tt TAB} characters are replaced with a space.  The
ending {\tt ESC} does not appear in this list.  All elements are {\tt INTERN}
identifiers. See the function \indexfunction{bytestream-tty} in {\it wffing.lisp}.

\item [\indexData{RdCList}]
This data structure appears to be all but obsolete; the last remnants 
of it are in the file {\it wffing.lisp}. {\tt RdC} refers to the Concept terminal.
This list contains either integers between 0 and 127, lists containing
precisely one of 0, 1, or 3, or the identifier \indexData{CRLF}.  The lists
represent character set switches, the integers represent characters,
and {\tt CRLF} represents a carriage return/line feed combination.

\item [\indexData{LexList}]
This is a list of lexical objects, i.e. it contains name for logical
objects which will appear in the fully parsed formula.  It also
contains the brackets "[","]", and ".".  It also contains the type
symbols from the initial input.  These are distinguishable from the
other items in the list since they are stored as lists.  Hence,
LexList is a "flat" list of these three things.

\item [\indexData{TypeAssoc}]
This is an association list which associates to those identifiers in
the LexList which got a type, that type.  This is necessary so that an
identifier which is typed explicitly at one place in the formula can
have that type attributed to it at other non-typed occurrences.

\item [\indexData{GroupList}]
This is essentially the same as {\tt LexList}, except that the bracket
identifiers are removed, and nested s-expressions are used to denote
groupings. Type symbols are also "attached" to the identifier
preceding it.  Hence a GroupList contains only logical identifiers -
some with types and some without - grouped in a hierarchical fashion.

\item [\indexData{PreWff}]
This data structure is like that of the wff structure, except that not
all items are correctly typed yet.  The full prefix organization is
present in this formula.  The types for polymorphic definitions,
however, are not yet computed.
\end{description}

\subsection{Processing}

Input is first processed into {\tt ByteStream}s and then into {\tt LexList}s
by the function \indexfunction{LexScan}.

\indexfunction{GroupScan} now operates on {\tt LexList} in order to construct the
{\tt GroupList}.  This function has no arguments and uses a special
variable, called \indexparameter{LexList}, to communicate with
recursive calls to itself.  {\tt GroupScan} is also responsible for building
the {\tt TypeAssoc} list.

\indexfunction{InfixScan} converts a {\tt GroupList} into a {\tt PreWff}.
This requires using
the standard infix parser.  \indexfunction{MakeTerm} is used to build the prefix
subformulas of the input. 

Now that all logical items appear in their final positions, the actual
types of polymorphic abbreviations can be determined.  This is the job
of \indexfunction{FinalScan}.  This function takes a {\tt PreWff} and 
returns with a {\tt WFF}.

This is not a very efficient algorithm. A few of the passes could be joined
together, and a few might be made more efficient by using destructive
changes.  The parser, however, is rather easy to upgrade.


::::::::::::::
wffprint.tex
::::::::::::::
\chapter{Printing and Reading Well-formed formulas}
\section{Parsing}

Frank has implemented a type inference mechanism based on an algorithm by
Milner as modified by Dan Leivant.
Type inference is very local: The same variable, say "x" will
get different type variables assigned, when used in different formulas.
Since multiple use of names with different types is rare, the default
could be changed, so that after the first occurrence of an "x" during
a session {core image}, the type inferred the first time is remembered.

There are only a total of 26 type variables, so you may run out during
a session.  The function INITTYPES reset the way type variables are
assigned and treats everything except O and I as type variables.
Normally, a type variable once mentioned or assigned automatically
becomes a type constant.

If \indexflag{TYPE-IOTA-MODE} is {\tt NIL}, then 
TPS will assign type variables starting with Z and going backwards, as 
more are needed. \indexflag{TYPE-IOTA-MODE} defaults to {\tt T}.

Polymorphic abbreviations like SUBSET now may be given a type, so as to fix
the type of other variables.  E.g. the following is legal:
   "FORALL x . P x IMPLIES [Q x] IMPLIES . P SUBSET(O(OC)(OC)) Q"
Note that "x" will be typed "C" (Gamma).  The same typing could have been
achieved by
   "FORALL x(C) . P x IMPLIES [Q x] IMPLIES . P SUBSET Q"
If all the types were omitted and \indexflag{TYPE-IOTA-MODE} were {\tt NIL}, 
"x" would have been typed with the next available typevariable.

Using the same name for two variables of distinct type is legal, but not
recommended.  Consider, for example,
   "FORALL x . P x(I) AND . Q . x(II) a"
Here the type of the very first occurrence of "x" will be assumed as "II".
Leaving out the type of the third occurrence of "x" would have led to an
error message:  Rather than assume that "x(II)" was really meant, TPS
assumes instead that the scoping must have been incorrect, which seems much 
more likely.

All remaining type variables (after a parse) are automatically assumed
to be of {\it base-type} unless the flag {\tt TYPE-IOTA-MODE} is set in which
case they are assumed to be of type $\greeki$. In first-order mode identifiers have
only single characters (Thus "not Pxy" is parsed as "NOT . P x y").

When a wff is read in and parsed, each input token (where the number
of characters in a token is dependent on whether you are reading in
first-order-mode or not) is made into a lisp symbol which incorporates
the token's printed representation and type.  For example, entering
"x(A)" will result in a symbol being created whose print-name is
"x<A>".  When you try to print a symbol like this, first the part
without the type information is printed, then the type (if necessary)
is printed.  E.g., first we print "x", then print "(A)".  But the
information necessary to print "x" is really on the property list of
the symbol whose print-name is "x".  So all wffs of the form "x<...>"
will be printed the same way (except for the type).

So, if you enter "x1(A)", you get the symbol "x1<A>", but no
information about a superscript is put on the symbol "x1".  Thus when
you print it, you get no superscript, just "x1".  Where do
superscripts come from, then?  Well, when TPS renames a variable in
order to get a new one (such as alpha-normalizing a wff), it puts the
superscript information on the new symbol's property list.  I.e., if
we rename "x1<A>", we may get the symbol "x2<A>", and on the property
list of "x2", we get the superscript information.  Thus, the next time
the user types in "x2(A)" or even "x2(I)", the symbols created will
have the superscript information.

This can be a little confusing, because the "x1(A)" that you
originally entered still isn't superscripted, but the renamed
variables "x2", "x3", etc., will be.

\section{Printing of formulas}

\subsection{The Basics}

In this section we will talk about how a formula in internal representation
is printed on different output devices.  There are two main points
to take into consideration: how will the parts of the formula appear,
and where will they appear.  For the latter refer to section 
~\ref{Pretty-Printing}, the former we will discuss now.

\subsection{Prefix and Infix}

Since we deal with formulas of type theory, we can regard every formula
as built by application and $\lambda$-abstraction from a few primitives.
In order to make formulas more legible and closer to the form usually
used to represent formulas from first order logic, we furthermore have
quantification and definitions
internally, and quantification, definitions, and infix operators
for the purpose of input and output.

The application of a function to an argument is printed by simply
juxtaposing the function and its argument.  As customary in type
theory, we do not have an explicit notation for functions of more
than one argument.  Predicates are represented as functions with truth
values as their codomain.

Infix operators have to be declared as such.
Only conjunction, disjunction, and implication are automatically
declared to be infix operators.  In general,
infix operators
will be associated to the left, if  explicit brackets are missing.
For example
\begin{Example}
$A \land B \land C$    will be  $[[A \land B] \land C]$
\end{Example}
Internally every infix operator has a property \indexProperty{Infix}
which is a number. This number is the relative binding strength of
this infix operator. You will have to specify it, if you define a
new connective to be infix. The higher the priority, the stronger
the binding. As usual, `$\land$' binds stronger than `$\lor$' which has precedence
over `$\limplies$' (implication).

(As an aside, if you don't want conjunctions bound more tightly than disjunctions,
but want brackets to appear, make the {\tt INFIX} property of {\tt OR} the same
as {\tt AND}. Thus, do: {\tt (GET 'AND 'INFIX)}, to find it is 5, and then 
{\tt (PUTPROP 'OR 5 'INFIX)})

Unfortunately prefix operators like negation, do not currently  have
a binding strength associated with them and will always be associated
to the left.
This has to be kept in mind, when formulas are typed in.

Definitions can be infix or prefix and the same rules hold for them.
There are flags which control whether a definition or its instantiation
will be printed. Similarly, logical atoms can appear as names or as
values (or both).  In general the appearance of a formula and in particular
of a definition very much depends on which output device is used.  See
section ~\ref{Styles and Fonts} for more detail, but remember that this
only affects the way the primitive or defined symbols appear, but not
how the formula is assembled from its parts.

\subsection{Parameters and Flags}\label{Printing Flags}
The flags listed below are global parameters which can be set by the user
to control the way formulas are printed. These settings can be overridden
if specific commands are given.

\begin{description}

\item [\indexflag{PrintTypes}] = {\tt T} causes all types to be printed. 
	If a typed symbol occurs more than once, only the first occurrence
	will have a type symbol, unless the same symbol name appears in the
	same formula with a different type.

 = {\tt NIL} suppresses type symbols.

\item [\indexparameter{PrintDepth}] This is a parameter which determines how deep the recursion
	which prints the formula will go.  Subformulas located at a lower
	level will simply be replaced by an \&.  A {\tt PrintDepth} of 0 means
	that everything will be printed, regardless of its depth.  {\tt PrintDepth}
	has to be an integer. It is initialized to 0. The most useful
	application of this parameter is in the formula-editor, where one
	usually does not like to see the whole formula.

\item [\indexflag{AtomValFlag}] This flag should usually not be touched by the user. If it
	is true, under each atom its value will appear.

\item [\indexflag{AllScopeFlag}] This flag should be {\tt NIL} most of the time. If it is
	{\tt T} brackets and dots will always be inserted, i.e. no
	convention of associativity to the left is followed. The
	precedence values of infix operators are also ignored.
	It can be forced to {\tt T} by calling the function
	{\w \indexmexpr{PWScope GWff}}.
\end{description}

\subsection{Functions available}\label{Printing Functions}

There are of course a variety of occasions to print wffs, For example in plans,
as lines, after the {\tt P} or {\tt PP} -command in the editor etc.
Associated with these are different printing commands given 
by the user. Some of these commands override globally set parameters or
flags. Internally, however, there is only one function which prints wffs.
This function \indexfunction{PrtWff} is called whenever formulas have to be 
printed.  The various flags controlling the way printed formulas will appear,
will either be defaulted to the global value, or be passed to this
function as arguments. The general form of a call of {\tt PrtWff} is as follows

{\tt 
(PrtWff Wff {(Parameter$_1$ Value$_1$)} ... {(Parameter$_n$ Value$_n$)} )
}

Before the actual printing is done Parameter$_1$... Parameter$_n$ will be
set to Value$_1$ ... Value$_n$, resp. If a parameter of the following list
is not included in the call of the function, its global value will be
assumed. Possible parameters with their range and the section they are
explained in are

% @Tabdivide(3)
\begin{tabular}{lll}
\indexflag{PrintTypes} & T,NIL & ~\ref{Printing Flags} \\
\indexparameter{PrintDepth} & 0,1, ... & ~\ref{Printing Flags} \\
\indexflag{AllScopeFlag} & T,NIL & ~\ref{Printing Flags} \\
\indexflag{AtomValFlag} & T,NIL & ~\ref{Printing Flags} \\
\indexflag{PPWfflag} & T,NIL & ~\ref{Pretty-Printing Flags} \\
\indexflag{LocalLeftFlag} & T,NIL & ~\ref{Pretty-Printing Flags} \\
\indexflag{FilLineFlag} & T,NIL & ~\ref{Pretty-Printing Flags} \\
\indexflag{FlushLeftFlag} & T,NIL & ~\ref{Pretty-Printing Flags} \\
\indexparameter{Leftmargin} & 1 ... Rightmargin & ~\ref{More Printing Functions} \\
\indexparameter{Rightmargin} & 1, 2 ... & ~\ref{More Printing Functions} \\
\indexparameter{Style} & XTERM, SCRIBE, CONCEPT, \\
 & GENERIC, SAIL, TEX ... & ~\ref{Styles and Fonts} \\
\end{tabular}

\subsection{Styles and Fonts}\label{Styles and Fonts}

\TPS can work with a variety of different output devices, producing special 
characters like $\forall$ or $\land$ where possible, and spelling them out 
(as {\tt FORALL} and {\tt AND}) where not. Details of how to produce output 
files for various purposes are in the \ETPS and User's Manuals.

At no point does the user actually make a commitment whether to work with special
characters or not, since she can easily switch back and forth. The 
internal representation is completely independent of these switches
in the external representation.

A few commands, such as \indexfunction{VPForm} and \indexfunction{VPDiag} 
have an argument \indexparameter{Style} which specifies
the style in which a file is produced. Furthermore there is a flag, \indexflag{STYLE},
which \TPS will use in the absence of any other indication as to the 
appropriate form of output.

Along with the style the user can usually specify an appropriate linelength
by using the \indexflag{LEFTMARGIN} and \indexflag{RIGHTMARGIN} flags.
Some commands (most notably \indexcommand{SETUP-SLIDE-STYLE}) will change 
both the style and the default line length.

\begin{description}
\item [\indexstyle{CONCEPT}, \indexstyle{CONCEPT-S} ] this is the style used for a Concept terminal,
which might also
	occasionally also be useful to produce a file which can be
	displayed on the Concept terminal with {\tt CAT} or {\tt MORE}.
The difference between {\tt CONCEPT} and {\tt CONCEPT-S} is that the latter assumes 
that your Concept is equipped with special characters and the former does not.
If special characters are available,
	you will then get types as greek subscripts,
	the universal quantifier as $\forall$, etc.  The default linelength
	is 80.

\item [\indexstyle{GENERIC} ] this style assumes no special features and defaults the 
	linelength to 80.  For example the existential quantifier shows up as EXISTS
	and types are enclosed in parentheses.

\item [\indexstyle{GENERIC-STRING} ]  is much like {\tt GENERIC}, but prints in a format that 
can be re-read by \tps.

\item [\indexstyle{SCRIBE} ] corresponds to the style used by the Scribe text processor.
	A file produced in this style has to be processed by {\tt SCRIBE}
	before it can be printed. All special characters,
	superscripts and subscripts, etc. are available. The main drawback
	of a {\tt SCRIBE}-file is that precise formatting as necessary
	for vertical path diagrams is impossible. The font used is 10-point, except 
        when doing \indexcommand{SLIDEPROOF}, when an 18-point font is used.

\item [\indexstyle{TEX} ] is the output style used by the \TeX text processor.
A file produced in this style has to be processed by \TeX before it can be printed.
All special characters, superscripts, etc. are available, and vertical path diagrams
are correctly formatted (although often too wide to print). 

\item [\indexstyle{XTERM} ] produces the special characters used by X-windows. You should
set the value of \indexflag{RIGHTMARGIN} to reflect the width of the window containing 
\tps.

\item [\indexstyle{SAIL} ] {\tt SAIL} is a style (now all but obsolete) used for printing
on a Dover printer. The font used is 10-point, with 120 characters 
per line in landscape format (used for vertical path diagrams), and 86 in portrait format
(used for all other applications).
When you dover the file , you have to remember size and orientation and
specify it in the switches of your call of {\tt DOVER}.
A {\tt SAIL} file does not have subscripts, but has as variety
of other special characters.

\end{description}

From the information about the style, the low-level printing functions
determine which sequence of characters, including control characters, to
send to the selected output device.  If a symbol expands to a list of known 
symbols with different names (e.g. \indexData{EQUIVS} expands to an {\tt EQUIV} 
symbol with a superscript {\tt S}), then it has a property \indexProperty{FACE}
which contains this information. Various other properties give the way that the character 
is to be printed in different styles.
The \indexProperty{CFONT} property
is a pair {\tt (KSet . AsciiValue) . \indexData{Kset}} can be 0,1,2, or 3,
although currently only the character sets 0, 1, and 3 are used; this gives the 
appropriate character for a Concept terminal.
Similarly, the \indexProperty{DFONT} property is a string {\tt "whatever"} which will be printed
into Scribe files as {\tt @whatever}. The \indexProperty{TEXNAME} property does the
same for the \TeX output style.
There are some special fonts that are declared in the file
\indexfile{tps.mss}.  A list of the available special characters for
the Concept and for the Dover (in a {\tt SCRIBE}-file) are explicitly stored
in the files \indexfile{cfont.lisp} and \indexfile{dfont.lisp} and loaded into
\TPS at the time the system is being built.

Consider the following example:
\begin{Example}
SIGMA1 is a binder.
It has a property FACE of value (CAPSIGMA SUP1).

CAPSIGMA is a tex special character, a scribe special character, 
and a concept special character.
It has a property CFONT of value (3 .  83).
It has a property DFONT of value "g{S}".
It has a property TEXNAME of value "Sigma".

SUP1 is a tex special character, a scribe special character, 
and a concept special character.
It has a property CFONT of value (1 .  49).
It has a property DFONT of value "+{1}".
It has a property TEXNAME of value "sup1".
\end{Example}

In a scribe or tex file, or on a Concept with special characters, {\tt SIGMA1} will
appear as $\Sigma^1$; elsewhere it will be written as {\tt SIGMA1}. The actual Scribe
output produced will be \begin{verbatim}@g{S}@\;@^{1}@\;\end{verbatim}; the actual \TeX
output will be \begin{verbatim} \Sigma^{1} \end{verbatim}.

\subsection{More about Functions}\label{More Printing Functions}

In this section some more details of the functions which are used to
do the printing are given. 

As mentioned earlier, the main connection with the rest of \TPS is
the MACRO \indexfunction{PrtWff}.  It expands into a {\tt PROG} in which
all the parameters given as arguments are {\tt PROG}-variables.  In the body
of the {\tt PROG}, all parameters are set to the value specified in the call,
then the function \indexfunction{PWff} is called, just with {\tt Wff} as its
argument.  All the other parameters and flags are now global, or, in {\tt LISP}
terminology, special variables.

The function {\tt PWff} performs two main tasks. First a few special variables
are set to the correct value.   After this is done, {\tt PWff} checks whether
pretty-printing is desired, i.e. whether {\tt PPWfflag} is {\tt T}.
For an explanation of what happens during pretty-printing see section 
~\ref{Pretty-Printing} and in particular ~\ref{Pretty-Printing Functions}.
Otherwise the recursive function \indexfunction{PrintWffPlain} is called
with the appropriate arguments.

At this point the current style is available to the functions in the
flag \indexflag{STYLE}. The calling function has to make sure
that \indexflag{LEFTMARGIN} and
\indexflag{RIGHTMARGIN} will be bound.  They are important
for the printing functions in order to determine where to break lines,
and where to start formulas on the line.  This holds, whether pretty-
printing is switched on or off.

Below {\tt PWff} two functions appear.
{\tt PrintWffPlain} prints a formula without any delimiting symbols
around it.  For example (with {\tt STYLE SCRIBE})  
%\begin{Example,Spacing=1.5}
\begin{Example}
((x<I> . FORALL) . ((OR . (P<OI> . x<I>)) . q<O>))  appears as \\
$\forall x_\greeki . [P_{\greeko\greeki} x] \lor q_\greeko$ if BRACKETS = T and as \\
$\forall x_\greeki [[P_{\greeko\greeki} x] \lor q_\greeko]$ if BRACKETS = NIL .
\end{Example}
\indexfunction{PrintWffScope} delimits a composite formula with a preceding
dot, if the argument \indexparameter{BRACKETS} is {\tt T} , and with brackets
around it , if {\tt BRACKETS} is {\tt NIL}.  Other than that the functions are
identical.  In the above example we would get
%\begin{Example,Spacing=1.5}
\begin{Example}
$\forall x_\greeki . [P_{\greeko\greeki} x] \lor q_\greeko$ if Brackets = T and \\
$[\forall x_\greeki . [P_{\greeko\greeki} x] \lor q_\greeko]$ if Brackets = NIL
\end{Example}
Both {\tt PrintWffPlain} and {\tt PrintWffScope} call \indexfunction{PrintWff},
where the real work of distinguishing the different kinds of formulas
and symbols is being done.  The distinction between {\tt PrintWffPlain} and 
{\tt PrintWff} is only made for the sake of pretty-printing (see 
~\ref{Pretty-Printing Functions}).

At an even lower level is the function (actually a macro) \indexfunction{PCALL},
which determines the appropriate way to print a particular symbol in the 
current style, and prints an error if the relevant function is undefined.
{\tt PCALL} actually applies to printing functions, rather than characters, so
each function will have a different definition for different styles. For example,
in style scribe the \indexfunction{print-symbol} function is called \indexfunction{PP-SYMBOL-SCRIBE}, 
whereas in style xterm it's called \indexfunction{PP-SYMBOL-XTERM}. (Examine
the plists of {\tt SCRIBE} and {\tt XTERM} to verify this, if you like.)

\section{Pretty-Printing of Formulas}\label{Pretty-Printing}

The most commonly used way of printing formulas, such as lines or plans,
is to pretty-print them. This is a feature quite similar to the way LISP
pretty-prints functions. Formulas which are too long to fit on one line
of the current output device, are broken at the main connective and printed
in several lines. The main difference to the LISP pretty-printing is that
we have to consider infix operators.

The general structure of the functions doing the pretty-printing allows 
future changes to the way printing in general is done without making changes
to the pretty-printer.  Whenever a formula is to be pretty-printed the
usual printing functions as described above are called, but instead of
printing the characters, they will be appended to a list.  Later this list
is used to actually output the characters after the decision where to break
the formula has been made.  From this structure it is clear that all the
parameters and flags controlling the appearance of a formula on the several
printing devices still work in the way described before.  There are however,
a few additional flags which determine how subformulas will be arranged
within a line.

\subsection{Parameters and Flags} \label{Pretty-Printing Flags}

As new flags particularly for pretty-printing we have
\begin{description}

\item [\indexflag{PPWfflag} ] = {\tt T}  means that formulas will usually be pretty printed. 
This is the default value.

 = {\tt NIL} 
means that formulas never will be pretty printed unless the
command is given explicitly.

\item [\indexflag{LocalLeftFlag} ] ={\tt T} 
will cause the left hand side of an infix expression
to be aligned with the operator and not with the right hand side.

 = {\tt NIL} 
is the default and prints left and right hand side of an
infix expression with the same indentation.

\item [\indexflag{FilLineFlag} ] ={\tt T} 
will try to fill a line as much as possible before
starting a new one. This only makes a difference for associative
infix operators.

 = {\tt NIL} starts a new line for each of the arguments of an infix operator
even if only one of several arguments would be too long to fit on the 
remainder of the line.

\item [\indexflag{FlushleftFlag} ] = {\tt T} switches off indentation completely, i. e. every line will
be aligned with the left margin.

 = {\tt NIL}  indents the arguments of infix operators.
\end{description}

\subsection{Creating the PPlist} \index{PPlist} \label{PPlist}

The pretty-printing is achieved in two steps. During the first phase
printing will be done without any formatting and the characters are
not actually printed, but appended to a list, called {\tt PPlist}. In the second
phase, this list will then be printed. The decisions, when to start
a new line, how to indent etc. are only made in this second stage.

The {\tt PPlist} is of the following syntactical structure.

\begin{description}

\item [\indexData{pplist} ::=]  ((aplicnlist . (pdepth . pgroup)) . plength) 
           \\ | ((gencharlist . (pdepth . pgroup)) . plength)

\item [\indexData{aplicnlist} ::=]  (aplicn . aplicnlist) | {\tt NIL} | (aplicn . {\tt MARKATOM})

\item [\indexData{aplicn} ::=]  (pplist . pplist)

\item [\indexData{plength} ::=]  {\tt 0} | {\tt 1} | {\tt 2} | ...

\item [\indexData{pgroup} ::=]  {\tt BRACKETS} | {\tt DOT} | {\tt NIL}

\item [\indexData{pdepth} ::=]  {\tt 0} | {\tt 1} | {\tt 2} | ...

\item [\indexData{gencharlist} ::=]  (genchar . gencharlist) | {\tt NIL}

\item [\indexData{genchar} ::=]  char | (ascnumber) | (gencharlist)

\item [\indexData{char} ::=]  <any non-control character>

\item [\indexData{ascnumber} ::=]  {\tt 0} | {\tt 1} | ... | {\tt 127}

\end{description}

The {\tt PPlist} contains a list of all the top-level applications , along with
the grouping (pgroup),
its print-depth (pdepth) and its print-length (plength).
If the grouping is {\tt BRACKETS} brackets will be printed around the formula.
A grouping {\tt DOT} means that
a dot will precede the formula, otherwise the formula will just be printed
without any delimiting symbols. The plength is the total
length of the formula if printed in one line, including spaces, brackets,
a.s.o., but not control characters which are used to denote character
sets, or {\tt SCRIBE} -commands.

The pdepth is recursively defined as the maximum pdepth of the left-hand sides
plus the maximum pdepth of the right-hand sides of the
applications, if the {\tt PPlist} contains applications, and the plength
of the generalized-character list (gencharlist) otherwise.
The plength of a gencharlist is 
its length after all members of the form
`(gencharlist)' have been deleted.  This means that characters that have to
be sent to the selected output device but do not occupy space (in the final
document) will simply be enclosed in parentheses.  By this convention
the function which then formats and actually prints the formula from the 
{\tt PPlist} can keep track of the vertical position within a line.  The 
pdepth associated with each subformula is used to decide the amount of
indentation, as described below.

The list of applications, aplicnlist, typically contains 
contains only one pair with the left-hand side
a function, and the right-hand side the argument the function is
applied to.  In case we have infix operators or multiple conjunctions
or disjunctions, like $A \equiv B$, 
$A \land B \land C \land D$, or $E \lor F$, aplicnlist will contain a 
different pair for each argument.  The left-hand side contains the
infix operator, if one has to be printed in front of the argument, the
right-hand side contains the argument itself.  Quantifiers are regarded as
single applications, where the left-hand side is the quantifier plus the
quantified variable, while the right-hand side is its scope.
Consider the following examples.
\begin{Example}
$A \equiv B$ \\
will be translated to\\
aplicnlist  =  ( (<> . <A>) (<EQUIV> . <B>) )\\
\\
$A \land B \land C \land D$\\
will be translated to \\
aplicnlist  =  ( (<> . <A>) (<AND> . <B>) (<AND> . <C>)
                 (<AND> . <D>) )\\
\\
$E \lor F$\\
will be translated to\\
aplicnlist  =  ( (<> . <E>) (<OR> . <F>) )\\
\\
$\forall x_\greeki G$ \\
will be translated to \\
aplicnlist  =  ( (<FORALL X<I>> . <G>) ) \\
\\
where <x> denotes the PPlist corresponding to the subformula $x$,\\
and <> stands for the empty PPlist ((NIL . (0 . NIL)) . 0)
\end{Example}

A generalized character, genchar, is defined to be an arbitrary
non-control ASCII character, the number of an ASCII character in parentheses,
or another generalized character list in double parentheses.  When an ASCII
character is
printed it is assumed that the cursor advances one position, while
everything in the sub-gencharlist is assumed not to appear
on the screen or in the document after being processed by SCRIBE.

An aplicnlist with the structure (aplicn .  \indexData{{\tt MARKATOM}})
signals that the aplicn is the internal representation of a logical
atom (For example {\tt ATM15}).  In case AtomValFlag is {\tt T}, the program notes the
cursor position, whenever it encounters such an aplicnlist during
printing and prints the name of the atom in the next line at this
position.

\subsection{Printing the PPlist} \index{PPlist}
After the {\tt PPlist} is created by the function \indexfunction{PWff}, the actual
output is done by the function \indexfunction{PrintPPlist}.  This function
takes a {\tt PPlist} and {\tt INDENT} as arguments and has the following
basic structure.

\begin{description}
\item [(1) ] Does the formula fit on the remainder of the line
(from {\tt INDENT} to {\tt RightMargin}) ?
If yes, just print it from the {\tt PPlist}.
If not, go to (2).

\item [(2) ] Is the formula composed of subformulas ?
If not, go to the next line and print it at the very right.
If yes, go to (3).

\item [(3) ] Is the formula a single application ?
If yes, call {\tt PrintPPlist} recursively, first with the function
then with the argument such that the function will appear at {\tt INDENT} and
the argument right after the function.
If not, go to (4).

\item [(4) ] Print each application in the application list in a new line,
the operators at the vertical position {\tt INDENT} and the
arguments at the position {\tt INDENT} + maximal length of the operators.
\end{description}

This algorithm will be slightly different if the flags described above
do not have their default values. See section ~\ref{Pretty-Printing Flags} for a description.

Some heuristics are employed to avoid the pathological case where the
formula appears mostly in the rightmost 10\% of each line.  Used in these
heuristics is the print-depth (pdepth), which is equal to the furthest
extension of the formula to the right if printed with the above algorithm.
Whenever the pdepth is greater than the remainder of the line, the indentation
will be minimized to two spaces. This is most useful if special
characters are not available, for example if `$\forall$' is printed as `FORALL'.

\subsection{Pretty-Printing Functions}\label{Pretty-Printing Functions}

Most of the functions used for the first phase of pretty-printing, i.e. for
building the {\tt PPlist} are already described in section ~\ref{More Printing Functions}.
The internal flag {\tt PPVirtFlag} controls whether functions like {\tt PrintFnTTY}
will actually produce output or create a {\tt PPlist}.  Here it is now of
importance, what the different printing functions return, something that was 
completely irrelevant for direct printing.

The general schema can be described as follows.  \indexfunction{PrintWffPlain} and
\indexfunction{PrintWffScope} return a {\tt PPlist}.  If called from \indexfunction{PrintWff}, these
{\tt PPlists} are assembled to an aplicnlist and returned.  In this case {\tt PrintWff}
returns an aplicnlist.  The lower level functions, \indexfunction{PrintFnDover}
and \indexfunction{PrintFnTTY} return the gencharlist which contains the
characters that would be printed in direct mode.  Note that therefore {\tt PrintWff}
will sometimes return a gencharlist instead of an aplicnlist.  These
two are interchangeable as far as the definition of the {\tt PPlist} is
concerned, and can hence be treated identically by {\tt PrintWffPlain}
which constructs a {\tt PPlist} from them.  

The special parameters \indexparameter{PPWfflist} and \indexparameter{PPWfflength}
keep track of the characters "virtually printed" and the length of
the formula "virtually printed", respectively.

On the very lowest level \indexfunction{PPrinc} and \indexfunction{PPTyo} perform a {\tt PRINC}
or {\tt TYO} virtually by appending the appropriate characters to the
{\tt PPWfflist}.  Characters that do not appear in the final document
or on the screen, are virtually printed by \indexfunction{PPrinc0} and \indexfunction{PPTyo0}.
They prevent the counter {\tt PPWfflength} from being incremented.  Similar
functions are \indexfunction{PP-Enter-Kset} and \indexfunction{PPTyos} which correspond to
{\tt Enter-Kset} and {\tt TYOS}.

In the second phase of pretty-printing as described in the previous section
{\tt PrintPPlist} is the main function.  If the remainder of a {\tt PPlist} fits
on the rest of the current line, \indexfunction{SPrintPPlist} is called which just 
prints the {\tt PPlist} without any counting or formatting.

\subsection{JForms and Descr-JForms}\index{JForm}\index{Descr-JForm}

A JForm is an alternative way of representing well-formed formulas and
is used by the matingsearch package and for printing vertical path
diagrams. In JForms multiple conjunction are represented as lists and
not as trees. Consider the following example.
\begin{Example}
$A \land B \land C \land [D lor E lor F]$
\end{Example}
As a wff in internal representation this will be
\begin{Example}
( (AND . ((AND .((AND . A) . B)) . C))
  . ((OR . ((OR . D) . E)) . F))
\end{Example}
Obviously this is not a very suitable form for vertical path
diagrams.  As a JForm, however, the above wff would read as 
\begin{Example}
(AND A B C (OR D E F))
\end{Example}
which is already close to what we would like to see.

The function \indexfunction{Describe-VPForm} takes a JForm like the one
above as an argument and returns a Descr-JForm, where we have the
information about the height and width of the subformulas, which we
need in order to format the output, explicitly attached to the parts of
the JForm.

Quantifiers are handled similarly. Multiple identical quantifiers
are combined in a list whose first element is the quantifier and the
rest is the list of variables which are quantified.
\begin{Example}
$\forall x \forall y \exists z \exists u A$
\end{Example}
is in internal representation
\begin{Example}
((x . FORALL) . ((y . FORALL)
       . ((z . EXISTS) . ((u . EXISTS) . A)))),
\end{Example}
and as a JForm it looks like
\begin{Example}
((FORALL x y) ((EXISTS z u) A)) .
\end{Example}
The following is a formal description of what a JForm and a Descr-JForm
are. Note that a descr-jform is entirely an internal concept, used by the
file \indexfile{vpforms.lisp} for working out how to format a vpform; a 
jform is a concept which is accessible to users (e.g. users have commands
to translate from gwffs to jforms and back)

\begin{description} % Description,Spacing=1.5}
\item [\indexData{\it JForm} ::=] {\it Literal} | {\it SignAtom} | ({\tt OR} [{\it JForm}]$^n_2$)
	|({\tt AND}  [JForm]$^n_2$ )
	\\ | (({\tt FORALL}  [Var]$_1^n$) JForm)
	| (({\tt EXISTS}  [Var]$_1^n$) JForm)

\item [\indexData{\it Literal} ::=] {\tt LIT1 | LIT2 | ...}

\item [\indexData{\it SignAtom} ::=] ({\tt NOT} {\it Atom}) | ({\it Atom})

\item [\indexData{\it Var} ::=] < any logical variable >

\item [\indexData{\it Atom} ::=] < any logical atom >
\end{description}
It should be noted here that some programs might expect the arguments
of a JForm starting with {\tt OR} not to start itself with an {\tt OR},
the argument of a JForm starting with {\tt FORALL} not to start with another {\tt FORALL}
etc., but this is by no means essential for vertical path diagrams.

%\begin{Description,Spacing=1.5}
\begin{description}
\item [\indexData{Desc-Jform} ::=] (\{$^{Literal}_{SignAtom}$\} Height Width (Width Width) (GenCharList PPlist)
	\\ (({\tt OR} [Desc-JForm]$^n_2$ )
	 Height Width ([Cols]$^n_2$ ))
	\\ (({\tt AND} [Desc-JForm]$^n_2$ )
  	 Height Width ([Rows]$^n_2$ ))
	\\ (((\{$^{\tt FORALL}_{\tt EXISTS}$\} [Var]$^n_1$)
	 Desc-JForm) Height Width Width GenCharList)

\item [\indexData{Height} ::=] {\tt 0} | {\tt 1} | {\tt 2} | ...

\item [\indexData{Width} ::=] {\tt 0} | {\tt 1} | {\tt 2} | ...

\item [\indexData{Cols} ::=] {\tt 0} | {\tt 1} | {\tt 2} | ...

\item [\indexData{Rows} ::=] {\tt 0} | {\tt 1} | {\tt 2} | ...
\end{description}
In a Descr-JForm the second and third element (Height and Width) contain
the height and width of the JForm that is described by the Descr-JForm.
In case the JForm was a literal or a signed atom the next two elements
are lists. The left element of each of these sublists
gives the width or print-representation
of the literal or atom, the right element gives the width or print-representation
of the literal's or atom's value.

If the JForm was a conjunction or disjunction, the last element of the 
corresponding Descr-JForm is a list of the rows or columns in which the
conjuncts or disjuncts begin.

If we deal with a top-level quantifier in our JForm, the last two
elements contain the width and the print-representation of the
quantifier together with the quantified variables.  For a description of
a GenCharList or PPlist see section ~\ref{PPlist}.

\subsection{Some Functions}

The function which is called by {\tt VPForm} and {\tt VPDiag} is
\indexfunction{\%VPForm}.  The handling of the comment and the different
files that have to be opened is done here.
The main function which translates a JForm into
a Descr-JForm is
\indexfunction{Describe-VPForm}.  SignAtoms and Literals are described by
\indexfunction{Describe-VPAtom} and \indexfunction{Describe-VPLit} , respectively.  The
virtual printing functions used for this process are \indexfunction{FlatSym}
and \indexfunction{FlatWff}.

{\tt FlatSym} takes an arbitrary {\tt LISP} identifier as an argument and
returns a pair (gencharlist . length) for this identifier.
{\tt FlatWff} takes a wff as argument and returns a {\tt PPlist} for
it.

The main function which then prints the Descr-JForm is \indexfunction{Print}-VPForm.
It takes the line of the Descr-JForm which should be printed as an
additional argument.  On lower levels \indexfunction{\%SPrintAplicn} and
\indexfunction{\%SPrintPPlist} print an aplicn or a {\tt PPlist} much in the same fashion
{\tt SPrintAplicn} and {\tt SPrintPPlist} do, except that \indexfunction{\%\%PRINC} takes
the role of {\tt PRINC} and {\tt TYO}.  This is necessary from the way
the actual output is handled.  If the vertical path diagram does
not fit on one page, several temporary files are opened and each
file contains the information for one of the pages.  This means 
that the characters have to be counted and a new file to be selected
as the current ouput file, whenever the character count exceeds
the global parameter \indexparameter{VPFPage}.  The counting
as well as the change of the current output file is done by the 
function \indexfunction{\%\%PRINC}.  The argument has to be either a {\tt LISP}-atom,
in which case it will be {\tt PRINC}'ed , or a single element list, 
in which case this element will be {\tt TYO}'ed.


\section{How to speed up pretty-printing (a bit)}

Pretty printing in TPS or ETPS is slow, for various reasons.  One of
them is the tremendous amount of temporary list space used, which takes
time and more time through garbage collection.  Another is the forgetfulness
of the printing routine which recomputes length and other information
over and over again.  Below we will try to explore ways to improve
the performance of the pretty printer without sacrificing any of the niceness
of the output.

Let us recount which factors make pretty-printing wffs more difficult than
pretty-printing Lisp S-Expressions.  For once, Lisp does not have infix
operators and can therefore get by with a significantly smaller amount of
lookahead.  Moreover, the lookahead can be done during the printing, where
the extra time delay is hardly noticeable, while TPS' lookahead must
all be done ahead of time, before the first character is printed.  Secondly,
Lisp does not deal with a variety of output devices, which makes counting
symbol lengths as well as printing symbols much faster and more transparent.

The result of a first attempt at pretty-printing is described earlier in this
chapter.  The solution is nicely recursive and a lot of information is made
available for deciding where to break and how to indent lines.  It is a sad
fact that the algorithm does not reuse any information whatsoever.  For example,
the printed representation of identifiers is recomputed over and over again.
Even worse, the characters comprising the printed representation of an
identifier are stored in a list, copies of which typically occur
in many places in the {\it pplist} of a single wff.

Let us now look at some of the problems and possible solutions of the
pretty-printing problem.

\subsection{Static and Dynamic Parameters}
Crucial to finding a good solution is to understand which factors affect
the appearance of wffs when printed.  These can be divided into two
classes.
\begin{description}
\item {\it Static Parameters}.  Static parameters are not changed during the printing
of a given wff.  In particular their values are identical for a wff
and their subformulas.  Of course, they may be changed from one printing
task to another, but not within printing a particular wff.  Examples
of such static parameters are {\tt AllScopeFlag}, {\tt Style}, {\tt KsetsAvailable},
{\tt PrintAtomnames}, etc.  One other characteristic of static parameters
is that one frequently would like to (and sometimes does) expand the
number of static parameters.

\item {\it Dynamic Parameters}.  Dynamic parameters are the ones which change from
a wff to a subwff.  They are highly context-dependent and are often
not explicitly available as flags, but implicitly computed.  Examples
of such parameters are ``{\it should I print a type for this identifier?}'',
{\tt PrintDepth}, ``{\it should I print brackets or a dot?}''.  An example
for the last question would be that we can sometimes write
$Q_{\greeko\greeki} . f_{\greeki\greeki} \; x_\greeki$ and sometimes
$Q_{\greeko\greeki} [f_{\greeki\greeki} \; x_\greeki]$ depending on
the brackets in wff containing this as a subformula.
\end{description}

One can easily see that static parameters can be handled fairly easily,
while dynamic parameters can become a headache if we are trying to save
information about the appearance of wffs and symbols.

\subsection{A grand solution, and why it fails}

A first stab at a solution could be briefly described as follows:

During the printing of a wff we permanently attach relevant printing
information like length, depth, or printing characters to each label and symbol
in the wff.  When the label or symbol appears again somewhere else, the
information does not have to be recomputed.

We would then have to somehow code the information about the current
static and dynamic parameters into the property of the label or symbol
which stores this information.

With the aid of a hashing function this is straightforward for the
static parameters, since we can compute the name of the relevant property
once and for all for the printing of a wff.  For dynamic parameters
this is still in theory possible, but in practice unfeasible.  We would
have to recompute (rehash) the values of the dynamic and static parameters
for each subformula.  To see that this is very difficult, if not impossible,
consider the following example.

The simple wff $P_{\greeko\greeka\greeka} \; x_\greeka \; y_\greeka$
may
appear as $Pxy$, $P_{\greeko\greeka\greeka}\; x y$,
$Px_\greeka\;y_\greeka$,
$Pxy_\greeka$, etc., with almost endless
possibilities for larger wffs.  All the information about which symbols should
have types etc. would have to be coded into the property name for, say, the
printing length of a label.

This clearly demonstrates that a grand solution is infeasible.

\subsection{A modest solution, and why it works}
Everything would work out fine if we could limit the number of dynamic
parameters.  This can be achieved very simply by restricting ourselves
to saving information about symbols only, and not about labels in general.

Of the various dynamic parameters, only one survives this cut. ``{\it Do I
put a type on this identifier}'' is the only question that can be solved
from the context only.  This simplification also reduces the number of
static parameters, For example {\it AllScopeFlag} is irrelevant to the printing
of symbols (wffs without proper subwffs).

However, care must be taken when the appearance of identifiers is changed. 
We will return to this problem later in the section about other issues.

\subsection{Implementation}
All printing requests go through the function {\tt PWFF}.  When {\tt PWFF} is
entered all static parameters have their final value.  Inside {\tt PWFF}
we will set two more special (global) variables: {\tt Hash-Notype} and
{\tt Hash-Type}.

{\tt Hash-Type} and {\tt Hash-Notype} will have as value of the name of the
property, which contains the symbol's {\it pplist}.  When constructing
the {\it pplist} for the given wff (the first pass during pretty-printing),
it is checked whether symbols have the appropriate property.  If yes,
the symbol itself stands for a {\it pplist}. (We are thus modifying the recursive
definition of {\it pplist}.)  If not, the {\it pplist} will be computed and
stored under the appropriate name on the property list of the symbol.
In this case, too, the symbol itself will appear in the {\it pplist}.

During the actual printing phase of the {\it pplist}, the necessary information
about symbols is retrieved from the property lists of the identifiers.

This presents one additional problem:  we have to preserve the
information about the dynamic parameters in the {\it pplist} itself, so
that the correct property can be accessed.  This could be done  in
a very general way (but for specific problems maybe wasteful way)
namely by including the name of the relevant property in the
{\it pplist}.  Alternatively we may use the special circumstance that
there are usually more identifiers without type.  We would then only
mark those identifiers with type, while all others are assumed to be
printed without types.

The solution above requires some auxiliary data structures.  There
should be a global variable, say {\tt static-printing-flags}, which
contains a list of all flags affecting the printing of symbols.
Then there must be a function {\tt hash-printing-flags} which takes
one argument (signifying whether types are to be printed) and returns
an identifier coding the value of the {\tt static-printing-flags} and
the argument.

\subsection{Other Issues}
In the solution proposed above it is left open, whether the actual
ASCII character representation of a symbol should be computed once and
for all (for each set of static and dynamic parameters) and saved in a
list which is part of the {\it pplist}, or simply recomputed every time
the identifier is printed.  The first solution would require significantly
more permanently occupied list space, the second solution would take
more time during each printing.

Notice, that the time required for the printing is not that long, since
the identifier will have to be printed only during the actual printing
phase, not during the virtual printing phase.  The length is already
known through the symbols property list.  It therefore seems to be much
better only to save the printing length of the identifier.

Another issue arises, when we allow that the printing appearance of
identifiers be changed.  Since all
the length information attached to the identifier will be wrong, it
is necessary to remove that information.  In order to be able to do
this, we need to recognize the properties which stem from the printing
algorithm sketched above. The simplest way to achieve this is to
declare a global variable {\tt hash-properties}, which is a list
of all the properties that have been used for printing so far.  This
must be updated, whenever {\tt PWFF} is called.  The hope is that due to
the limited number of static and dynamic parameters this list remains
manageable in size.  An alternative would be to write the hashing
function in such a way that all names produced by it start with
a unique pattern, say {\tt *@*}.  One can then systematically
look for properties whose name starts with {\tt *@*}.

\subsection{How to save more in special cases}
There is a straightforward generalization of this to case where we would
like to save information about the appearance of arbitrary labels.  The most
general solution fails, as demonstrated above, but if we restrict ourselves
to cases where the number of dynamic parameters is limited, we can get
somewhere.

We could make a case distinction of the kind:  save and use printing
info for labels only if {\tt PrintDepth} is {\tt 0}, {\tt PrintTypes} is {\tt NIL},
{\tt AllScopeFlag} is {\tt NIL}.  The only remaining dynamic parameter that
comes to mind is the bracketing information (which can take two different
values).  This is what makes this fragment of the grand solution
feasible.

Notice that this is not just of academic interest.  ETPS in first-order
mode satisfies all the criteria above.

\section{Entering and printing formulas}

\subsection{Parsing of Wffs}
Wffs can be specified in \TPS in a variety of ways, e.g. as strings
and with or without special characters.  Regardless how
a wff is specified there are general rules of syntax which always apply.
Sometimes one has to distinguish between first-order mode and higher-order
mode with slightly different syntactic rules.  If the global variable
\indexflag{First-Order-Mode} is {\tt T}, all parsing will be done
in first-order mode.  Similarly, the global variable
\indexflag{First-Order-Print-Mode} determines whether wffs are
printed as first-order or higher-order formulas.  It is important
to note that wffs printed in higher-order mode can only be parsed in 
higher-order mode, and formulas  printed in first-order mode can only be
parsed in first-order mode.

\begin{itemize} %, Spacing=1.5}
\item {\bf Operator precedence} - The parser for wffs is a standard operator
precedence parser.  The binding priority of an infix or prefix operator
is a simple integer and conforms with the usual conventions on how
to restore brackets in formulas.  ``{\tt [}'' and ``{\tt ]}'' serve as 
brackets and a period ``{\tt .}'' is to be  replaced by a left bracket
and a matching right bracket as far right as consistent with the
brackets already present, when brackets are restored from left to right.
For operations of equal binding priority, association to the left is assumed.
In order of ascending priority we have \\
%\begin{format}
$\equiv$ or {\tt EQUIV} (2) \\
$\limplies$ or {\tt IMPLIES} (3) \\
$\lor$ or {\tt OR} (4) \\
$\land$ or {\tt AND} (5) \\
$\lnot$ or {\tt NOT} or \verb+~+ (100) \\
applications (like $Pxy$ or {\tt $[\lambda \; x \; x]t$}) \\
binders ($\lambda$,$\forall$,$\exists$)
%\end{format}

\item {\bf Types} - Function types are built from single letter primitive types.  Grouping
is indicated by parentheses ``{\tt (}'' and ``{\tt )}''.  The basic types
are @subomicron or {\tt O} for truth values and @subiota or {\tt I} for
individuals.  Any letter (except {\tt T}, i.e.  $_\tau$) may serve as a
typevariable.  A pair $_{(\greeka\greekb)}$ or {\tt (AB)} is the type of
a function from elements of type {\tt B} to type {\tt A}.
E.g.  {\tt (O(OI))} or $_{\greeko(\greeko\greeki)}$ is the type of
a collection of sets of individuals.  Association to the left is
assumed, so {\tt (OAAA)} or $_{\greeko\greeka\greeka\greeka}$
is the type of a
three place predicate on variables of type {\tt (A)}.

\item {\bf Identifiers in higher-order mode} - In higher-order mode identifiers
may consists of any string of ASCII and special characters.  Greek
subscripts are reserved for type symbols and superscripts may only
appear at the end of the identifier.  The following symbols terminate
identifiers:  ``{\tt <Space> [ ] ( ) .  ~ <Return> <Tab>}''.  They may not
appear inside an identifier.  Reserved for special purposes package are
``{\tt :  ; ` < >}'' and should therefore not be used.  Also with special characters
 $\forall$, $\exists$, and $\lambda$ are also single character identifiers.
In strings, superscripted numbers are preceded by ``{\tt \verb+~+ }''.

\item {\bf Identifiers in first-order mode} - In first-order mode all identifiers
consist of a single letter.  Upper and lower case letters denote 
distinct identifiers.  In addition there is a set of keywords,
currently {\tt AND, OR, IMPLIES, NOT, FORALL, EXISTS, LAMBDA, EQUIV},
which are multi-letter identifiers and are always converted to all
uppercase.  They have to be delimited by one of the terminating
characters listed above, while all other identifiers may be typed without
spaces in between.

\item {\bf Type inference} - \TPS implements a version of Milner's algorithm
to infer the most general type of a wff with no or incomplete
type information.  Internally every identifier in a wff is typed.
Only the first occurrence
of an identifier will be typed in printing, unless the same identifier occurs
with different types in the same wff.
\end{itemize}

\section{Printing Vertical Paths}\index{Vertical Paths} % @tag(vpf)

There are a number of operations available in the editor and mate top levels
for printing vertical path diagrams. Also,
the following wff operation is available for printing vertical diagrams
of jforms:

\begin{itemize}
\item \indexother{VPFORM JFORM \{FILE\} \{STYLE\} \{PRINTTYPES\} \{BRIEF\} \{VPFPAGE\}}
\end{itemize}

The default values are:
\begin{itemize}
\item {\tt File} defaults to {\tt TTY:}, the terminal.

\item {\tt Style} defaults to the value of the flag \indexflag{STYLE}.

\item {\tt PrintTypes} defaults to the value of the flag \indexflag{PRINTTYPES}.

\item {\tt Brief} has three possible settings: {\tt T} means that only the names of logical atoms will be printed, 
	and not their values, {\tt NIL} means that under each atom its value will appear, and
\item {\tt L} means that just the atomnames will be printed in the diagram
	but a legend which contains every atom with its value will be 
	appended to the first page of output.

\item \indexparameter{VpfPage} is the number of characters which fit on one line.

\item \indexparameter{AndHeight} is an optional global variable
	which is equal to the number of
	blank lines to be left for a conjunction. It defaults
	to 1. 

\item \indexparameter{ForallIndent} is another optional global variable, containing the number
	of columns the quantifier is set off its scope. The default is 1.

\end{itemize}

{\tt BRIEF} can assume the values	{\tt T} for printing the diagram in brief 
format,	{\tt L} for a  brief diagram, but with a legend (atomnames with their
associated values) at the end of the first page, {\tt LT} for a legend with
type symbols forced to print and {\tt NIL} which gives the the full diagram.

Both of these functions will prompt you for a comment after a few statistics
about the diagram are given. The comment will
be spread across the top lines of the diagram with carriage returns placed
where you type them. 

\section{Global Parameters and Flags} \label{printflag}% @tag(printflag)

The following Lisp identifiers are either flags or values used by the
functions which read or write formulas.

\begin{description}
\item [\indexparameter{CFontTable}] 
This is a two dimensional array which is used to translate between
special characters on the Concept screen and their internal name.  For
example, {\tt (CFontTable 1 91)} is {\tt AND}.

\item [\indexflag{FIRST-ORDER-PRINT-MODE}] 
If {\tt T} wffs will be printed in first-order mode, otherwise in higher-order
mode.

\item [\indexflag{FIRST-ORDER-MODE-PARSE}] 
If {\tt T}, wffs will be parsed in first-order mode, otherwise higher-order
parsing mode is in effect.  See the section on parsing for a more
detailed explanation.

\item [\indexflag{LOWERCASERAISE}] 
If this identifier is set to {\tt T} then lower case letters will be converted
to their upper case equivalents.  This conversion is done when the formula
is first parsed.  The default value is {\tt NIL}.

\item [\indexparameter{PC}] 
A variable used by the formula printing functions.  It stores the previous
character printed.  It is used to help determine spacing within the
formula.  Set to {\tt NIL} in \indexfile{prt.lisp}.  Not important to the user.

\item [\indexflag{PRINTDEPTH}] 
When a formula is printed, subformulas at a depth of more that
{\tt PrintDepth} are not printed, but replaced by a "{\tt \&}".
In the formula editor, it is
set to \indexflag{EDPRINTDEPTH}.  A {\tt PRINTDEPTH} of {\tt 0} means that the formula
will be printed up to arbitrary depth.

\item [\indexflag{PRINTTYPES}] 
If this is set to {\tt T}, type symbols will be printed at least once
on all primitive symbols.  Otherwise, no types are printed.  This
defaults to {\tt T}, and can be toggled with the command \indexcommand{shownotypes}.

\item [\indexparameter{SailCharacters}] 
This is a list of pairs, {\tt (SYMBOL . NUM)}.  Here {\tt NUM} is the position
in the {\tt SAIL} character set for {\tt SYMBOL}.
\end{description}

The following flags are used to control the way formulas are printed.  Usually
the default setting of all these flags will be adequate.  For more information
see the section on pretty-printing in the \TPS user manual.

\begin{description}
\item [\indexflag{PPWFFLAG}] if {\tt T}, formulas will be pretty-printed.  This is
the default setting, except in the editor, where you can achieve pretty-printing
with the @Ited(PP) command.

\item [\indexflag{FLUSHLEFTFLAG}] 
If {\tt T}, no line of a pretty-printed formula will be indented.  The default
is {\tt NIL}

\item [\indexflag{FILLINEFLAG}] 
If {\tt NIL}, every argument of an associative infix operator will have a
separate line.  The default in {\tt NIL}.

\item [\indexflag{LOCALLEFTFLAG}] 
If {\tt T}, arguments of infix operators start in the same column as
the operator.  The default is {\tt NIL}.

\item [\indexflag{ATOMVALFLAG}] 
If {\tt T}, the name of every atom will be printed below its value.

\item [\indexflag{ALLSCOPEFLAG}] 
If {\tt T}, all punctuations (``{\tt []}'', ``{\tt .}'') will appear in the formulas
to be printed.  No association to the left or precedence of logical
connectives will be assumed.
\end{description}
::::::::::::::
wffrep.tex
::::::::::::::
\chapter{Representing Well-formed formulae}

\section{Types}

\begin{description}
\item [\indextypes{\it typeconstant} ] ::= Type Constant

An identifier with a non-{\tt NIL} {\tt TypeConst} property.
For example, {\tt O} and {\tt I}:
%\begin{tpsexample}
\begin{verbatim}
(def-typeconst o
  (mhelp "The type of truth values."))
\end{verbatim}
%\end{tpsexample}

\item [\indextypes{\it typevariable} ] ::= Type Variable

 An identifier with a non-{\tt NIL} {\tt TypeVar} property.
\end{description}
It is the parsers responsibility to give the {\tt TypeVar} property to types 
not previously encountered.

\begin{description}
\item [\indextypes{\it typesymbol} ] ::= {\it typeconstant} | {\it typevariable} | 
{\tt ({\it typesymbol} . {\it typesymbol})}
\end{description}

\section{Terminal Objects of the Syntax}\label{terminalobjects}

Before going into detail about the terminal objects of the syntax,
some general remarks about type polymorphism in \TPS are needed.

\TPS supports polymorphic objects, like $\subseteq$ (subset), which is
a relation that may hold between sets of any type.  It must be understood,
however, that the parser completely eliminates this ambiguity of types,
when actually reading a wff.  In a given wff every proper subwff has
a type!  Therefore, there is a class of objects with polymorphic type,
which never appear in a wff, but nevertheless may be typed by the user.
The instances of those polymorphic abbreviations or polymorphic proper
symbols inside the formula will refer, however, to those polymorphic primitive
symbols or polymorphic abbreviating symbols.

For reasons of efficiency, binders are handled slightly differently.
Binders are also polymorphic in the sense that a certain binder, say
$\forall$, may bind variables of any type.  The case of binder, however,
is slightly different from that of polymorphic abbreviations, since
a binder is not a proper subwff.   Binders, therefore, are left without
having a proper type.  We must, however, be able to figure out the type
of any given bound wff.  Thus each binder carries the information
about the type of the scope, the bound variable and the resulting bound
wff with it.  See Section ~\ref{Binders} for more details.

The list below introduces syntactic categories of objects known to the
parser only, which are not legal in wffs themselves.

\begin{description}
\item [\indexSyntax{pmprsym} ] ::= Polymorphic Primitive Symbol

 {\it pmprsyms} are the {\tt STANDS-FOR} property of 
{\it pmpropsyms}, but cannot appear in {\it gwffs} themselves.  
Examples would be {\tt PI} or {\tt IOTA}.

\item [\indexSyntax{pmabbsym} ] ::= Polymorphic Abbreviating Symbol

 \indexSyntax{pmabbsym} are the {\tt STANDS-FOR} property of 
{\it pmabbrevs}, but cannot appear in {\it gwffs} themselves. 
Examples are {\tt SUBSET}, {\tt UNION}, or {\tt IMAGE}.

\end{description}

The following categories are the ``terminal'' objects of proper wffs.
The parser may not produce a formula with any other atomic (in the 
Lisp sense) object then from the list below.

\begin{description}
\item [\indexSyntax{logconst} ] ::= Logical Constants

 For example: {\tt AND,} {\tt OR,} {\tt IMPLIES,} {\tt NOT,} {\tt FALSEHOOD,} {\tt TRUTH}:
%\begin{tpsexample}
\begin{verbatim}
(def-logconst and
   (type "OOO")
   (printnotype t)
   (infix 5)
   (prt-associative t)
   (fo-single-symbol and)
   (mhelp "Denotes conjunction."))
\end{verbatim}
%\end{tpsexample}

\item [\indexSyntax{propsym} ] ::= Proper Symbols

 For example: {\tt P<OA>}, {\tt x<A>}, {\tt y<A>}, {\tt Q<OB>}, {\tt x<B>} are
proper symbols after parsing
$ \forall \,x \forall \,y . \,P_{\greeko\greeka\greeka} \,x \,y \land \,Q_{\greeko\greeka} \,x$.
This example
demonstrates part of the parser.  Since in a given wff, a proper symbol
may appear with more than one type, the type of each proper must somehow
be encoded in its name.  \TPS does this by appending the type, {\tt (} and
{\tt )} replaced by {\tt <} and {\tt >}, respectively, to the print name of the
symbol.

\item [\indexSyntax{pmpropsym} ] ::= Polymorphic Proper Symbols

 These are just like \indexSyntax{propsym}, except that they also have 
a {\tt STANDS-FOR} property, which is the polymorphic primitive symbol
(\indexSyntax{pmprsym}) this polymorphic proper symbol was constructed
from.  Note that this particular instance of the polymorphic primitive
symbol always has a specific given type.  For example: {\tt IOTA<I<OI>>}
is a pmpropsym after parsing $y_{\greeki} = \iota [QQ y]$:
%\begin{tpsexample}
\begin{verbatim}
(def-pmpropsym iota
  (type "A(OA)")
  (typelist ("A"))
  (printnotype t)
  (fo-single-symbol iota)
  (mhelp "Description operator"))
\end{verbatim}
%\end{tpsexample}

\item [\indexSyntax{abbrev} ] ::= Abbreviations

 For example: @EQUIV.  This is separate category from polymorphic
abbreviations only for reasons of efficiency.  An abbreviation could
be thought of as a polymorphic abbreviation with an empty list of
type variables. For example:
%\begin{tpsexample}
\begin{verbatim}
(def-abbrev equiv
  (type "OOO")
  (printnotype t)
  (fo-single-symbol equiv)
  (infix 2)
  (defn "[=(OOO)]"))
\end{verbatim}
%\end{tpsexample}

\item [\indexSyntax{pmabbrev} ] ::= Polymorphic Abbreviations

 For example: {\tt SUBSET<O<OA><OA>>}, {\tt SUBSET<O<OB><OB>>} are polymorphic
abbreviations after parsing {\wt A@f12(oa) @SUBSET B @or [R@f12(obb) a] @~
@SUBSET [R b]}. For example:
%\begin{tpsexample}
\begin{verbatim}
(def-abbrev subset
   (type "O(OA)(OA)")
   (typelist ("A"))
   (printnotype t)
   (infix 8)
   (fo-single-symbol subset)
   (defn "lambda P(OA) lambda R(OA). forall x . P x implies R x"))
\end{verbatim}
%\end{tpsexample}

\item [\indexSyntax{binder} ] ::=  Variable Binders

For example: $\forall$, $\exists$, $\lambda$, $\exists_1$. See the section below.

\item [\indexSyntax{label} ] ::= A Label referring to one or more other wffs.

 For example: {\tt AXIOM1}, {\tt ATM15}, {\tt LABEL6}.  See Section ~\ref{Labels}.
\end{description}

In principle, the implementation is completely free to choose the
representation of the different terminal objects of the syntax.
The functions with test whether a given terminal object is of a given
kind is the only user visible functions.  Once defined, the particular
implementation of the object should not be needed or relied upon
by other functions.

It is explained more precisely what is meant by ``quick'' and ``slow''
predicates to decide whether a given object is in a certain syntactic
category in section ~\ref{quickslow}.  Here is a table of the different
syntactic categories with the ``slow'' test function for it and
the properties that are required or must be absent.
Keep in mind that the list reflects the current implementation, and
may not be reliable.

%\begin{Format, Group}
%@TabSet(1inch,2.25inch,4inch)
\begin{tabular}{lll}
Category & Predicate & Required Properties \\
& & Absent Properties \\
\\
{\it pmprsym} & {\tt PMPRSYM-P} & {\tt TYPE}, {\tt TYPELIST} \\ & & {\tt DEFN} \\
{\it pmabbsym} & {\tt PMABBSYM-P} & {\tt TYPE}, {\tt TYPELIST}, {\tt DEFN} \\
 \\
{\it logconst} & {\tt LOGCONST-P} & {\tt TYPE}, {\tt LOGCONST} \\
{\it propsym} & {\tt PROPSYM-P} & {\tt TYPE} \\ & & {\tt LOGCONST}, {\tt STANDS-FOR} \\
{\it pmpropsym} & {\tt PMPROPSYM-P} & {\tt TYPE}, {\tt POLYTYPELIST}, {\tt STANDS-FOR}
(a {\it pmprsym}) \\ 
\\
{\it pmabb} & {\tt PMPROPSYM-P} & {\tt TYPE}, {\tt POLYTYPELIST}, {\tt STANDS-FOR}
(a {\it pmabbsym}) \\
\\
{\it abbrev} & {\tt ABBREV-P} & {\tt TYPE}, {\tt DEFN} \\ & & {\tt TYPELIST} \\
{\it label} & {\tt LABEL-P} & {\tt FLAVOR} \\
 \\
{\it binder} & {\tt BINDER-P} & {\tt VAR-TYPE}, {\tt SCOPE-TYPE}, {\tt WFF-TYPE} \\ \\
\end{tabular}
%\end{Format}

\section{Explanation of Properties}
The various properties mentioned above are as follows:
\begin{description}
\item [{\tt TYPE} ] The type of the object.  Common are {\tt "OOO"} for
binary connectives and {\tt "I"} for individual constants.

\item [{\tt PRINTNOTYPE} ] In first-order mode, this is insignificant, but
if specified and {\tt T}, \TPS will never print types following the object.
It is almost always appropriate to specify this.

\item [{\tt INFIX} ] The binding priority of an infix operator.  This will declare
the connective to be infix.  The absolute value of {\tt INFIX} is irrelevant,
only the relative precedence of the infix and prefix operators matters.
If two binders have identical precedence, association will be to the left.
For example, if R1 and R2 are operators with {\tt INFIX} equal to 1 and 2,
respectively, {\tt "p R1 q R2 r R2 s"} will parse as 
{\tt "[p R1 [[q R2 r] R2 s]]"}.

\item [{\tt PREFIX} ] The binding priority of a prefix operator.  Binders are considered
prefix operators (see about binders below) and thus have a binding
priority.  The main purpose of a prefix binding priority is to allow
formulas like {\tt "~a=b"} to be parsed correctly as {\tt "~[a = b]"} by
giving {\tt =} precedence over {\tt ~}.

\item [{\tt PRT-ASSOCIATIVE} ] indicates whether to assume that the operator is
left associative during printing.  You may want to switch this off (specify
{\tt NIL}) for an infix operator like equivalence, say {\tt <=>}, since
{\tt "p <=> q <=> r"} is often considered to mean {\tt "p <=> q \& q <=> r"}.

\item [{\tt FO-SINGLE-SYMBOL} ] this is meaningful only in first-order mode and
declares the object to be a ``keyword'' in the sense that
it may be typed in all upper or lower case.  Moreover, the printer will
surround it by blanks if necessary to set off surrounding text.  Also
the parser will expect that the symbol is delimited by spaces, dots,
brackets, unless the symbol just consists of one letter, in which case
it doesn't matter.  You {\bf MUST} use this attribute in first-order
mode for an identifier with more than one character.

\item [{\tt MHELP} ] An optional help string.
\end{description}

Properties specific to binders are described in the section below about binders.
Here are some more examples. These examples do not actually exist under these names in \tps.
%\begin{tpsexample}
\begin{verbatim}
(def-logconst &
   (type "OOO")
   (printnotype t)
   (infix 5)
   (prt-associative t)
   (fo-single-symbol &)
   (mhelp "Conjunction."))
\end{verbatim}

Note that the {\tt (fo-single-symbol \&)} will make sure that spaces are printed
around {\tt \&} in formulas.

In the next example the character {\tt /} is used to make sure that the
disjunction is printed in lowercase, that is as {\tt v} instead of {\tt V}.

\begin{verbatim}
(def-logconst /v
   (type "OOO")
   (printnotype t)
   (infix 4)
   (prt-associative t)
   (fo-single-symbol /v)
   (mhelp "Disjunction."))

(def-logconst =>
   (type "OOO")
   (printnotype t)
   (infix 3)
   (fo-single-symbol =>)
   (mhelp "Implication."))
\end{verbatim}

We do not like spaces after negation.  So we do not declare it to be
a {\tt fo-single-symbol}.  That works only because {\tt -} consists of only
one character.

\begin{verbatim}
(def-logconst -
   (type "OO")
   (printnotype t)
   (prefix 6)
   (mhelp "Negation."))

\end{verbatim}
%\end{tpsexample}

\section{Non-terminal Objects of the Syntax}

%@TabSet(1inch)
%\begin{Format, Group}
\begin{tabular}{ll}
\indexSyntax[lsymbol] ::= &  {\it logconst} \\
&  | {\it propsym} \\
&  | {\it pmpropsym} \\
&  | {\it abbrev} \\
&  | {\it pmabbrev} \\
%\end{Format}
\end{tabular}

{\it lsymbol} roughly corresponds to what was called
\indexSyntax{hatom} {for Huet-atom from Huet's unification algorithm}
in the old representation.

{\bf Generalized WFFs}

%\begin{Format, Group}
\begin{tabular}{ll}
{\it gwff} ::=&   {\it lsymbol} \\
&  | {\tt (({\it propsym} . {\it binder}) . {\it gwff})}  ; Generalized binder \\
&  | {\tt ({\it gwff1} . {\it gwff2})}  where {\tt (cdr (type {\it gwff1}))} = {\tt (type {\it gwff2})} \\
&  | {\it label} \\
%\end{Format}
\end{tabular}

\section{Binders in TPS}\label{Binders} 

In the discussion about the internal representation of wffs the issue
of binders has been neglected so far.  Currently, TPS allows three
binders, $\lambda$, $\forall$, $\exists$ (plus some ``buggy'' fragments of
support for the $\exists_{1}$ binder).

Since most binders are inherently polymorphic, there is only one kind of
binder.  Notice that the definition is formulated such that a binder may
have a definition, but need not.

In order to determine the type of a bound wff, the type of the scope of
the binder must be matched against the type stored in the {\tt SCOPE-TYPE}
property.  Also, the type of the bound variable must match the
type in the {\tt VAR-TYPE} property.  These matches are performed, keeping
in mind that all types in the {\tt TYPELIST} property are considered to
be variables.  Then the bindings established during the match are used
to construct the type of the whole bound wff, using the {\tt WFF-TYPE}
property of the binder.

An example may illustrate this process.  The binder {\tt LAMBDA} has the
following properties

% @TabDivide(4)
\begin{tabular}{ll}
TYPELIST & (A B) \\
VAR-TYPE & B \\
SCOPE-TYPE & A \\
WFF-TYPE & (A . B)
\end{tabular}

When trying to determine the type of 
$\lambda x_\greeki . R_{\greeko\greeki\greeki} x$,
\TPS determines that {\tt A} must be $\greeki$, and that {\tt B} must be $\greeko\greeki$.
The type of the original formula is {\wt (A . B)} which then turns out to
be $\greeko\greeki\greeki$.

Note that {\tt TYPELIST} may be absent, i.e.  could be {\wt ()}, which
amounts to stating that the binder has no variable types.  Currently, we
are not using such binders.  An example would be {\it Foralln} , which can
bind only variables of type $\sigma$.

In addition to the properties mentioned above, a binder (except $\lambda$)
would have a definition.  One can then instantiate a binder just as a 
definition can be instantiated.  The definition is to be written with
two designated variables, one for the bound variable and one for the scope.
For example 
{\it THAT} {\it has definition}
$$\iota_{\greeka(\greeko\greeka)} . \lambda b_\greeka S_\greeko$$
Here the {\tt TypeList} would be ($\greeka$), designation for the bound
variable would be $b_\greeka$, designation for the scope would be
$S_\greeko$.

The internal representation for a binder inside a wff is always the
same and simply {\tt ((bdvar .  binder) .  scope)}, but all of the above
information must be present to determine the type of a wff, or to check
whether formulas are well-formed.

Fancy ``special effects'' such as 
$\forall x\in S . A$
must be handled
via special flavors of labels and are not treated as proper binders themselves.

Here are some examples of binders:
%\begin{tpsexample}
\begin{verbatim}

(def-binder lambda
   (typelist ("A" "B"))
   (var-type "A")
   (scope-type "B")
   (wff-type "BA")
   (prefix 100)
   (fo-single-symbol lambda)
   (mhelp "Church's lambda binder."))

(def-binder forall
   (typelist ())
   (var-type "I")
   (scope-type "O")
   (wff-type "O")
   (prefix 100)
   (fo-single-symbol forall)
   (mhelp "Universal quantifier."))
{\it The above definition is for math-logic-1, where forall can only bind individual 
variables. In math-logic-2, the definition is as follows:}

(def-binder forall
   (typelist ("A"))
   (var-type "A")
   (scope-type "O")
   (wff-type "O")
   (prefix 100)
   (fo-single-symbol forall)
   (mhelp "Universal quantifier."))
\end{verbatim}
%\end{tpsexample}

\subsection{An example: How to See the Wff Representations}

You can see examples of how wffs are represented by comparing the output 
of the editor commands \indexedop{P} and \indexedop{edwff}:

%\begin{tpsexample}
\begin{verbatim}
<44>ed x2106
<Ed45>p

FORALL x(I) [R(OI) x IMPLIES P(OI) x] AND FORALL x [~Q(OI) x IMPLIES R x]
 IMPLIES FORALL x.P x OR Q x

<Ed46>edwff
((IMPLIES (AND (|x<I>| . FORALL) (IMPLIES R<OI> . |x<I>|) P<OI> . |x<I>|) 
 (|x<I>| . FORALL) (IMPLIES NOT Q<OI> . |x<I>|) R<OI> . |x<I>|) 
 (|x<I>| . FORALL) (OR P<OI> . |x<I>|) Q<OI> . |x<I>|)
\end{verbatim}
%\end{tpsexample}

Another way to do this is as follows:

%\begin{tpsexample}
\begin{verbatim}
<3>ed x2106
<Ed4>cw
LABEL (SYMBOL):  [No Default]>x2106a
<Ed5>(plist 'x2106a)
(REPRESENTS ((IMPLIES (AND (|x<I>| . FORALL) (IMPLIES R<OI> . |x<I>|) 
 P<OI> . |x<I>|) (|x<I>| . FORALL) (IMPLIES NOT Q<OI> . |x<I>|) R<OI> . |x<I>|) 
 (|x<I>| . FORALL) (OR P<OI> . |x<I>|) Q<OI> . |x<I>|) FLAVOR WEAK)
\end{verbatim}
%\end{tpsexample}

And another way:

%\begin{tpsexample}
\begin{verbatim}
<2>(getwff-subtype 'gwff-p 'x2106) 

((IMPLIES (AND (|x<I>| . FORALL) 
                 (IMPLIES R<OI> . |x<I>|) P<OI> . |x<I>|) (|x<I>| . FORALL)
               (IMPLIES NOT Q<OI> . |x<I>|) R<OI> . |x<I>|) 
     (|x<I>| . FORALL) (OR P<OI> . |x<I>|) Q<OI> . |x<I>|)
\end{verbatim}
%\end{tpsexample}

And finally a way that only works at type O (the 0 below is a zero, not a capital O):

%\begin{tpsexample}
\begin{verbatim}
<3>(get-gwff0 'x2106)

((IMPLIES (AND (|x<I>| . FORALL) 
                 (IMPLIES R<OI> . |x<I>|) P<OI> . |x<I>|)
                  (|x<I>| . FORALL)
                   (IMPLIES NOT Q<OI> . |x<I>|) R<OI> . |x<I>|) 
                     (|x<I>| . FORALL)
                      (OR P<OI> . |x<I>|) Q<OI> . |x<I>|)
\end{verbatim}
%\end{tpsexample}
::::::::::::::
wffs.tex
::::::::::::::
